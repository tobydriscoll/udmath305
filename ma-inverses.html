<!DOCTYPE html>
<!--**************************************-->
<!--*    Generated from PreTeXt source   *-->
<!--*    on 2019-05-08T21:20:35-04:00    *-->
<!--*                                    *-->
<!--*   http://mathbook.pugetsound.edu   *-->
<!--*                                    *-->
<!--**************************************-->
<html lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Matrix inverses</title>
<meta name="Keywords" content="Authored in PreTeXt">
<meta name="viewport" content="width=device-width,  initial-scale=1.0, user-scalable=0, minimum-scale=1.0, maximum-scale=1.0">
<script src="https://sagecell.sagemath.org/static/jquery.min.js"></script><script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['\\(','\\)']],
    },
    TeX: {
        extensions: ["extpfeil.js", "autobold.js", "https://aimath.org/mathbook/mathjaxknowl.js", ],
        // scrolling to fragment identifiers is controlled by other Javascript
        positionToHash: false,
        equationNumbers: { autoNumber: "none", useLabelIds: true, },
        TagSide: "right",
        TagIndent: ".8em",
    },
    // HTML-CSS output Jax to be dropped for MathJax 3.0
    "HTML-CSS": {
        scale: 88,
        mtextFontInherit: true,
    },
    CommonHTML: {
        scale: 88,
        mtextFontInherit: true,
    },
});
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS_CHTML-full"></script><script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script><script src="https://aimath.org/knowl.js"></script><script src="https://aimath.org/mathbook/js/lib/jquery.sticky.js"></script><script src="https://aimath.org/mathbook/js/lib/jquery.espy.min.js"></script><script src="https://pretextbook.org/js/0.1/pretext.js"></script><link href="https://fonts.googleapis.com/css?family=Open+Sans:400,400italic,600,600italic" rel="stylesheet" type="text/css">
<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700&amp;subset=latin,latin-ext" rel="stylesheet" type="text/css">
<link href="https://aimath.org/mathbook/stylesheets/mathbook-3.css" rel="stylesheet" type="text/css">
<link href="https://aimath.org/mathbook/mathbook-add-on2.css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="mathbook-delaware.css">
</head>
<body class="mathbook-book has-toc has-sidebar-left">
<a class="assistive" href="#content">Skip to main content</a><div class="hidden-content" style="display:none">\(  \newcommand{\dd}[2]{\frac{d #1}{d #2}}
  \renewcommand{\Re}{\text{Re}}
  \renewcommand{\Im}{\text{Im}}
  \newcommand{\Span}[1]{\langle#1\rangle}
  \newcommand{\spr}[1]{^{(#1)}}
  \newcommand{\dd}[2]{\frac{d #1}{d #2}}
  \newcommand{\ddd}[2]{\frac{d^2 #1}{d #2^2}}
  \newcommand{\pp}[2]{\frac{\partial #1}{\partial #2}}
  \newcommand{\ppp}[2]{\frac{\partial^2 #1}{\partial #2^2}}
  \newcommand{\norm}[1]{\left\| #1 \right\|}
  \newcommand{\det}{\operatorname{det}}
  \newcommand{\bzero}{\boldsymbol{0}}
  \newcommand{\bu}{\mathbf{u}}
  \newcommand{\bA}{\mathbf{A}}
  \newcommand{\bB}{\mathbf{B}}
  \newcommand{\bC}{\mathbf{C}}
 \newcommand{\bD}{\mathbf{D}}
  \newcommand{\bI}{\mathbf{I}}
  \newcommand{\bV}{\mathbf{V}}
\newcommand{\bff}{\mathbf{f}}
  \newcommand{\bx}{\mathbf{x}}
  \newcommand{\by}{\mathbf{y}}
  \newcommand{\bb}{\mathbf{b}}
  \newcommand{\bu}{\mathbf{u}}
  \newcommand{\bv}{\mathbf{v}}
  \newcommand{\bw}{\mathbf{w}}
  \newcommand{\bc}{\mathbf{c}}
  \newcommand{\bz}{\mathbf{z}}
  \newcommand{\inprod}[2]{\langle #1, #2 \rangle}
  \newcommand{\twovec}[2]{\begin{bmatrix}#1\\#2\end{bmatrix}}
  \newcommand{\twodet}[4]{\begin{vmatrix}#1 \amp #2\\#3 \amp #4\end{vmatrix}}
  \newcommand{\twomat}[4]{\begin{bmatrix}#1 \amp #2\\#3 \amp #4\end{bmatrix}}
  \newcommand{\twodiag}[2]{\begin{bmatrix} #1 \amp 0 \\ 0 \amp #2 \end{bmatrix}}
  \newcommand{\fourdiag}[3]{\begin{bmatrix} #1 \amp  \amp \amp  \\                    \amp #2 \amp  \amp  \\  \amp \amp \ddots \amp \\  \amp  \amp  \amp #3 \end{bmatrix}}
\newcommand{\lt}{&lt;}
\newcommand{\gt}{&gt;}
\newcommand{\amp}{&amp;}
\)</div>
<header id="masthead" class="smallbuttons"><div class="banner"><div class="container">
<a id="logo-link" href=""></a><div class="title-container">
<h1 class="heading"><a href="notes.html"><span class="title">Notes for MATH 305</span></a></h1>
<p class="byline">Toby Driscoll</p>
</div>
</div></div>
<nav id="primary-navbar" class="navbar"><div class="container">
<div class="navbar-top-buttons">
<button class="sidebar-left-toggle-button button active" aria-label="Show or hide table of contents sidebar">Contents</button><div class="tree-nav toolbar toolbar-divisor-3"><span class="threebuttons"><a id="previousbutton" class="previous-button toolbar-item button" href="ma-multiplication.html" title="Previous">Prev</a><a id="upbutton" class="up-button button toolbar-item" href="matrix-algebra.html" title="Up">Up</a><a id="nextbutton" class="next-button button toolbar-item" href="ma-nullspace.html" title="Next">Next</a></span></div>
<button class="sidebar-right-toggle-button button active">Annotations</button>
</div>
<div class="navbar-bottom-buttons toolbar toolbar-divisor-4">
<button class="sidebar-left-toggle-button button toolbar-item active">Contents</button><a class="previous-button toolbar-item button" href="ma-multiplication.html" title="Previous">Prev</a><a class="up-button button toolbar-item" href="matrix-algebra.html" title="Up">Up</a><a class="next-button button toolbar-item" href="ma-nullspace.html" title="Next">Next</a>
</div>
</div></nav></header><div class="page">
<div id="sidebar-left" class="sidebar" role="navigation"><div class="sidebar-content">
<nav id="toc"><h2 class="link"><a href="index.html" data-scroll="index"><span class="title">Front Matter</span></a></h2>
<ul><li><a href="preface-1.html" data-scroll="preface-1">Preface</a></li></ul>
<h2 class="link"><a href="first-order-ode.html" data-scroll="first-order-ode"><span class="codenumber">1</span> <span class="title">First-order ODEs</span></a></h2>
<ul>
<li><a href="linearity.html" data-scroll="linearity">Introduction</a></li>
<li><a href="exponentials.html" data-scroll="exponentials">Exponentials</a></li>
<li><a href="first-linear-ode.html" data-scroll="first-linear-ode">Linear ODEs</a></li>
<li><a href="first-linear-const.html" data-scroll="first-linear-const">Linear, constant-coefficients</a></li>
<li><a href="complex-exp.html" data-scroll="complex-exp">Using complex numbers</a></li>
<li><a href="variable-coeffs.html" data-scroll="variable-coeffs">Linear, variable coefficient</a></li>
<li><a href="first-order-models.html" data-scroll="first-order-models">Modeling with first-order ODEs</a></li>
<li><a href="steady-states.html" data-scroll="steady-states">Steady states</a></li>
<li><a href="logistic-equation.html" data-scroll="logistic-equation">Logistic equation</a></li>
<li><a href="separable.html" data-scroll="separable">Separable equations</a></li>
<li><a href="laplace-solutions.html" data-scroll="laplace-solutions">Laplace transform methods</a></li>
</ul>
<h2 class="link"><a href="second-order-ode.html" data-scroll="second-order-ode"><span class="codenumber">2</span> <span class="title">Second-order ODEs</span></a></h2>
<ul>
<li><a href="so-simple-harmonic.html" data-scroll="so-simple-harmonic">Simple harmonic motion</a></li>
<li><a href="so-complex-exponentials.html" data-scroll="so-complex-exponentials">Complex exponentials</a></li>
<li><a href="so-constant-coefficients.html" data-scroll="so-constant-coefficients">Constant coefficients</a></li>
<li><a href="so-forced-oscillators.html" data-scroll="so-forced-oscillators">Exponentially forced oscillators</a></li>
<li><a href="so-higher-order.html" data-scroll="so-higher-order">Higher-order ODEs</a></li>
<li><a href="so-applications.html" data-scroll="so-applications">Applications of oscillators</a></li>
<li><a href="so-general-forcing.html" data-scroll="so-general-forcing">Other forcing functions</a></li>
<li><a href="so-laplace.html" data-scroll="so-laplace">Laplace transform methods</a></li>
<li><a href="so-laplace-repeated.html" data-scroll="so-laplace-repeated">Repeated and complex poles</a></li>
<li><a href="so-convolutions.html" data-scroll="so-convolutions">Convolutions</a></li>
</ul>
<h2 class="link"><a href="two-dimensional-systems.html" data-scroll="two-dimensional-systems"><span class="codenumber">3</span> <span class="title">2D systems</span></a></h2>
<ul>
<li><a href="twod-slope-fields.html" data-scroll="twod-slope-fields">Slope fields</a></li>
<li><a href="twod-fixed-points.html" data-scroll="twod-fixed-points">Linear 2D systems</a></li>
<li><a href="twod-linearization.html" data-scroll="twod-linearization">Linearization</a></li>
</ul>
<h2 class="link"><a href="matrix-algebra.html" data-scroll="matrix-algebra"><span class="codenumber">4</span> <span class="title">Matrix algebra</span></a></h2>
<ul>
<li><a href="ma-rows-columns.html" data-scroll="ma-rows-columns">Rows and columns</a></li>
<li><a href="ma-elimination.html" data-scroll="ma-elimination">Elimination for linear systems</a></li>
<li><a href="ma-multiplication.html" data-scroll="ma-multiplication">Multiplying matrices</a></li>
<li><a href="ma-inverses.html" data-scroll="ma-inverses" class="active">Matrix inverses</a></li>
<li><a href="ma-nullspace.html" data-scroll="ma-nullspace">Nullspaces</a></li>
<li><a href="ma-eigenvalues.html" data-scroll="ma-eigenvalues">Eigenvalues</a></li>
<li><a href="ma-diagonalization.html" data-scroll="ma-diagonalization">Diagonalization</a></li>
<li><a href="ma-linearODE.html" data-scroll="ma-linearODE">Linear ODE systems</a></li>
<li><a href="ma-exponential.html" data-scroll="ma-exponential">Matrix exponential</a></li>
</ul>
<h2 class="link"><a href="fourier-series.html" data-scroll="fourier-series"><span class="codenumber">5</span> <span class="title">Fourier series</span></a></h2>
<ul>
<li><a href="fs-orthgonality.html" data-scroll="fs-orthgonality">Main idea</a></li>
<li><a href="fs-realimag.html" data-scroll="fs-realimag">Special forms</a></li>
<li><a href="fs-convergence.html" data-scroll="fs-convergence">Convergence</a></li>
</ul>
<h2 class="link"><a href="fourier-series.html" data-scroll="fourier-series"><span class="codenumber">6</span> <span class="title">Boundary-value problems</span></a></h2>
<ul>
<li><a href="bvp-boundaries.html" data-scroll="bvp-boundaries">Boundaries</a></li>
<li><a href="bvp-eigenfunctions.html" data-scroll="bvp-eigenfunctions">Eigenfunctions</a></li>
<li><a href="bvp-diffusion.html" data-scroll="bvp-diffusion">Diffusion equation</a></li>
<li><a href="bvp-separation.html" data-scroll="bvp-separation">Separation of variables</a></li>
<li><a href="bvp-nondim.html" data-scroll="bvp-nondim">Nondimensionalization</a></li>
</ul></nav><div class="extras"><nav><a class="mathbook-link" href="https://mathbook.pugetsound.edu">Authored in PreTeXt</a><a href="https://www.mathjax.org"><img title="Powered by MathJax" src="https://www.mathjax.org/badge/badge.gif" alt="Powered by MathJax"></a></nav></div>
</div></div>
<main class="main"><div id="content" class="mathbook-content"><section class="section" id="ma-inverses"><h2 class="heading hide-type">
<span class="type">Section</span> <span class="codenumber">4.4</span> <span class="title">Matrix inverses</span>
</h2>
<a href="ma-inverses.html" class="permalink">Â¶</a><section class="introduction" id="introduction-17"><p id="p-381">From section 4.4.</p>
<p id="p-382">From now on, we're going to focus on square, \(n\times n\) matrices.</p>
<p id="p-383">It's pretty immediate that for any square matrix \(\bA\text{,}\) \(\bA\boldsymbol{0}=\boldsymbol{0}\bA=\boldsymbol{0}\) for the all-zero square matrix \(\boldsymbol{0}\text{.}\) So we have a multiplicative zero matrix. But the situation is a bit less obvious when it comes to a multiplicative unit element. Some reflection on matrix multiplication properties reveals that if we define an <dfn class="terminology">identity matrix</dfn> as the \(n\times n\) matrix</p>
<div class="displaymath">
\begin{equation*}
\bI = \begin{bmatrix} 
1 \amp 0 \amp 0 \amp \cdots \amp 0 \\
0 \amp 1 \amp 0 \amp \cdots \amp 0 \\
\vdots \amp  \amp \ddots \amp  \amp \vdots\\
0 \amp \cdots \amp 0 \amp 1 \amp 0 \\
0 \amp \cdots \amp 0 \amp 0 \amp 1
\end{bmatrix}\text{,}
\end{equation*}
</div>
<p>then \(\bI\bA=\bA\) and \(\bA\bI=\bA\text{.}\) Actually, we don't need to restrict \(\bA\) to be square here, and it's also true that \(\bI\bx=\bx\) for all \(n\times 1\) vectors \(\bx\text{.}\)</p></section><section class="subsection" id="subsection-57"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.4.1</span> <span class="title">Inverse matrix</span>
</h3>
<p id="p-384">With the multiplicative identity in hand, the next order of business is to look for multiplicative inverses. To wit, if we have a square matrix \(\bA\text{,}\) can we find another square matrix \(\bC\) such that \(\bA\bC = \bC\bA = \bI \text{?}\) If such a matrix exists, we call it the <dfn class="terminology">inverse</dfn> of \(\bA\) and write \(\bA^{-1}=\bC\text{.}\)</p>
<p id="p-385">(Interestingly, while the lack of commutativity means we formally need to specify two products to define an inverse, it turns out that one can prove [not so easily!] that one of these products automatically implies the other; that is, a "left-inverse" is also a "right-inverse", and vice versa.)</p>
<p id="p-386">Because the zero matrix gives zero for every multiplicative partner, it cannot have an inverse. That's completely in line with what we know about the scalar zero. However, unlike scalars, there are nonzero matrices that do not have any inverse. The simplest is</p>
<div class="displaymath">
\begin{equation*}
\bA = \begin{bmatrix}  0 \amp 1 \\ 0 \amp 0 \end{bmatrix}\text{.}
\end{equation*}
</div>
<p>(Proof: The second column of \(\bC\bA\) is zero for any square \(\bC\text{.}\)) We call such a matrix <dfn class="terminology">singular</dfn>. A matrix in the opposite situation (having an inverse)is said to be <dfn class="terminology">invertible</dfn>, or <dfn class="terminology">nonsingular</dfn>.</p>
<p id="p-387">This notion of singularity is the same one we encountered with 2-by-2 linear systems. For, if \(\bA\) is not singular, then it has an inverse (by definition), and the linear system \(\bA\bx=\bb\) is equivalent to</p>
<div class="displaymath">
\begin{equation*}
\bA^{-1} \bb = \bA^{-1} (\bA \bx) = \bigl(  \bA^{-1} \bA \bigr) \bx = \bI \bx = \bx\text{.}
\end{equation*}
</div>
<p>That is, if \(\bA\) is nonsingular, the linear system has the unique solution \(\bA^{-1} \bb\text{.}\)  That the converse is also true (unique solution implies nonsingularity) is part of what's usually called the Fundamental Theorem of Linear Algebra, which is a little beyond our scope here.</p>
<p id="p-388">It's one thing to say "the inverse exists," and quite another to say "here it is." Finding the inverse of a given matrix is in principle a matter of solving \(n\) linear systems. That's because if we let \(\mathbf{e}_j\) be the \(j\)th column of \(\bI\text{,}\) then according to the columnwise interpretation of \(\bA\bA^{-1}\text{,}\) the solution of \(\bA \bx = \mathbf{e}_j\) is the \(j\)th column of \(\bA^{-1}\text{.}\) There is a well-known algorithm for solving all those systems simultaneously. However, it's beside the point. If our goal is to solve a linear system \(\bA\bx=\bb\text{,}\) then there is little reason to solve \(n\) other systems first to get the inverse! The inverse tends to be much more important theoretically than for practical computation.</p></section><section class="subsection" id="subsection-58"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.4.2</span> <span class="title">\(2\times 2\) inverses and systems</span>
</h3>
<p id="p-389">One of the few circumstances in which it's easy to get our hands on a matrix inverse is in the \(2\times 2\) case:</p>
<div class="displaymath">
\begin{equation*}
\begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix}^{-1} = 
\frac{1}{ad-bc} \begin{bmatrix} d \amp -b \\ -c \amp a \end{bmatrix}\text{.}
\end{equation*}
</div>
<p>It's even simpler to remember this if you note that \(ad-bc\) is the determinant of the matrix.</p>
<p id="p-390">Even though a 2x2 inverse is easy, it's still not the most convenient way to solve a linear system. There is an even faster equivalent shortcut known as <dfn class="terminology">Cramer's Rule</dfn>:</p>
<div class="displaymath">
\begin{align*}
x_1 \amp = \frac{ \twodet{b_1}{A_{12}}{b_2}{A_{22}} }{ \det(\bA) }\\
x_2 \amp = \frac{ \twodet{A_{11}}{b_1}{A_{21}}{b_2} }{ \det(\bA) }\text{.}
\end{align*}
</div>
<article class="example-like" id="example-51"><a knowl="" class="id-ref" refid="hk-example-51"><h6 class="heading">
<span class="type">Example</span> <span class="codenumber">4.4.1</span>
</h6></a></article><div id="hk-example-51" class="hidden-content tex2jax_ignore"><article class="example-like"><p id="p-391">For</p>
<div class="displaymath">
\begin{align*}
-x + 3y \amp = 1 \\
3x + y \amp = 7\text{,}
\end{align*}
</div>
<p>Cramer's Rule says</p>
<div class="displaymath">
\begin{align*}
x \amp = \frac{ \twodet{1}{3}{7}{1} }{ \det(\bA) }=  \frac{ \twodet{1}{3}{7}{1} }{ \twodet{-1}{3}{3}{1} } = \frac{-20}{-10} \\
y \amp = \frac{ \twodet{-1}{1}{3}{7} }{ \det(\bA) } = \frac{ \twodet{-1}{1}{3}{7} }{ \twodet{-1}{3}{3}{1} } = \frac{-10}{-10}\text{.}
\end{align*}
</div></article></div></section><section class="subsection" id="subsection-59"><h3 class="heading hide-type">
<span class="type">Subsection</span> <span class="codenumber">4.4.3</span> <span class="title">Determinants</span>
</h3>
<p id="p-392">We're going to rely on a key fact (from the FTLA that I alluded to above).</p>
<article class="theorem-like" id="fact-5"><h6 class="heading">
<span class="type">Fact</span> <span class="codenumber">4.4.2</span>
</h6>
<p id="p-393">A square matrix \(\bA\) is singular if and only if its determinant is zero.</p></article><p id="p-394">Of course this result requires us to say what a determinant <em class="emphasis">is</em> beyond the 2-by-2 case. One way to compute a determinant is as the product of pivots in the elimination process, provided you flip the sign for each row swap. This makes the connection to singularity clear, and it's the best way for a computer to find the determinant, but most humans prefer a recursive method called <dfn class="terminology">cofactor expansion</dfn>, in which</p>
<div class="displaymath">
\begin{equation*}
\det(\bA) = \sum (-1)^{i+j} A_{ij} \det( \mathbf{M}_{ij} )\text{,}
\end{equation*}
</div>
<p>where the sum is taken over any row or column of \(\bA\text{,}\) and \(\mathbf{M}_{ij}\) is the matrix that results from deleting row \(i\) and column \(j\) from \(\bA\text{.}\) (You may have learned vector cross products as a 3-by-3 determinant using these cofactors across the top row.)</p></section></section></div></main>
</div>
</body>
</html>
