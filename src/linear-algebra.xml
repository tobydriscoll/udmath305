<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch1-linear-algebra" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Linear algebra</title>

<section xml:id="la-linear-systems">
	<title>Linear systems of equations</title>
	<p>
		When you first learned algebra, there was a lot of focus on solving equations. It's very likely that you started with linear equations, because these are the easiest ones.
		</p>
		<p>
		It might feel silly, but let's review what it means to solve the linear equation
		<me>ax = b</me>
		for <m>x</m>.
		<ol>
		<li>If <m>a\neq 0</m>, there is a single solution, <m>x=b/a</m>.</li>
		<li><p>Otherwise:
		<ol>
		<li>If also <m>b=0</m>, then every value of <m>x</m> is a valid solution.</li>
		<li>Otherwise, there are no solutions.</li>
		</ol>
		</p>
		</li>
		</ol>
		To summarize, for nonzero <m>a</m> there is exactly one solution, and otherwise there are infinitely many or zero solutions (depending on <m>b</m>). A linear system with no solutions is called <term>inconsistent</term> <idx>inconsistent linear system</idx>, with the opposite being <em>consistent</em>.
	</p>

	<p>
	  Our goal is to understand equations that depend linearly on <em>multiple</em> variables. But before even writing out what that means, let me give away the punch line: <em>the only possibilities are exactly the same three outcomes described above</em>. The sole difference is that the condition "<m>a=0</m>" is generalized to something a little different.
	</p>

	<p>
	  Going one more baby step to two equations in two variables, we want to solve
	  <md>
	  <mrow> ax + by \amp = f </mrow>
	  <mrow> cx + dy \amp = g </mrow>
	  </md>.
	  There is some easy geometric intuition here. Each equation represents a straight line in the plane. So our cases above translate directly into
	  <ol>
		<li>If the lines are not parallel, there is a single solution.</li>
		<li><p>Otherwise:
		  <ol>
		  <li>If the lines are identical, there are infinitely many solutions.</li>
		  <li>Otherwise, there are no solutions.</li>
		  </ol></p>
		</li>
		</ol>
		It's not hard to turn those statements into algebraic conditions. The slopes of the two lines are (allowing infinities for a moment) <m>-a/b</m> and <m>-c/d</m>, so we can say they are equal if and only if <m>ad=bc</m>.
		<ol>
			<li>If <m>ad\neq bc</m>, there is a single solution.</li>
			<li><p>Otherwise:
			  <ol>
			  <li>If one line's equation is a multiple of the other, there are infinitely many solutions.</li>
			  <li>Otherwise, there are no solutions.</li>
			  </ol></p>
			</li>
			</ol>
	</p>
	<example>
		<p>
			If we look at the equations
			<md>
				<mrow> x - 3y \amp = 1 </mrow>
				<mrow> -2x + 6y \amp = 2 </mrow>
			</md>,
			then we identify <m>(a,b,c,d)=(1,-3,-2,6)</m>, and <m>ad-bc=6-6=0</m>. So we know there is no unique solution (parallel lines). Dividing the second equation by <m>-2</m> leads to the equivalent system
			<md>
				<mrow> x - 3y \amp = 1 </mrow>
				<mrow> x - 3y \amp = -1 </mrow>
			</md>.
			It's clear that there is no way to satisfy both equations simultaneously.
		</p>
	</example>
	<p>
		Time to put on our big-kid pants. For <m>m</m> equations in <m>n</m> variables, we need to use subscripts rather than different letters for everything:
		<md>
			<mrow>a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \amp = b_1 </mrow>
			<mrow>a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \amp = b_2 </mrow>
			<mrow number="no">\amp \vdots </mrow>
			<mrow>a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n \amp = b_m </mrow>
		</md>.
		The first index on <m>a_{ij}</m> is the equation number, and the second is the variable number. Or we can think of them as <q>row</q> and <q>column</q> if we put all these numbers into a separate table,
		<men xml:id="la-eq-mbynmatrix">
			\bA = \begin{bmatrix}
			a_{11} \amp a_{12}  \amp \cdots \amp a_{1n} \\
			a_{21} \amp a_{22}  \amp \cdots \amp a_{2n} \\
			\vdots \amp  \amp  \amp \vdots\\
			a_{m1} \amp \cdots  \amp a_{m,n-1} \amp a_{mn}
			\end{bmatrix}
		</men>.
		This is our first look at a <idx>matrix</idx> <term>matrix</term>. We say it has size (or shape) <m>m</m> by <m>n</m>. I will use bold capital letters for matrices. While we are at it we should collect the other indexed items into single objects, too:
		<me>
			\bx = \begin{bmatrix}
			x_1 \\ x_2 \\ \vdots \\ x_n
			\end{bmatrix}, \qquad
			\bb = \begin{bmatrix}
			b_1 \\ b_2 \\ \vdots \\ b_m
			\end{bmatrix}
		</me>.
		These are <term>vectors</term>. I use bold lowercase letters for vectors.
	</p>
	<aside><p>We often think of a vector as a matrix with one column (or one row, if we use that shape). That is how MATLAB does things. However, other languages, such as Python, are different in this respect.</p></aside>
	<p>
		We will soon be writing a linear system of equations as simply <m>\bA\bx=\bb</m>. And just as above, there is a property of <m>\bA</m> that determines whether or not there is a unique solution. If not, then it depends on both <m>\bA</m> and <m>\bb</m> whether there are infinitely many solutions, or none. Those are the only possibilities.
	</p>
</section>

<section xml:id="la-elimination">
	<title>Row elimination</title>
	<p>You've probably solved small systems of equations by substitution. In order to solve systems with more equations and variables, we take a slightly different point of view and use <term>elimination</term>. The goal of elimination is to transform the system to an equivalent one whose solution(s) we can just read off.</p>
	<example xml:id="la-ex-gausselim">
		<p>
			We start with a system
			<md>
			<mrow>  x_1 - x_2  - x_3 \amp = 2 </mrow>
			<mrow> 3x_1 - 2x_2 \amp = 9 </mrow>
			<mrow> x_1 - 2x_2 - x_3 \amp = 5 </mrow>
			</md>.
			The first step is to use the first equation to eliminate <m>x_1</m> from the second and third equations. We subtract 3 times equation 1 from equation 2, and 1 times equation 1 from equation 3:
			<md>
				<mrow>  x_1 - x_2  - x_3  \amp = 2 \amp \amp </mrow>
				<mrow> (3x_1 - 2x_2) - 3(x_1 - x_2  - x_3) \amp = 9 - 3(2)\quad  \amp \Rightarrow \quad  x_2 +3x_3 \amp = 3</mrow>
				<mrow> (x_1 - 2x_2 - x_3) - 1(x_1 - x_2  - x_3) \amp = 5 - 1(2) \quad \amp \Rightarrow \quad  -x_2 \amp = 3</mrow>
			</md>.
			</p>
			<aside><p>I know you want to use that last equation to get rid of <m>x_2</m> everywhere right away. That would be fine for this particular problem, but we are aiming for a systematic process that works every time.</p></aside>
			<p>
			The next step of the recipe is to leave the first equation alone, and use the second to eliminate <m>x_2</m> from all the others below it (i.e., the third).
			<md>
				<mrow>  x_1 - x_2  - x_3  \amp = 2  \amp \amp </mrow>
					<mrow> x_2 + 3x_3 \amp = 3 \amp \amp </mrow>
					<mrow> (-x_2) + (x_2+3x_3)  \amp = 3 + 3 \amp \quad \Rightarrow \quad 3x_3 \amp = 6</mrow>
			</md>.
		</p>
	</example>
	<p>
		The process in <xref ref="la-ex-gausselim"/> is most commonly known as <idx>Gaussian elimination</idx><term>Gaussian elimination</term>. (It's a misnomer, as the process was known in China thousands of years before Gauss, but never mind.) We <em>could</em> solve the system at the end of the example by starting with the last equation to deduce that <m>x_3=2</m>. We then put that value into the second equation and can solve that for <m>x_2</m>, etc.
	</p>
	<p>
		Instead, though, we are going to continue to manipulate the system to get something even simpler.
	</p>
	<example xml:id="la-ex-gaussjordan">
		<p>
		(Continuation of <xref ref="la-ex-gausselim"/>.) Having reached the last variable and equation, we turn around and eliminate <em>upwards</em> instead:
		<md>
			<mrow>  (x_1 - x_2 - x_3) + \frac{1}{3}(3x_3) \amp = 2  +\frac{1}{3}(6) \amp \quad \Rightarrow \quad x_1  -x_2 \amp = 4</mrow>
			<mrow> (x_2 + 3x_3) - (3x_3) \amp = 3 - (6)\amp \quad \Rightarrow \quad x_2 \amp = -3</mrow>
			<mrow> 3x_3  \amp = 6 \amp  \amp</mrow>
		</md>.
		Continue moving back upwards, to the second equation, and use it to eliminate above:
		<md>
			<mrow>  (x_1 - x_2 ) + (x_2) \amp = 4  + (-3)  \amp \quad \Rightarrow \quad x_1  \amp = 1</mrow>
			<mrow> x_2 \amp = -3 \amp \amp </mrow>
			<mrow> 3x_3  \amp = 6 \amp \amp </mrow>
		</md>.
		Obviously, the (unique) solution is <m>x_1=1</m>, <m>x_2=-3</m>, <m>x_3=2</m>.
		</p>
	</example>
	<p>
		That was a mouthful. We can make it easier if we express the system as <m>\bA\bx=\bb</m> for a constant matrix and vector, and just manipulate the numbers without carrying around extra notation. We start with the <idx>augmented matrix</idx> <term>augmented matrix</term> <m>[\bA,\,\bb]</m>.
	</p>
	<example>
		<p>
		We repeat <xref ref="la-ex-gausselim"/> and <xref ref="la-ex-gaussjordan"/> but just with the relevant constants. The key is to use zeros as coefficients where there are <q>silent</q> variables. The starting augmented matrix is
		<me>
			\begin{bmatrix}
				1 \amp -1 \amp -1 \amp 2 \\
				3 \amp -2 \amp 0 \amp 9 \\
				1 \amp -2 \amp -1 \amp 5
			\end{bmatrix}
		</me>.
		First elimination step uses the first row to eliminate below it:
		<me>
				\begin{bmatrix}
					1 \amp -1 \amp -1 \amp 2 \\
					0 \amp 1 \amp 3 \amp 3 \\
					0 \amp -1 \amp 0 \amp 3
				\end{bmatrix}
		</me>.
		Next we use the second row to eliminate below it:
		<me>
			\begin{bmatrix}
				1 \amp -1 \amp -1 \amp 2 \\
				0 \amp 1 \amp 3 \amp 3 \\
				0 \amp 0 \amp 3 \amp 6
			\end{bmatrix}
		</me>.
		Having reached the last row, we turn around and eliminate above it.
		<me>
			\begin{bmatrix}
				1 \amp -1 \amp 0 \amp 4 \\
				0 \amp 1 \amp 0 \amp -3 \\
				0 \amp 0 \amp 3 \amp 6
			\end{bmatrix}
		</me>.
		We move up to the second row and eliminate above that:
		<me>
				\begin{bmatrix}
					1 \amp 0 \amp 0 \amp 1 \\
					0 \amp 1 \amp 0 \amp -3 \\
					0 \amp 0 \amp 3 \amp 6
				\end{bmatrix}
		</me>.
		Finally, to be super pedantic, we normalize each row:
		<me>
				\begin{bmatrix}
					1 \amp 0 \amp 0 \amp 1 \\
					0 \amp 1 \amp 0 \amp -3 \\
					0 \amp 0 \amp 1 \amp 2
				\end{bmatrix}
		</me>.
		The last column is now the solution.
		</p>
		<p>
			The following code performs these operations in MATLAB. Note the square brackets used to define matrices and vectors, and the notation used to access a row or part of a row.
		</p>
		<listing xml:id="la_rowops">
			<caption>Row operations in MATLAB</caption>
			<program language="matlab">
				<input>
					A = [ 1 -1 -1; 3 -2 0; 1 -2 -1 ]
					b = [ 2; 9; 5 ]
					AA = [ A b ]

					AA(2,1:end) = AA(2,1:end) - 3*AA(1,1:end)
					AA(3,1:end) = AA(3,1:end) - AA(1,1:end)

					AA(3,2:end) = AA(3,2:end) + AA(2,2:end)

					AA(2,3:end) = AA(2,3:end) - AA(3,3:end)
					AA(1,3:end) = AA(1,3:end) + (1/3)*AA(3,3:end)

					AA(1,2:end) = AA(1,2:end) + AA(2,2:end)
				</input>
			</program>
		</listing>
	</example>
	<p>
		Before we generalize, let's look at an example that works out differently.
	</p>
	<example xml:id="la-ex-rowelim">
		<p>
		Let's solve the system having augmented matrix
		<me>
			\begin{bmatrix}
				1 \amp 1 \amp -1 \amp 4 \\
				1 \amp 2 \amp 2 \amp 3 \\
				2 \amp 1 \amp -5 \amp 9
			\end{bmatrix}
		</me>.
		Use multiples of the first row to eliminate below it.
		<me>
			\begin{bmatrix}
			1 \amp 1 \amp -1 \amp 4 \\
			0 \amp 1 \amp 3 \amp -1 \\
			0 \amp -1 \amp -3 \amp 1
			\end{bmatrix}
		</me>
		Use a multiple of the second row to eliminate below that.
		<me>
			\begin{bmatrix}
			1 \amp 1 \amp -1 \amp 4 \\
			0 \amp 1 \amp 3 \amp -1 \\
			0 \amp 0 \amp 0 \amp 0
			\end{bmatrix}
		</me>
		We have reached the last row, and it's time to turn around and eliminate upwards. But we can't, because the last row is entirely zero. So skip that and move up to the next row, eliminating upwards again in the second column:
		<me>
			\begin{bmatrix}
			1 \amp 0 \amp -1 \amp 5 \\
			0 \amp 1 \amp 3 \amp -1 \\
			0 \amp 0 \amp 0 \amp 0
			\end{bmatrix}
		</me>.
		This is as simple as we can get things. The last row is telling us the astounding fact that <m>0=0</m>, i.e., nothing at all. The fact that there was nothing there to determine <m>x_3</m> means that we can set it to be anything we like, say <m>x_3=s</m> for arbitrary <m>s</m>. Now the second row says
		<me>
			x_2 = -1 - 3s
		</me>,
		which is as specific as we can get. Finally, the first row says
		<me>
			x_1 = 5 + s
		</me>.
		So <m>[5+s,-1-3s,s]</m> is a solution for every <m>s</m>, and we have infinitely many solutions.
		</p>
	</example>
	<p>
		The process demonstrated in <xref ref="la-ex-rowelim"/> is best known as <idx>Gauss-Jordan elimination</idx> <term>Gauss-Jordan elimination</term>, though I prefer to call it <term>row elimination</term>. As seen in the examples, row elimination consists of two phases, one downward (Gaussian elimination) and one upward. The goal is to put the augmented matrix into a special form.
	</p>
	<p>
		Time for some more formal definitions. We start with the downward phase of the elimination. It's organized a tad differently in order to make everything easier to describe. In what follows we use the term <idx>leading nonzero</idx><term>leading nonzero</term> to mean the first (leftmost) entry of a row that is not zero.
	</p>
	<definition xml:id="la-def-phase1">
		<title>Row elimination Phase 1 (Gaussian elimination)</title>
		<statement>
		<p>
		<ol>
			<li>Set <m>i=1</m>.</li>
			<li>Find the leftmost leading nonzero in rows <m>i</m> and below. The column of this leading nonzero is known as the <term>pivot column</term>. If no such column exists, stop.</li>
			<li>As necessary, swap rows and/or multiply a row by a constant to put a 1 in the pivot column of row <m>i</m>.</li>
			<li>Add multiples of row <m>i</m> to the rows below it in order to put zeros into the pivot column.</li>
			<li>Increment <m>i</m> and return to step 2.</li>
		</ol>
		</p>
		</statement>
	</definition>
	<p>
		At the end of Phase 1 the augmented matrix has a simple form. However, it's not the cleanest form to work with theoretically, so we continue.
	</p>
	<definition xml:id="la-def-phase2">
		<title>Row elimination Phase 2</title>
		<statement>
		<p>
		<ol>
			<li>Set <m>i=m</m> (the number of equations).</li>
			<li>Use multiples of row <m>i</m> to put zeros above the leading 1 in that row.</li>
			<li>Decrement <m>i</m>. If <m>i\gt 1</m>, return to step 2.</li>
		</ol>
		</p>
		</statement>
	</definition>
	<example>
		<p>
			Here we use MATLAB to do the arithmetic in the row operation steps. (Meant to be run in sections.)
		</p>
		<listing xml:id="la_rowelim">
			<caption>Row elimination in matlab.</caption>
			<console language="matlab">
			<input>
A = [
     2    0    4    3 
    -2    0    2  -13
    -4    5   -7  -10 
     1   15    2   -4.5
    ];
b = [ 4; 40; 9; 29 ];

S = [A, b]

%%
S(1,:) = S(1,:)/S(1,1)
S(2,:) = S(2,:) - S(2,1)*S(1,:)

%%
S(3,:) = S(3,:) - S(3,1)*S(1,:);
S(4,:) = S(4,:) - S(4,1)*S(1,:);
S

%%
S(2:3,:) = S([3 2],:)

%%
S(2,:) = S(2,:)/S(2,2);
S(3,:) = S(3,:) - S(3,2)*S(2,:);
S(4,:) = S(4,:) - S(4,2)*S(2,:)

%%
S(3,:) = S(3,:)/S(3,3);
S(4,:) = S(4,:) - S(4,3)*S(3,:);
S(4,:) = S(4,:)/S(4,4)

%%
S(3,:) = S(3,:) - S(3,4)*S(4,:);
S(2,:) = S(2,:) - S(2,4)*S(4,:);
S(1,:) = S(1,:) - S(1,4)*S(4,:)

%%
S(2,:) = S(2,:) - S(2,3)*S(3,:);
S(1,:) = S(1,:) - S(1,3)*S(3,:);

S(1,:) = S(1,:) - S(1,2)*S(2,:)

%%
x = S(:,end)
dif = b - A*x
			</input>
			</console>
		</listing>
	</example>


	<p>
		At the end of Phase 2, the matrix is in a form that we define formally here.
	</p>
	<definition>
		<title>RREF</title>
		<statement>
		<p>A matrix is in <idx>RREF (row reduced echelon form)</idx> <term>RREF</term> (reduced row echelon form) if it meets all of these requirements:
		<ol>
		<li>Any rows of all zeros appear below all nonzero rows.</li>
		<li>The leading nonzero of any row is a one.</li>
		<li>Every leading one is the only nonzero in its column.</li>
		</ol>
		The columns that have leading ones are called <idx>pivot column</idx> <term>pivot columns</term>. The other columns are called <idx>free column</idx> <term>free columns</term>.
		</p>
		</statement>
	</definition>

	<p>
		As seen in the examples above, the RREF of an augmented matrix represents a linear system that we can solve by inspection:
		<ul>
		<li>Ignore all zero rows.</li>
		<li>If a leading one occurs in the last column, the system is inconsistent.</li>
		<li>Otherwise, each variable associated with a free column is assigned to a free parameter (e.g., <m>s</m>, <m>t</m>, etc.).</li>
		<li>Use the pivot columns to solve for their corresponding variables in terms of the free parameters.</li>
		</ul>
	</p>
	<aside>
		<p>
			MATLAB has a command <c>rref</c> to put a matrix in RREF. However, it's not ideal, because MATLAB uses floating point and cannot always produce exact zeros. You can also <url href="https://www.wolframalpha.com/input/?i=rref%20%7B%7B1%2C2%2C3%7D%2C%7B4%2C5%2C6%7D%2C%7B7%2C8%2C9%7D%7D">try Wolfram Alpha</url> for this.
		</p>
	</aside>
 </section>

  <section xml:id="la-vector-algebra">
	  <title>Vector algebra</title>
	  <subsection xml:id="la-vectors">
	  <title>Linear combinations</title>
	  <p>
	  	<notation>
			<usage>\Rn{n}</usage>
			<description>all vectors having <m>n</m> real components</description>
		</notation>
	  	We use <m>\Rn{n}</m> to denote the set of all vectors with <m>n</m> real elements. (Every vector is assumed to have a single-column shape.) If <m>\bx</m> is in <m>\Rn{n}</m>, then for any real number <m>c</m> we define <m>c\bx</m> by elementwise multiplication. Because this just scales the magnitude of the vector, a real value is often called a <term>scalar</term><idx>scalar</idx>. If <m>\bx</m> and <m>\by</m> are in <m>\Rn{n}</m>, then we can add or subtract them elementwise. (Addition of a scalar and a vector, and addition of vectors of different lengths, are undefined.)
	  </p>
	  <p>
	  Scaling and addition are often rolled together into a singly named operation.
	  </p>
	  <definition>
		<title>Linear combination</title>
		<statement>
		<p>A <idx>linear combination</idx> <term>linear combination</term> of vectors <m>\ba_1,\ldots,\ba_k</m>, all in <m>\Rn{n}</m>, is
		<men xml:id="la-va-lincomb">
		  x_1 \ba_1 + x_2 \ba_2 + \cdots + x_k \ba_k = \sum_{j=1}^k x_j \ba_j
		</men>,
		where the <m>x_j</m> are scalars known as <term>coefficients</term> (or <term>weights</term>).
		</p></statement>
	  </definition>
		<p>
	  		One motivation for this definition is to express linear systems.
		</p>
	<example>
		<p>
			Consider the system
			<md>
				<mrow>  x_1 - x_2  - x_3 \amp = 2 </mrow>
				<mrow> 3x_1 - 2x_2 \amp = 9 </mrow>
				<mrow> x_1 - 2x_2 - x_3 \amp = 5 </mrow>
			</md>.
			Interpreting vector equalities elementwise, this is equivalent to
			<me>
				x_1 \threevec{1}{3}{1} + x_2 \threevec{-1}{-2}{-2} + x_3 \threevec{-1}{0}{-1} = \threevec{2}{9}{5}
			</me>.
			We can identify the three vectors on the left side as the columns of the coefficient matrix <m>\bA</m>, and the one on the right is <m>\bb</m>. In an earlier example, we found the solution <m>\bx=[1,-3,2]</m>. Hence
			<me>
				\ba_1 - 3 \ba_2 + 2\ba_3 = \bb
			</me>,
			where we gave names to the columns. In MATLAB syntax,
		</p>
		<listing xml:id="la_lincombination">
			<caption>Linear combination of columns in MATLAB.</caption>
			<program language="matlab">
			<input>
				A = [ 1 -1 -1; 3 -2 0; 1 -2 -1 ]
				b = [ 2; 9; 5 ]
				x = [1; -3; 2 ]
				x(1)*A(:,1) + x(2)*A(:,2) + x(3)*A(:,3)
			</input>
			</program>
		</listing>
	</example>
	</subsection>
	<subsection>
		<title>Matrix times vector</title>
		<p>
			The linear combination definition <xref ref="la-va-lincomb"/> actually serves as the foundation of multiplication between a matrix and a vector. We use <m>\Rmn{m}{n}</m> to mean the set of all matrices with <m>m</m> rows and <m>n</m> columns.
		</p>
		<definition>
			<title>Matrix times vector</title>
			<notation>
			  <usage>\Rmn{m}{n}</usage>
			  <description>All <m>m\times n</m> matrices with real elements</description>
			</notation>
			<statement>
			<p>
				Given <m>\bA\in\Rmn{m}{n}</m> and <m>\bx\in\Rn{n}</m>, the product <m>\bA\bx</m> is defined as
				<men xml:id="la-mv-matvec">
		  			\bA\bx = x_1 \ba_1 + x_2 \ba_2 + \cdots + x_n \ba_n = \sum_{j=1}^n x_j \ba_j
				</men>.
			</p>
			</statement>
		</definition>
		<example> 
		 <p>
		   The product
			<me>
				\begin{bmatrix} 
					1 \amp -1 \amp -1 \\ 3 \amp -2 \amp 0 \\ 1 \amp -2 \amp -1 
				\end{bmatrix} \threevec{-1}{2}{-1} 
			</me> 
			is equivalent to 
		  	<me>
				(-1) \threevec{1}{3}{1} + (2) \threevec{-1}{-2}{-2} + (-1) \threevec{-1}{0}{-1} = \threevec{-2}{-7}{-4}
			</me>.
			We often don't write out the product in this much detail. Instead we "zip together" the rows of the matrix with the entries of the vector: 
			<me>
				\threevec{(-1)(1)+(2)(-1)+(-1)(-1)}{(-1)(3)+(2)(-2)+(-1)(0)}{(-1)(1)+(2)(-2)+(-1)(-1)}  = \threevec{-2}{-7}{-4}
			</me>. You might recognize the expressions in this vector as dot products from vector calculus, but that's a whole 'nother story.
		 </p>  
		</example>
		<p>
			What justifies calling this operation multiplication? In large part, it's the natural distributive properties
			<md>
				<mrow> \bA(\bx+\by) \amp =  \bA\bx + \bA\by</mrow>
				<mrow> (\bA+\bB)\bx \amp =  \bA\bx + \bB\bx</mrow>
			</md>
			(where matrix addition is also defined elementwise), which can be checked with just a little determination. It's also true that <m>\bA(c\bx)=c(\bA\bx)</m>. However, <alert>the product <m>\bA\bx</m> is <em>not</em> commutative; <m>\bx\bA</m> is not even defined.</alert>
		</p>
		<p>
			As a matter of fact, these properties combine in a way worth noting. We can define a function <m>L</m> on vectors by <m>L(\bx)=\bA\bx</m>. It has very important properties that we want to single out by name. 
		</p>
		<definition xml:id="la-def-linearfunction">
			<title>Linear function</title>
			<statement><p>
				A function <m>L</m> from <m>\Rn{n}</m> to <m>\Rn{m}</m> is a <idx>linear function</idx><term>linear function</term> if 
				<me>
					L(c\bx + \by)=cL(\bx)+L(\by)
				</me>
				for all <m>n</m>-vectors <m>\bx,\by</m> and scalars <m>c</m>.
			</p></statement>
		</definition>
		<p>
			This property would not hold for <m>L(\bx)=x_1x_2</m>, for instance. But that nonlinear function also cannot be described using a matrix-vector multiplication. 
		</p>
	</subsection>
</section>

  <section xml:id="la-matrix-algebra">
	<title>Matrix algebra</title>
	<subsection>
	<title>Matrix multiplication</title>
	<p>Addition and scalar multiplication of matrices is exactly the same as for vectors: elementwise. Things get interesting when we get to matrix products.</p>
	<p>
		In order to define the product <m>\bA\bB</m>, we require that the number of <em>columns</em> in <m>\bA</m> is the same as the number of <em>rows</em> in <m>\bB</m>. We say that the <em>inner dimensions</em> must agree; otherwise, the product is considered undefined.
	</p>
	<definition>
		<title>Matrix times matrix</title>
		<statement>
			<p>If <m>\bA</m> is <m>m\times n</m> and <m>\bB</m> is <m>n\times p</m>, then the product <m>\bA\bB</m> is defined as
				<men xml:id="la-eq-matmat">
					\bA\mathbf{B} =
						\bA \begin{bmatrix}
						\mathbf{b}_1 \amp \mathbf{b}_2 \amp \cdots \amp \mathbf{b}_p
						\end{bmatrix}
						= \begin{bmatrix}
						\bA\mathbf{b}_1 \amp \bA\mathbf{b}_2 \amp \cdots \amp \bA\mathbf{b}_p
						\end{bmatrix}
				</men>.
			</p>
		</statement>
	</definition>
	<p>In words, a matrix-matrix product is the horizontal concatenation of matrix-vector products involving the columns of the right-hand matrix. Note that when we multiply <m>m\times n</m> by <m>n\times p</m>, the result is <m>m\times p</m>; that is, the <em>outer dimensions</em> remain.</p>
	<p>
	When we compute a matrix product by hand, we usually don't write out the above. Instead we use a more compact definition for the individual entries of <m>\bC = \bA\bB</m>,
	<men xml:id="la-eq-matmatinner">
		C_{ij} = \sum_{k=1}^n a_{ik}b_{kj}, \qquad i=1,\ldots,m, \quad j=1,\ldots,p
	</men>.
	</p>
	<example>
		<p>
		Let
		<me>
			\bA = \begin{bmatrix}
				1 \amp -1 \\ 0 \amp 2 \\ -3 \amp 1
				\end{bmatrix}, \qquad
				\mathbf{B} = \begin{bmatrix}
				2 \amp -1 \amp 0 \amp 4 \\ 1 \amp 1 \amp 3 \amp 2
				\end{bmatrix}
		</me>.
		Then
		<md>
			<mrow>\bA\mathbf{B} \amp= \begin{bmatrix}
				(1)(2) + (-1)(1) \amp (1)(-1) + (-1)(1) \amp (1)(0) + (-1)(3) \amp (1)(4) + (-1)(2) \\
				(0)(2) + (2)(1) \amp (0)(-1) + (2)(1) \amp (0)(0) + (2)(3) \amp (0)(4) + (2)(2) \\
				(-3)(2) + (1)(1) \amp (-3)(-1) + (1)(1) \amp (-3)(0) + (1)(3) \amp (-3)(4) + (1)(2)
				\end{bmatrix} </mrow>
			<mrow>\amp = \begin{bmatrix}
						1 \amp -2 \amp -3 \amp 2 \\ 2 \amp 2 \amp 6 \amp 4 \\ -5 \amp 4 \amp 3 \amp -10
					\end{bmatrix}</mrow>
		</md>.
		</p>
		<p>
		   Observe that
			<me>
				\bA \begin{bmatrix} 2 \\ 1 \end{bmatrix} = 2 \begin{bmatrix} 1 \\ 0 \\ -3
					\end{bmatrix} + 1 \begin{bmatrix} -1 \\ 2 \\ 1 \end{bmatrix}
					= \begin{bmatrix} 1 \\ 2 \\ -5 \end{bmatrix}
			</me>,
			and so on.
		</p>
		<p>
			MATLAB interprets the <c>*</c> operator to mean multiplication in the sense of matrices and vectors.
		</p>
		<listing xml:id="la_matrixmult">
			<caption>Matrix multiplication in MATLAB.</caption>
			<program language="matlab">
				<input>
					A = [ 1 -1; 0 2; -3 1 ]
					B = [ 2 -1 0 4; 1 1 3 2 ]

					A*B

					A(3,:)*B(:,1)
				</input>
			</program>
		</listing>
	 </example>
	 </subsection>
	 <subsection>
		<title>Properties</title>
		<p>
			 First, the bad news. In the previous subsection we computed the product <m>\bA\bB</m>, where the matrices are <m>3\times 2</m> and <m>2\times 4</m>, to get a result that is <m>3\times 4</m>. However, the product <m>\bB\bA</m> isn't even defined, because <m>4\neq 3</m>. Moreover, even when both <m>\bA\bB</m> and <m>\bB\bA</m> <em>are</em> defined, we cannot automatically expect them to be equal. You may have to unlearn some very old habits.
		</p>
		<aside><p>In a product of matrices (and vectors), position matters. You cannot change the ordering without an explicit justification!</p></aside>
		<fact>
			 <p>Matrix multiplication is not commutative. That is, the order of terms in a product affects the result.</p>
		</fact>
		<p>
		Fortunately, other familiar properties of multiplication do come along for the ride:
		<ol>
			<li><m>(\bA\bB)\bC=\bA(\bB\bC)</m></li>
			<li><m>\bA(\bB+\bC) = \bA\bB + \bA\bC</m></li>
			<li><m>(\bA+\bB)\bC = \bA\bC + \bB\bC</m></li>
			<!--li><m>\alpha(\bB+\bC) = \alpha\bB + \alpha\bC</m></li-->
		</ol>
		</p>
		<example>
			<p>
				These properties are easy to demonstrate (not prove!) in MATLAB. Any undefined matrix operation throws an error.
			</p>
			<listing xml:id="la_algprops">
				<caption>Demonstration of matrix algebra.</caption>
				<program language="matlab">
					<input>
						A = [ 1 -1; 0 2; -3 1 ]
						B = [ 2 -1 0 4; 1 1 3 2 ]

						A*B
						%B*A  % causes an error

						size(B)
						size(A)

						C = [ 1 -1 2; 2 2 0; 5 -2 -3; 4 -1 -1 ]
						size(C)

						(A*B)*C - A*(B*C)

						A = round(10*rand(4,4))
						B = round(10*rand(4,4))
						C = round(10*rand(4,4))

						( A*(B+C) ) - ( A*B + A*C )

						( (A+B)*C ) - ( A*C + B*C )
					</input>
				</program>
			</listing>
		</example>
	</subsection>
	<subsection>
	<title>Identity matrix</title>
		<p>
			It's pretty immediate that for any square matrix <m>\bA</m>, <m>\bA\boldsymbol{0}=\boldsymbol{0}\bA=\boldsymbol{0}</m> for the all-zero square matrix <m>\boldsymbol{0}</m> of appropriate size. So we have a multiplicative zero. But the situation is a bit less obvious when it comes to a multiplicative unit element (the counterpart of the scalar 1).
		</p>
		<definition>
			<title>Identity matrix</title>
			<notation>
			  <usage>\bI</usage>
			  <description>identity matrix</description>
			</notation>
			<statement>
			<p>The <m>n\times n</m> <idx>identity matrix</idx> <term>identity matrix</term> is
				<me>
						\bI = \begin{bmatrix}
						1 \amp 0 \amp 0 \amp \cdots \amp 0 \\
						0 \amp 1 \amp 0 \amp \cdots \amp 0 \\
							\vdots \amp  \amp \ddots \amp  \amp \vdots\\
							0 \amp \cdots \amp 0 \amp 1 \amp 0 \\
							0 \amp \cdots \amp 0 \amp 0 \amp 1
							\end{bmatrix}
				</me>.
			</p>
			</statement>
		</definition>
		<p>
		The definition of the identity is such that <m>\bA\bI=\bA</m> for all <m>\bA\in\Rmn{m}{n}</m> and <m>\bI\bB=\bB</m> for all <m>\bB\in\Rmn{n}{p}</m>. This includes vectors, i.e., <m>\bI\bx=\bx</m> for all <m>\bx \in \Rn{n}</m>.
		</p>
		<p>
			The MATLAB command to create an identity matrix is <c>eye</c>.
		</p>
</subsection>
</section>

<section xml:id="la-general-solution">
	<title>General and particular solutions</title>
	<introduction>
		<p>
		Our next step is to observe that solutions to <m>\bA\bx=\bb</m> can all be expressed as <m>\bx=\bx_h+\bx_p</m>, with the two parts belonging to different sets. The same thing is going to happen when we study differential equations, pretty much for exactly the same reasons.
		</p>
	</introduction>
	<subsection>
		<title>Homogeneous solutions</title>
		<p>
		The linear system <m>\bA\bx=\bzero</m> plays a special role and is called a <idx><h>homogeneous</h> <h>linear system</h></idx> <term>homogeneous linear system</term>.
		</p>
		<theorem xml:id="la-gs-homolincomb">
			<title>Homogeneous linear systems</title>
			<statement><p>
				If <m>\bx_1,\ldots,\bx_k</m> are solutions of a homogeneous system <m>\bA\bx=\bzero</m>, then every linear combination of them is also a solution.
			</p></statement>
			<proof><p>
				Let <m>c_1\bx_1 + \cdots + c_k\bx_k</m> be such a linear combination. Then
				<md>
					<mrow>\bA(c_1\bx_1 + \cdots + c_k\bx_k) \amp = \bA(c_1 \bx_1 ) + \cdots + \bA(c_k \bx_k) </mrow>
					<mrow>\amp = c_1\bA\bx_1 + \cdots + c_k\bA \bx_k </mrow>
					<mrow>\amp = \bzero</mrow>
				</md>.
			</p></proof>
		</theorem>
		<p>
			By itself this is nice, but what makes this theorem really important is that <em>every</em> homogeneous solution can obtained this way. 
		</p>
		<example xml:id="la-ex-33homog">
			<p>
				Suppose
				<me>
					\bA =
					\begin{bmatrix}
					0 \amp -2 \amp 4 \\ 0 \amp 1 \amp -2 \\ 0 \amp 3 \amp -6
					\end{bmatrix}
				</me>.
				The RREF of the augmented matrix <m>[\bA,\bzero]</m> is found to be
				<me>
						\bA =
						\begin{bmatrix}
						0 \amp 1 \amp -2 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0
						\end{bmatrix}
				</me>.
				Hence the only pivot column is column 2, and all solutions are of the form
				<me>
					\threevec{s}{2t}{t} = s \threevec{1}{0}{0} + t \threevec{0}{2}{1}
				</me>,
				where <m>s</m> and <m>t</m> are arbitrary scalars. You can easily check that <m>[1,0,0]</m> and <m>[0,2,1]</m> are each solutions of <m>\bA\bx=\bzero</m>.
			</p>
		</example>
		<p>
			Motivated by the theorem and example above, we make a new definition.
		</p>
		<definition xml:id="la-gs-homogen">
			<title>General solution of a linear system (homogeneous)</title>
			<notation>
			  <usage>\bx_h</usage>
			  <description>homogeneous solution (linear algebra)</description>
			</notation>
			<statement><p>
				We say that <m>\bx_h = c_1\bx_1 + \cdots + c_k\bx_k</m> is the <term>general solution</term><idx><h>general solution</h><h>of homogeneous linear system</h></idx> of <m>\bA\bx=\bzero</m> if every solution to the system can be so written for some choice of the coefficients.
			</p></statement>
		</definition>
	</subsection>
	<subsection xml:id="la-gs-particular">
	<title>Particular solutions</title>
		<p>
			We now come to one of the simplest yet most confusing terms in the course. A <term>particular solution</term><idx><h>particular solution</h><h>of a linear system</h></idx> of the linear system <m>\bA\bx=\bb</m> is just any one solution of the problem. The only reason the term exists is to distinguish it from the <em>general</em> solution, which (as above) is an expression for <em>every possible</em> solution of the system.
			<notation>
			  <usage>\bx_p</usage>
			  <description>particular solution (linear algebra)</description>
			</notation>
		</p>
		<theorem xml:id="la-gs-general">
			<title>General solution of a linear system</title>
			<statement><p>
				All solutions of <m>\bA\bx=\bb</m> may be written as
				<me>\bx = \bx_h + \bx_p</me>,
				where <m>\bx_h</m> is the general solution of <m>\bA\bx=\bzero</m> and <m>\bx_p</m> is any particular solution of <m>\bA\bx=\bb</m>. We call this the <term>general solution</term><idx><h>general solution</h><h>of a linear system</h></idx> of <m>\bA\bx=\bb</m>.
			</p></statement>
			<proof><p>
				Let <m>\bx_p</m> be a particular solution, and suppose <m>\bu</m> is another solution. Then
				<me>\bA(\bu-\bx_p)=\bA\bu - \bA\bx_p = \bb - \bb = \bzero</me>.
				Hence <m>\bu-\bx_p</m> is a homogeneous solution, which means that we can write it as <m>\bx_h</m>.
			</p></proof>
		</theorem>
		<p>
			Thus, one way to attack <m>\bA\bx=\bb</m> is to first find the general homogeneous solution, then find any particular solution of the nonhomogeneous problem. You will do this many times over for differential equations in future chapters.
		</p>
		<example>
			<p>
				We continue with the matrix <m>\bA</m> from <xref ref="la-ex-33homog"/>, now with <m>\bb=[-8,4,12]</m>. The RREF of the augmented matrix works out to be
				<me>
					\begin{bmatrix}
					0 \amp 1 \amp -2 \amp 4 \\ 0 \amp 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0
					\end{bmatrix}
				</me>.
				The general homogeneous solution is found, as before, by replacing the last column with zeros to get 
				<me>\bx_h =	s \threevec{1}{0}{0} + t \threevec{0}{2}{1}</me>.
				For the particular solution we have free variables from columns 1 and 3. But since we can use <em>any</em> particular solution, we will take the easy route and set the free variables to zero. Then the first row tells us that <m>x_2=4</m>, and we get the general solution
				<me>\bx = \threevec{0}{4}{0} + s \threevec{1}{0}{0} + t \threevec{0}{2}{1}</me>.
			</p>
		</example>
		<p>
			This process doesn't really save us any algebraic work right now, because we would have gotten the same expression for the solution directly from the RREF. So you might be wondering what the point of this section is. Have patience, grasshopper.
		</p>
	</subsection>
</section>

<section xml:id="la-basis">
	<title>Basis and dimension</title>
	<subsection>
		<title>Span and independence</title>
		<p>In the last section we saw that solutions of <m>\bA\bx=\bzero</m> play an important role in all linear systems.</p>
		<definition xml:id="la-df-nullspace">
			<title>Nullspace</title>
			<notation>
			  <usage>\null(\bA)</usage>
			  <description>nullspace of a matrix</description>
			</notation>
			<statement><p>
				The <term>nullspace</term><idx>nullspace</idx> of a matrix <m>\bA\in\Rmn{m}{n}</m> is
				<men xml:id="la-eq-nullspace">
					\null(\bA) = \{ \bx\in\Rn{n} : \bA\bx=\bzero \}
				</men>.
			</p></statement>
		</definition>
		<aside>
			<p>
				The term <em>space</em> itself has a technical definition, and there are many different types of spaces. But the nullspace is the only one we will need. 
			</p>
		</aside>
		<p>We also saw that every element of <m>\null(\bA)</m> can be expressed as a linear combination, <m>c_1\bx_1+\cdots+c_k\bx_k</m>.</p>
		<definition xml:id="la-df-span">
			<title>Span</title>
			<notation>
			  <usage>\span(\bx_1,\ldots,\bx_k)</usage>
			  <description>span of vectors</description>
			</notation>
			<statement><p>
				The <term>span</term> of vectors <m>\bx_1,\ldots,\bx_k</m> is
				<men xml:id="la-eq-span">
					\span(\bx_1,\ldots,\bx_k) = \{ c_1\bx_1+\cdots+c_k\bx_k : c_1,\ldots,c_k \in \mathbb{R} \}
				</men>.
			</p></statement>
		</definition>
		<p>
			The notion of span gives us a shorthand for expressing the nullspace (equivalently, the general homogeneous solution). For reasons we won't get into, every span is referred to as a space. However, it's important to know that there are infinitely many different ways to express the same space. For instance,
			<me>\span\left(\bx_1\right) = \span\left(\bx_1,\bx_1,\bx_1\right)</me>.
			That's kind of an obvious <q>cheat</q>. Here is a more subtle one:
			<me>
				\span\left( \twovec{1}{0}, \twovec{0}{1}, \twovec{1}{1} \right) = \span\left( \twovec{1}{0}, \twovec{0}{1} \right)
			</me>.
			The reasoning here is that the first two vectors already span all of <m>\Rn{2}</m>, so there's nothing more that the third vector can bring to the table. We have a term for these observations.
		</p>
		<definition xml:id="la-df-independence">
			<title>Linear independence</title>
			<statement><p>
				The vectors <m>\bx_1,\dots,\bx_k</m> are <term>linearly dependent</term> if there are scalars <m>c_1,\ldots,c_k</m>, not all zero, such that
				<me>c_1\bx_1+\cdots+c_k\bx_k = \bzero</me>.
				Otherwise the vectors are <term>linearly independent</term>.
			</p></statement>
		</definition>
		<p>
			Why is this the right concept? Well, if the vectors are dependent, then by definition there is some <m>j</m> for which we can write
			<me> c_j \bx_j = - \sum_{i\neq j} c_i \bx_i</me>,
			where <m>c_j\neq 0</m>. If we divide this through by <m>c_j</m>, then we have shown that <m>\bx_j</m> lies within the span of all the other vectors, and therefore including it with linear combinations is redundant.
		</p>
	</subsection>
	<subsection xml:id="la-ba-homogeneous">
		<title>Independence and homogeneous systems</title>
		<p>
			Matters of linear independence can be restated in terms of homogeneous linear systems (and vice versa). That is because of the equivalence between linear combinations of vectors and matrix-vector multiplication: 
			<me>
				\sum_{j=1}^k c_j \bx_j = c_1\bx_1+\cdots+c_k\bx_k = \begin{bmatrix} \bx_1 \amp \cdots \amp \bx_k \end{bmatrix} \begin{bmatrix} c_1 \\ \vdots \\ c_k \end{bmatrix} = \bX \bc. 
			</me>
			Hence the question <q>Are <m>\bx_1,\dots,\bx_k</m> dependent?</q> is equivalent to <q>Does <m>\bX\bc=\bzero</m> have any nonzero solutions?</q> Because homogeneous problems always have at least one solution (the zero vector), this question is also equivalent to <q>Does <m>\bX\bc=\bzero</m> have infinitely many solutions?</q>
		</p>
		<example>
			<p>
				Suppose we want to check the independence of 
				<me>
					\bx_1 = \threevec{1}{2}{-1}, \quad \bx_2 = \threevec{2}{4}{-2}, \quad \bx_3 = \threevec{-1}{1}{4}
				</me>. 
				We assemble these as the columns of a matrix 
				<me>
					\begin{bmatrix}
					1 \amp 2 \amp -1 \\
					2 \amp 4 \amp 1 \\
					-1 \amp -2 \amp 4
					\end{bmatrix} 
				</me>. 
				Technically we ought to tack on a zero column to get the augmented matrix, but row operations can never change that column. So we can find the RREF without it: 
				<me>
					\begin{bmatrix}
					1 \amp 2 \amp -1 \\
					2 \amp 4 \amp 1 \\
					-1 \amp -2 \amp 4
					\end{bmatrix} \mapsto
					\begin{bmatrix}
					1 \amp 1 \amp -1 \\
					0 \amp 0 \amp 3 \\
					0 \amp 0 \amp 3
					\end{bmatrix} \mapsto
					\begin{bmatrix}
					1 \amp 1 \amp 0 \\
					0 \amp 0 \amp 1 \\
					0 \amp 0 \amp 0
					\end{bmatrix}
			   </me>.
			   Thus the homogeneous problem will have at least one free parameter. 
			</p>
		</example>
	</subsection>
	<subsection xml:id="la-ba-basis">
	<title>Basis</title>
		<p>Put span and independence together and you get something good.</p>
		<definition xml:id="la-df-basis">
			<title>Basis</title>
			<statement><p>
				If <m>\bx_1,\dots,\bx_k</m> are independent, then they form a <term>basis</term> of <m>\span(\bx_1,\dots,\bx_k)</m>.
			</p></statement>
		</definition>
		<p>
			The following can be proved as a theorem. 
		</p>
		<fact xml:id="la-fact-dimension">
			<statement>
				<p>
					Every basis of a given space has the same number of vectors in it.
				</p>
			</statement>
		</fact>
		<p>
			This number is called the <idx>dimension</idx><term>dimension</term> of the space. 
		</p>
		<example>
				<p>We find the RREF of a <m>3\times 3</m> matrix:
				<me>
					 \bA = \begin{bmatrix}
					 1 \amp 2 \amp -1 \\
					 2 \amp 4 \amp 1 \\
					 -1 \amp -2 \amp 4
					 \end{bmatrix} \mapsto
					 \begin{bmatrix}
					 1 \amp 1 \amp -1 \\
					 0 \amp 0 \amp 3 \\
					 0 \amp 0 \amp 3
					 \end{bmatrix} \mapsto
					 \begin{bmatrix}
					 1 \amp 1 \amp 0 \\
					 0 \amp 0 \amp 1 \\
					 0 \amp 0 \amp 0
					 \end{bmatrix}
				</me>.
				Columns 1 and 3 are the pivot columns. We regard <m>x_2</m> as a free variable; say <m>x_2=s</m>. Then <m>x_3=0</m> and <m>x_1=-s</m>, and the homogeneous general solution is
				<me>
					 \bx_h = \begin{bmatrix} -s \\ s \\ 0 \end{bmatrix} = s \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}
				</me>.
				Equivalently, we say that
				<me>\null(\bA) = \span\left(\begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}\right)</me>,
				and in fact <m>[-1,1,0]</m>  is a basis of the nullspace, which is therefore one-dimensional.
				</p>
				<p>
					MATLAB has a <c>null</c> command that produces a basis of the nullspace.
				</p>
				<listing xml:id="la_nullspace">
					<program language="matlab">
						<input>
							A = [ 1 1 -1; 2 2 1; -1 -1 4 ]
							null(A)
						</input>
					</program>
				</listing>
				<p>
					This is equivalent to the answer we gave above, just with a different normalization of the vector.
				</p>
		  </example>
		  <example>
				<p>
					 Suppose the RREF of a matrix ends with
					 <me>
					 \begin{bmatrix}
					 1 \amp 0 \amp 0 \\
					 0 \amp 1 \amp 0 \\
					 0 \amp 0 \amp 1
					 \end{bmatrix}
					 </me>
					 (i.e., an identity matrix). There are no free variables, and the only solution is <m>\bx=[0,0,0]</m>. Hence <m>\null(\bA) = \{\bzero\}</m>. This singleton set does not have a basis and is considered to have dimension zero. (Try this example in MATLAB!)
				</p>
		  </example>
		  <p>
			  I emphasize that <em>we have done no new math here</em>. It's still just about linear systems and RREF matrices, with different terminology.
		  </p>
	</subsection>
</section>

<!--section xml:id="la-subspace">
<title>Subspaces</title>
</section-->

  <!--section xml:id="la-basis">
	  <title>Bases</title>
  </section-->

<section xml:id="la-inverse">
	<title>Inverses</title>
	<subsection>
		<title>Inverse matrix</title>
		<p>
			For scalars, the multiplicative inverse of <m>a</m> is the reciprocal, <m>a^{-1}</m>, except that zero has no inverse. Now we investigate the same question for square matrices.
		</p>
		<definition>
			<title>Inverse matrix</title>
			<notation>
				<usage>\bA^{-1}</usage>
				<description>inverse of a (square) matrix</description>
			</notation>
			<statement>
				<p>The <term>inverse</term> of an <m>n\times n</m> matrix <m>\bA</m> is a matrix <m>\bC\in\Rmn{n}{n}</m> such that <m> \bA\bC = \bC\bA = \bI </m>. If such a <m>\bC</m> exists, we write <m>\bA^{-1}=\bC</m> and say that <m>\bA</m> is <term>invertible</term>; otherwise, it is <term>singular</term>.</p>
			</statement>
		</definition>
		<aside><p>Inverses are possible for square matrices only. The concept does not directly translate to rectangular matrices.</p></aside>
		<p>
			From the start there are two important things to appreciate about inverses. 
		</p>
		<fact xml:id="la-fact-inverse">
			<statement>
				<p>
					<ol>
					  <li>
						<p>
						  If a matrix has an inverse, it is unique. (So it makes sense to talk about <q>the</q> inverse of a matrix.)
						</p>
					  </li>
					  <li> <m>\bA\mathbf{Z}=\bI</m> if and only if <m>\mathbf{Z}\bA=\bI</m>. (So we don't need to separately think about left-inverse and right-inverse.)   </li>
					</ol>
				</p>
			</statement>
		</fact>
		<p>
		Because the zero matrix gives zero for every multiplicative partner, it cannot have an inverse. However, unlike scalars, <alert>there are nonzero matrices that are singular</alert>. The simplest is
		<me>
			\bA = \begin{bmatrix}  0 \amp 1 \\ 0 \amp 0 \end{bmatrix}
		</me>.
		(Proof: The first column of <m>\bC\bA</m> is zero for any square <m>\bC</m>.)
		</p>
	</subsection>
	<subsection>
		<title>Inverses and linear systems</title>
		<p>
			If <m>\bA\in\Rmn{n}{n}</m> is invertible, then we can use its inverse to solve a linear system <m>\bA\bx=\bb</m>:
			<me>
				\bA^{-1} \bb = \bA^{-1} (\bA \bx) = \bigl(  \bA^{-1} \bA \bigr) \bx = \bI \bx = \bx
			</me>.
			The uniqueness of the inverse means that this solution is unique as well. We will see later on that the converse is also true: unique solution implies invertibility.
		</p>
		<p>
			It's one thing to say <q>the inverse exists</q> and quite another to say <q>here it is.</q> Finding the inverse of a given matrix is in principle a matter of solving <m>n</m> linear systems. That's because if we let <m>\mathbf{e}_j</m> be the <m>j</m>th column of <m>\bI</m>, then according to the columnwise interpretation of <m>\bA\bA^{-1}</m>, the solution of <m>\bA \bx = \mathbf{e}_j</m> is the <m>j</m>th column of <m>\bA^{-1}</m>.
		</p>
		<p>
			Row elimination can be used to solve those systems simultaneously. However, it's all beside the point. If our goal is to solve a linear system <m>\bA\bx=\bb</m>, then there is little reason to solve <m>n</m> other systems first to get the inverse! The inverse tends to be much more important theoretically than for practical computation.
		</p>
		<example>
			<p>
				MATLAB does have an <c>inv</c> command to find an inverse. However, it's not considered the best way to solve a linear system (it will be much slower for large matrices).
			</p>
			<listing xml:id="la_linsolve">
				<program language="matlab">
					<input>
						A = magic(3)
						b = [ 1; 6; -2 ]

						x = A\b
						b - A*x
					</input>
				</program>
			</listing>
			<p>
				MATLAB works with 16-digit approximations of real numbers, so things that are truly equal in exact arithmetic may only agree to many significant digits. Usually it's best to take the difference of two things to test near-equality.
			</p>
		</example>
	</subsection>
	<subsection>
		<title><m>2\times 2</m> inverses</title>
		<p>
			There does happen to be a convenient formula for the matrix inverse in the <m>2\times 2</m> case:
			<men xml:id="la-eq-twotwoinv">
				\begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix}^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d \amp -b \\ -c \amp a \end{bmatrix}
			</men>.
			This formula breaks down if <m>ad=bc</m>, in which case the matrix is singular. You might remember that this is the same algebraic condition quoted for the <m>2\times 2</m> linear system <m>\bA\bx=\bb</m> not having a unique solution.
		</p>
	</subsection>
</section>

<section xml:id="la-determinant">
	<title>Determinants</title>
	<introduction>
		<p>
			You should have seen some <m>2\times 2</m> and <m>3\times 3</m> determinants before now, in vector calculus. The <m>2\times 2</m> case is easy to describe:
			<me>\det\left( \twomat{a}{b}{c}{d} \right) = \twodet{a}{b}{c}{d} = ad-bc</me>.
			This can be extended to create a real-valued function for square matrices of any size. The formalities can be complicated, but we are going to use a practical definition.
		</p>
		<aside><p>Here and for the rest of this chapter, we are going to deal with square matrices only.</p></aside>
		<definition xml:id="la-df-det">
			<title>Determinant</title>
			<notation>
			<usage>\det(\bA),\,\bigl| \bA \bigr|</usage>
			<description>determinant of a (square) matrix</description>
			</notation>
			<statement><p>
				If <m>\bA\in\Rmn{n}{n}</m>, then
				<me>
					\det(\bA) = \sum (-1)^{i+j} a_{ij} \det( \mathbf{M}_{ij} )
				</me>,
				where the sum is taken over any row or column of <m>\bA</m>, and <m>\mathbf{M}_{ij}</m> is the matrix that results from deleting row <m>i</m> and column <m>j</m> from <m>\bA</m>.
			</p></statement>
		</definition>
		<p>
			The definition, which is called <term>cofactor expansion</term>, is recursive: the <m>n\times n</m> case is defined in terms of the <m>(n-1)\times (n-1)</m> case, and so on all the way back down to <m>2\times 2</m>.
		</p>
		<example>
			<p>
				Using expansion along the first row,
				<md>
				  <mrow> \begin{vmatrix} 2 \amp 0 \amp -1 \\ -2 \amp 3 \amp -1 \\ 2 \amp 0 \amp  1 end{vmatrix} \amp =  (2) \twodet{3}{-1}{0}{1} - (0) \twodet{-2}{-1}{2}{1} + (-1)\twodet{-2}{3}{2}{0}    </mrow>
				  <mrow>  \amp = 2(3-0) + (-1)(0-6) = 12. </mrow>
				</md>
				In this case it might have been a tad easier to exploit the zeros by expanding along the second column instead:
				<md>
				  <mrow> \begin{vmatrix} 2 \amp 0 \amp -1 \\ -2 \amp 3 \amp -1 \\ 2 \amp 0 \amp  1 end{vmatrix} \amp =  -(0) [\cdots] + (3) \twodet{2}{-1}{2}{1} - (0)[\cdots]    </mrow>
				  <mrow>  \amp = 3(2+2) = 12. </mrow>
				</md>
			</p>
		</example>
		<p>
			There are a few facts about determinants that are good to know.
		</p>
		<fact>
			<p>
			Let <m>\bA</m> and <m>\bB</m> be <m>n\times n</m>, and let <m>c</m> be a scalar. Then
			<ol>
				<li><m>\det(c\bA) = c^n \det(\bA)</m>,</li>
				<li><m>\det(\bA\bB) = \det(\bA)\det(\bB)</m>,</li>
				<li><m>\det(\bA)=0</m> if and only if <m>\bA</m> is singular, and</li>
				<!--li>If <m>\bA</m> is nonsingular, <m>\det(\bA^{-1})=1/\det(\bA)</m>.</li-->
			</ol>
			</p>
		</fact>
		<p>
			It's the third one that we will be using. The determinant is often the easiest way to check for singularity of a small matrix by hand.
		</p>
	</introduction>
	<subsection xml:id="la-det-cramer">
		<title>Cramer's Rule</title>
		<p>
			Even though a 2x2 inverse is easy, it's still not the most convenient way to solve a linear system <m>\bA\bx=\bb</m>. There is an even faster equivalent shortcut known as <idx><h>Cramer's Rule</h></idx> <term>Cramer's Rule</term>:
			<md>
				<mrow>x_1 \amp = \frac{ \twodet{b_1}{a_{12}}{b_2}{a_{22}} }{ \det(\bA) }</mrow>
				<mrow>x_2 \amp = \frac{ \twodet{a_{11}}{b_1}{a_{21}}{b_2} }{ \det(\bA) }</mrow>
			</md>.
			Obviously this does not work if <m>\det(\bA)=0</m>. In that case the matrix is singular, and it's best to investigate by row elimination as usual. 
		</p>
		<example>
			<p>
				For
				<md>
					<mrow>-x + 3y \amp = 1 </mrow>
					<mrow>3x + y \amp = 7</mrow>
				</md>,
				Cramer's Rule says
				<md>
					<mrow>x \amp = \frac{ \twodet{1}{3}{7}{1} }{ \det(\bA) }=  \frac{ \twodet{1}{3}{7}{1} }{ \twodet{-1}{3}{3}{1} } = \frac{-20}{-10} = 2 </mrow>
					<mrow>y \amp = \frac{ \twodet{-1}{1}{3}{7} }{ \det(\bA) } = \frac{ \twodet{-1}{1}{3}{7} }{ \twodet{-1}{3}{3}{1} } = \frac{-10}{-10} = 1</mrow>
				</md>.
			</p>
		</example>
	</subsection>
</section>

<section xml:id="la-equivalent">
	<title>Equivalent conditions</title>
	<p>
		You might have noticed by now that much of what we have covered is the same thing over and over with different notations and terminology. Here is where we pull all of that together.
	</p>
	<theorem xml:id="la-thm-FTLA">
		<title>Fundamental Theorem of Linear Algebra</title>
		<statement><p>
			Suppose <m>\bA\bx=\bb</m> is a linear system with an <m>n\times n</m> matrix <m>\bA</m>. Each of the following statements is equivalent to all of the others:
			<ol>
				<li><m>\bA\bx=\bb</m> has a unique solution,</li>
				<li>the linear system has no free variables,</li>
				<li>the RREF of <m>\bA</m> is the <m>n\times n</m> identity matrix,</li>
				<li><m>\bA</m> is invertible,</li>
				<li>the columns of <m>\bA</m> are a basis of <m>\Rn{n}</m>,</li>
				<li>the dimension of <m>\null(\bA)</m> is zero,</li>
				<li><m>\det(\bA)\neq 0</m>, and</li>
				<li><m>\bA</m> does not have a zero eigenvalue.</li>
			</ol>
		</p></statement>
	</theorem>
	<p>
		The last equivalent statement is a placeholder in anticipation of <xref ref="la-eigen"/>.
	</p>
	<p>
		There are related statements that can be made regarding non-square matrices, but we aren't going to get into them. It is worth knowing, though, that the square case is the only one in which a unique solution is a possibility. If the matrix is singular, then that is ruled out, and there may be zero or an infinity of solutions, depending on both <m>\bA</m> and <m>\bb</m>.
	</p>
</section>

<section xml:id="la-complex">
	<title>Complex numbers</title>
	<introduction>
		<p>(For a really smart and entertaining introduction to complex numbers, I recommend <url href="https://youtu.be/T647CGsuOVU">this video series</url>.)</p>

		<p>We need to take a short but highly relevant detour to talk about complex numbers.</p>

		<p>There's often a lot of uneasiness about complex numbers. Terminology is part of the reason. Using "real" and "imaginary" to label numbers is the residue of a value judgment that ruled mathematics for centuries. But complex numbers are actually just as "real" as so-called real numbers. If anything, they are actually <em>more</em> fundamental to the universe.</p>

		<p>As a practical matter, you can pretty much always replace a complex value with two real ones, and vice versa. But sometimes the manipulations are a lot easier in the complex form. In particular, you may be able to replace trigonometry with algebra.</p>
	</introduction>

	<subsection>
		<title>The reality of imaginary numbers (optional)</title>

		<p>Let's rewind a bit. We can probably take for granted the positive integers 1, 2, 3, and so on, and we'll throw in zero too (though this too was controversial for centuries). It's not long before we want to solve a problem like <m>x+1=0</m>. Annoyingly, we can pose the problem using just nonnegative integers, but we can't solve it. So we accept the existence of the negative integers.</p>

		<p>I can imagine quite a bit of skepticism about this historically. ("Sure, Wei. Ever seen a negative goat?") But if you've ever taken out a loan, you know that negative numbers can have very real consequences.</p>

		<p>Eventually, the negative integers seem "obvious" and perfectly natural. But then we run into a problem like
		<me>
		2x - 1 = 0
		</me>.
		We can pose this problem with integers, but we can't solve it. So we get used to accepting rational numbers, too.</p>

		<p>Rational numbers are pretty weird. Between any pair of them, you have infinitely more rational numbers! Yet it turns out they have (huge) gaps as well. You can't solve
		<me>
		x^2 - 2 = 0
		</me>
		using only rational numbers. So, you're willing to take on irrational numbers too. Talk about weird--every one of them has an infinite, non-repeating decimal expansion.</p>

		<p>So much for the "real" numbers. At least we have filled in the so-called number line. But then you get to
		<me>
		x^2 + 1 = 0
		</me>,
		which is purely "real" but insolvable. Solutions to this equation were widely resisted for a very long time (say, the 18th century), to the point they were called "imaginary" (thanks, Descartes). </p>

		<p>Yet something amazing happens if you do accept imaginary numbers, and their expansion to the complex numbers. Namely, <em>The Fundamental Theorem of Algebra</em>, which states that if you write down a polynomial using complex numbers, it will have only complex numbers as solutions. So there's no infinite ladder of hypercomplex numbers that we have to ascend--just one rung past the "real" ones.</p>
	</subsection>
	<subsection xml:id="la-cx-basic">
		<title>Basic arithmetic</title>
		<p>
			We can write a complex number <m>z\in \mathbb{C}</m> as <m>z=x+iy</m>, where <m>i^2=-1</m> and <m>x</m> and <m>y</m> are real numbers known as the <term>real</term> and <term>imaginary</term> parts of <m>z</m>,
			<me>
				z = x + i y \quad \Leftrightarrow \quad x = \Re(z), \quad y = \Im(z)
			</me>.
			Writing a complex number this way is equivalent to using rectangular or Cartesian coordinates in the plane to specify a point <m>(x,y)</m>. Thus complex numbers are on a <q>number plane</q> rather than a number line.
			<notation>
			  <usage>\Re(z),\,\Im(z)</usage>
			  <description>real/imaginary parts of a complex number</description>
			</notation>
		</p>
		<p>
			There is a generalization of absolute value to complex numbers known as the <term>modulus</term>, a real nonnegative quantity easily computed via
			<me>|z|^2 = [\text{Re}(z)]^2 + [\text{Im}(z)]^2</me>.
			Like the absolute value, <m>|z|</m> is the distance from <m>z</m> to the origin, and <m>|w-z|</m> is the distance between two complex numbers. Here, though, the distances are in the plane rather than on a line.
			<notation>
			  <usage>|z|</usage>
			  <description>modulus of a complex number</description>
			</notation>
		</p>
		<p>
			An important operation on complex numbers that has no real counterpart is the <term>conjugate</term>,
			<me>\bar{z} =\text{Re}(z) - i \,\text{Im}(z)</me>.
			Geometrically this is a reflection across the real axis of the plane. No matter how complicated an expression is, you just replace <m>i</m> by <m>-i</m> everywhere to get the conjugate.
			<notation>
			  <usage>\bar{z}</usage>
			  <description>conjugate of a complex number</description>
			</notation>
		</p>
		<p>
			You add, subtract, and multiply complex numbers by applying the usual algebraic rules, applying <m>i^2=-1</m> as needed. They should give little trouble. Division can be a little trickier, even though the rules are always the same. One trick is to give a complex ratio a purely real denominator:
			<me>\frac{w}{z} = \frac{w \bar{z}}{z \bar{z}} = \frac{w \bar{z}}{|z|^2}</me>.
			This is a lot like rationalizing a denominator with square roots. Memorize the special case <m>1/i = -i</m>.
		</p>
		<p>Here are some more simple rules to know:
		</p>
		<fact>
			<title>Complex arithmetic</title>
			<statement>
				<p>For complex numbers <m>w</m> and <m>z</m>,
					<ol>
						<li><m>|\bar{z}| = |z|</m></li>
						<li><m>|wz| = |w|\cdot |z|</m></li>
						<li><m>|w+z|\le |w| + |z|</m> (triangle inequality)</li>
						<li><m>\left| \frac{1}{z} \right| = \frac{1}{|z|}</m></li>
						<li><m>\overline{wz}=\bar{w}\cdot \bar{z}</m></li>
						<li><m>\overline{w+z}=\bar{w}+\bar{z}</m></li>
						<li><m>\overline{\left(\frac{1}{z} \right)} = \frac{1}{\bar{z}}</m></li>
						<li><m>|z|^2 = z\cdot \bar{z}</m></li>
					</ol>
				</p>
			</statement>
		</fact>
		<example>
			<p>
				MATLAB uses complex numbers without a fuss. Use <c>1i</c> to get the imaginary unit. The functions <c>abs</c> and <c>conj</c> do modulus and conjugate, respectively.
			</p>
		</example>
	</subsection>
	<subsection xml:id="la-cx-matrices">
		<title>Complex-valued matrices</title>
		<p>
			Up to now we have dealt entirely with vectors and matrices whose entries are real numbers. What changes if we consider ones with complex entries?
		</p>
		<p>
			For us, almost nothing. When we consider the definitions of span and nullspace, we have to allow complex coefficients in linear combinations and complex vectors in the nullspace, and a determinant can be complex-valued. But the rules of matrix algebra, and the solution of linear systems, are unaffected. TBH, the only reason we started with real matrices instead of complex ones is that complex numbers tend to freak people out a little.
		</p>
	</subsection>
</section>

<section xml:id="la-eigen">
	<title>Eigenvalues</title>
	<introduction>
		<p>
			The last stop on our whirlwind tour of linear algebra is a topic that is very important to the differential equations we study in future chapters. (I say <q>very important</q> here in the sense of oxygen being a very important part of breathing.)
		</p>
		<aside><p>Everything in this section still relates only to square matrices.</p></aside>
		<definition xml:id="la-df-eigenvalue">
			<title>Eigenvalue and eigenvector</title>
			<statement><p>
				Suppose <m>\bA\in\Cmn{n}{n}</m>. If there exist a scalar <m>\lambda</m> and a nonzero vector <m>\bv</m> such that
				<me>\bA \bv = \lambda \bv</me>,
				then <m>\lambda</m> is an <term>eigenvalue</term> of <m>\bA</m> with associated <term>eigenvector</term> <m>\bv</m>.
			</p></statement>
		</definition>
		<p>
			If you think of <m>\bA</m> as acting on vectors, then an eigenvector is a direction in which the action of <m>\bA</m> is the same as a scalar; we have found a little one-dimensional oasis in which the behavior of <m>\bA</m> is easy to comprehend.
		</p>
	</introduction>
	<subsection xml:id="la-ei-eigenspace">
	<title>Eigenspaces</title>
		<p>
			An eigenvalue is a clean, well-defined target. Eigenvectors can be slipperier. For starters, if <m>\bA\bv=\lambda\bv</m>, then
			<me>
				\bA(c\bv) = c(\bA\bv)=c(\lambda\bv)=\lambda(c\bv)
			</me>.
			Hence <alert>every nonzero multiple of an eigenvector is also an eigenvector for the same <m>\lambda</m></alert>. But there can be even more ambiguity than that.
		</p>
		<example>
			<p>Let <m>\bI</m> be an identity matrix. Then <m>\bI\bx=\bx</m> for any vector <m>\bx</m>, so every nonzero vector is an eigenvector!</p>
		</example>
		<p>
			Fortunately we already have the tools we need to describe a more robust target, based on the very simple reformulation <m>\bzero=\bA\bv-\lambda\bv=(\bA-\lambda\bI)\bv</m>.
		</p>
		<definition xml:id="la-df-eigenspace">
			<title>Eigenspace</title>
			<statement><p>
				Let <m>\lambda</m> be an eigenvalue of <m>\bA</m>. The <term>eigenspace</term> associated with <m>\lambda</m> is <m>\null(\bA-\lambda\bI)</m>.
			</p></statement>
		</definition>
		<p>
			Eigenspaces, unlike eigenvectors, are unique. We have to be a bit careful, though, because we usually express such spaces as the span of basis vectors, and those bases are not themselves unique. For instance, <m>\span\left(\twovec{1}{0},\twovec{0}{1}\right)</m> and <m>\span\left(\twovec{1}{1},\twovec{-1}{1}\right)</m> are both equal to <m>\Cn{2}</m>.
		</p>
		<p>
		Note that if <m>\lambda</m> is <em>not</em> an eigenvalue, then (by definition) the only solution of <m>(\bA-\lambda\bI)\bv=\bzero</m> is <m>\bv=\bzero</m>. In other words, the dimension of <m>\null(\bA-\lambda\bI)</m> is zero, in which case (by <xref ref="la-thm-FTLA"/>) <m>\bA-\lambda\bI</m> is invertible.
		</p>
		<fact><p><m>\lambda</m> is an eigenvalue of <m>\bA</m> if and only if <m>\bA-\lambda\bI</m> is singular.</p></fact>
		<p>
			In practice the most common way to find eigenvalues by hand is through the equivalent condition <m>\det(\bA-\lambda\bI)=0</m>.
		</p>
		<example>
			<p>
				We look for eigenvalues of
				<me>
					\bA = \begin{bmatrix} 1 \amp 1 \\ 4 \amp 1 \end{bmatrix}
				</me>.
				Start by computing
				<me>
					\left| \twomat{1}{1}{4}{1} - \twomat{\lambda}{0}{0}{\lambda} \right| = \twodet{1-\lambda}{1}{4}{1-\lambda} = (1-\lambda)^2 - 4 = \lambda^2-2\lambda-3
				</me>.
				We find eigenvalues by setting this equal to zero, which means <m>\lambda_1=3</m> or <m>\lambda_2=-1</m>. These are the eigenvalues.
			</p>
			<p>
				What about the eigenspaces? First consider <m>\lambda_1=3</m>:
				<me>
					\bA-3 \bI = \twomat{-2}{1}{4}{-2} \overset{\text{RREF}}{\Longrightarrow} \twomat{1}{-1/2}{0}{0}
				</me>.
				Therefore this nullspace (the eigenspace) is <m>\span\left(\twovec{1/2}{1}\right)</m>. As for <m>\lambda_2=-1</m>,
				<me>
						\bA+ \bI = \twomat{2}{1}{4}{2} \overset{\text{RREF}}{\Longrightarrow} \twomat{1}{1/2}{0}{0}
				</me>,
				leading to the eigenspace <m>\span\left(\twovec{-1/2}{1}\right)</m>.
			</p>
			<p>
				MATLAB computes eigenvalues and eigenvectors (through an entirely different process) with the <c>eig</c> command.
			</p>
			<listing xml:id="la_eig">
				<program language="matlab">
					<input>
						A = [ 1 1; 4 1 ];
						lambda = eig(A)

						[V,D] = eig(A)
					</input>
				</program>
			</listing>
			<p>
				In the second format (two outputs), the eigenvalues are on the diagonal of <c>D</c>. The corresponding columns of <c>V</c> are basis vectors for their eigenspaces, provided those are one-dimensional.
			</p>
			<aside>
				<p>
					By hand you usually find eigenvalues by finding the roots of a low-degree polynomial. It turns out that when the size is greater than <m>2\times 2</m>, other linear algebra methods for eigenvalues are much easier than finding the polynomial roots. In fact, MATLAB uses eigenvalues to find polynomial roots, rather than the other way around!
				</p>
			</aside>
		</example>
		<aside>
		<p>
			In the <m>2\times 2</m> case, suppose <m>\lambda</m> is already known to be an eigenvalue. Let the first row of <m>\bA-\lambda\bI</m> be <m>[\alpha,\beta]</m>. If <m>\alpha=\beta=0</m>, then the eigenspace is all of <m>\Cn{2}</m>. Otherwise, the eigenspace is <m>\span\left(\twovec{-\beta}{\alpha}\right)</m>.
		</p>
		</aside>
		<p>
			In the general <m>n\times n</m> case, <m>\det(\bA-\lambda\bI)</m> is a polynomial of degree <m>n</m> whose roots are the eigenvalues of <m>\bA</m>. (This can be seen through the cofactor expansion definition of the determinant.)
		</p>
		<definition xml:id="la-df-charpoly">
			<title>Characteristic polynomial</title>
			<notation>
			  <usage>\det(\bA-\lambda\bI)</usage>
			  <description>characteristic polynomial of a matrix</description>
			</notation>
			<statement><p>
				The <term>characteristic polynomial</term> of <m>\bA</m> is <m>\det(\bA-\lambda\bI)</m>.
			</p></statement>
		</definition>
		<p>
			Given what we know about roots of polynomials, there are some conclusions it's worth stating clearly.
		</p>
		<theorem xml:id="la-thm-eigenvalues">
			<title>Eigenvalue properties</title>
			<statement><p>
				Suppose <m>\bA</m> is an <m>n\times n</m> matrix. Then
				<ol>
					<li><m>\bA</m> has at most <m>n</m> distinct complex eigenvalues, and</li>
					<li>if <m>\bA</m> is real, then any complex eigenvalues occur in conjugate pairs, as do their associated eigenvectors.</li>
				</ol>
			</p></statement>
		</theorem>
		<example xml:id="la-ex-defective">
				<p>
					 Consider <m>\bA=\twomat{4}{0}{1}{4}</m>. The characteristic polynomial is
					 <me>
						 \twodet{4-\lambda}{0}{1}{4-\lambda} = (4-\lambda)^2
					 </me>,
					 so <m>\lambda_1=4</m> is the only eigenvalue. Since
					 <me>
						  \bA - 4\bI = \twomat{0}{0}{1}{0}
					 </me>,
					 the nullspace consists of all vectors <m>\twovec{0}{s}</m>, i.e., it is <m>\span\left(\twovec{0}{1}\right)</m>.
				</p>
		</example>
		<example>
			<p>
				We find the eigenvalues and eigenvectors of
				<me>
					\begin{bmatrix} 1 \amp -1 \\ 5 \amp -3 \end{bmatrix}
				</me>.
				The characteristic polynomial is <m>\lambda^2 +2 \lambda +2</m>, with roots  <m>\lambda_{1,2} = -1 \pm 1i.</m>
			</p>
			<p>
				To find an eigenvector for <m>\lambda_1</m>, we note that the first row of <m> \bA - \lambda_1 \mathbf{I} </m> is <m>[(2-i),-1]</m>. Thus a basis for this eigenspace is the single eigenvector <m>\twovec{1}{2-i}</m>.
			</p>
			<p>
				We get a benefit here from the complex eigenvalues: the conjugate of an eigenvector for <m>\lambda_1</m> will be an eigenvector for <m>\lambda_2</m>. So we have <m>\span\left(\twovec{1}{2+i}\right)</m> to go with <m>\lambda_2</m>.
			</p>
		</example>
	</subsection>
	<subsection xml:id="la-ei-multiplicity">
		<title>Multiplicity</title>
			<p>
				You should be familiar with the idea of multiplicity for roots of a polynomial. We can factor a characteristic polynomial <m>p</m> to get
				<me>
					p(z) = (z-\lambda_1)^{m_1}(z-\lambda_2)^{m_2}\dots(z-\lambda_k)^{m_k}
				</me>
				for nonnegative integer exponents such that <m>m_1+\cdots+m_k=n</m>. These exponents are the multiplicities of the roots.
			</p>
			<definition xml:id="la-df-algmult">
				<title>Algebraic multiplicity</title>
				<statement><p>
					The <term>algebraic multiplicity</term> of an eigenvalue is its multiplicity as a root of the characteristic polynomial.
				</p></statement>
			</definition>
			<p>
				But eigenvalues have another significant notion of multiplicity as well.
			</p>
			<definition xml:id="la-df-geommult">
				<title>Geometric multiplicity</title>
				<statement><p>
					The <term>geometric multiplicity</term> of an eigenvalue is the dimension of its associated eigenspace.
				</p></statement>
			</definition>
			<fact>
				<p>
					The geometric multiplicity of an eigenvalue is less than or equal to its algebraic multiplicity.
				</p>
			</fact>
			<definition xml:id="la-df-defective">
				<title>Defective eigenvalue</title>
				<statement><p>
					An eigenvalue <m>\lambda</m> whose geometric multiplicity is strictly less than its algebraic multiplicity is said to be <idx>defective eigenvalue</idx><term>defective</term>.
				</p></statement>
			</definition>
			<example>
				<p>
					The <m>n\times n</m> identity matrix has characteristic polynomial <m>p(z)=(z-1)^n</m> (by recursive cofactor expansion). Thus its only eigenvalue is <m>\lambda=1</m>, with algebraic multiplicity <m>n</m>. All nonzero vectors are eigenvectors, so the dimension of the eigenspace is also <m>n</m>. Hence <m>\lambda=1</m> is nondefective.
				</p>
			</example>
			<example>
					<p>
						<xref ref="la-ex-defective"/> showed that the matrix <m>\bA=\twomat{4}{0}{1}{4}</m> has eigenvalue <m>\lambda_1=4</m> with algebraic multiplicity 2 and geometric multiplicity 1. Thus this eigenvalue (and by extension the matrix itself) is defective.
					</p>
			</example>
			<p>
				Since multiplicities are always at least one, there is one simple but common case in which we are certain that there are no defective eigenvalues.
			</p>
			<theorem xml:id="la-thm-distincteigs">
				<title>Distinct eigenvalues imply nondefectiveness</title>
				<statement><p>
					If <m>\bA\in\Cmn{n}{n}</m> has <m>n</m> distinct eigenvalues, then <m>\bA</m> has no defective eigenvalues.
				</p></statement>
			</theorem>
			<p>
				This is <em>not</em> also an <q>only if</q> statement, though, as demonstrated by the identity matrix.
			</p>
	</subsection>
</section>


</chapter>
