<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="linear-algebra" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Linear algebra</title>

  <section xml:id="la-linear-systems">
	<title>Linear systems of equations</title>
	<subsection xml:id="la-ls-notation">
	  <title>Some terminology</title>
	  <p>
	  When you first learned algebra, there was a lot of focus on solving equations. It's very likely that you started with linear equations, because these are the easiest ones.
	  </p>
	  <p>
	  It might feel silly, but let's review what it means to solve the linear equation
	  <me>ax = b</me>
	  for <m>x</m>. 
	  <ol>
	  <li>If <m>a\neq 0</m>, there is a single solution, <m>x=b/a</m>.</li>
	  <li><p>Otherwise:
		<ol>
		<li>If also <m>b=0</m>, then every value of <m>x</m> is a valid solution.</li>
		<li>Otherwise, there are no solutions.</li>
		</ol>
		</p>
		</li>
	  </ol>
    To summarize, for nonzero <m>a</m> there is exactly one solution, and otherwise there are infinitely many or zero solutions (depending on <m>b</m>). A linear system with no solutions is called <term>inconsistent</term> <idx>inconsistent linear system</idx>, with the opposite being <em>consistent</em>.
	  </p>

	  <p>
	  Our goal is to understand equations that depend linearly on <em>multiple</em> variables. But before even writing out what that means, let me give away the punch line: <em>the only possibilities are exactly the same three outcomes described above</em>. The sole difference is that the condition "<m>a=0</m>" is generalized to something a little different.
	  </p>

	  <p>
	  Going one more baby step to two equations in two variables, we want to solve 
	  <md>
	  <mrow> ax + by \amp = f </mrow>
	  <mrow> cx + dy \amp = g </mrow>
	  </md>. 
	  There is some easy geometric intuition here. Each equation represents a straight line in the plane. So our cases above translate directly into
	  <ol>
		<li>If the lines are not parallel, there is a single solution.</li>
		<li><p>Otherwise:
		  <ol>
		  <li>If the lines are identical, there are infinitely many solutions.</li>
		  <li>Otherwise, there are no solutions.</li>
		  </ol></p>
		</li>
		</ol>
		It's not hard to turn those statements into algebraic conditions. The slopes of the two lines are (allowing infinities for a moment) <m>-a/b</m> and <m>-c/d</m>, so we can say they are equal if and only if <m>ad=bc</m>.
		<ol>
			<li>If <m>ad\neq bc</m>, there is a single solution.</li>
			<li><p>Otherwise:
			  <ol>
			  <li>If one line's equation is a multiple of the other, there are infinitely many solutions.</li>
			  <li>Otherwise, there are no solutions.</li>
			  </ol></p>
			</li>
			</ol>    
		</p>

		<p>
		Time to put on our big-kid pants. For <m>m</m> equations in <m>n</m> variables, we need to use subscripts rather than different letters for everything:
		<md>
			<mrow>a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \amp = b_1 </mrow>
			<mrow>a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \amp = b_2 </mrow>
			<mrow number="no">\amp \vdots </mrow>
			<mrow>a_{m1} x_1 + a_{m2} x_2 + \cdots + a_{mn} x_n \amp = b_m </mrow>
		</md>. 
		The first index on <m>a_{ij}</m> is the equation number, and the second is the variable number. Or we can think of them as <q>row</q> and <q>column</q> if we put all these numbers into a separate table, 
		<men xml:id="la-eq-mbynmatrix">
			\bA = \begin{bmatrix}
			a_{11} \amp a_{12}  \amp \cdots \amp a_{1n} \\
			a_{21} \amp a_{22}  \amp \cdots \amp a_{2n} \\
			\vdots \amp  \amp  \amp \vdots\\
			a_{m1} \amp \cdots  \amp a_{m,n-1} \amp a_{mn} 
			\end{bmatrix}
		</men>.
		This is our first look at a <idx>matrix</idx> <term>matrix</term>. We say it has size (or shape) <m>m</m> by <m>n</m>. I will use bold capital letters for matrices. While we are at it we should collect the other indexed items into single objects, too:
		<me>
			\bx = \begin{bmatrix}
			x_1 \\ x_2 \\ \vdots \\ x_n 
			\end{bmatrix}, \qquad 
			\bb = \begin{bmatrix}
			b_1 \\ b_2 \\ \vdots \\ b_m 
			\end{bmatrix}
		</me>.
		These are <term>vectors</term>. I use bold lowercase letters for vectors. 
		</p>
		<aside><p>We often think of a vector as a matrix with one column (or one row, if we use that shape). That is how MATLAB does things. Other languages, such as Python, are different, however.</p></aside>     
		<p>
		We will soon be writing a linear system of equations as simply <m>\bA\bx=\bb</m>. And just as above, there is a property of <m>\bA</m> that determines whether or not there is a unique solution. If not, then it depends on both <m>\bA</m> and <m>\bb</m> whether there are infinitely many solutions, or none. Those are the only possibilities. 
		</p>
	</subsection>
	<subsection xml:id="la-ls-elimination">
	<title>Row elimination</title>
		<p>You've probably solved small systems of equations by substitution. Essentially, all we need to do in the general case is systematize that process. The goal is to transform the system to an equivalent one whose solution(s) we can just read off.</p>
		<example>
		<p>
			We start with a system 
			<md>
			<mrow>  x_1 - x_2  - x_3 \amp = 2 </mrow>
			<mrow> 3x_1 - 2x_2 \amp = 9 </mrow>
			<mrow> x_1 - 2x_2 - x_3 \amp = 5 </mrow>
			</md>. 
			The first step is to use the first equation to eliminate <m>x_1</m> from the second and third equations. We subtract 3 times equation 1 from equation 2, and 1 times equation 1 from equation 3: 
			<md>
				<mrow>  x_1 - x_2  - x_3  \amp = 2 \amp \amp </mrow>
				<mrow> (3x_1 - 2x_2) - 3(x_1 - x_2  - x_3) \amp = 9 - 3(2)\quad  \amp \Rightarrow \quad  x_2 +3x_3 \amp = 3</mrow>
				<mrow> (x_1 - 2x_2 - x_3) - 1(x_1 - x_2  - x_3) \amp = 5 - 1(2) \quad \amp \Rightarrow \quad  -x_2 \amp = 3</mrow>
			</md>. 
			</p>
			<aside><p>I know you want to use that last equation to get rid of <m>x_2</m> everywhere right away. That would be fine for this particular problem, but we are aiming for a systematic process that works every time.</p></aside> 
			<p>
			The next step of the recipe is to leave the first equation alone, and use the second to eliminate <m>x_2</m> from all the others below it (i.e., the third).
			<md>
				<mrow>  x_1 - x_2  - x_3  \amp = 2  \amp \amp </mrow>
					<mrow> x_2 + 3x_3 \amp = 3 \amp \amp </mrow>
					<mrow> (-x_2) + (x_2+3x_3)  \amp = 3 + 3 \amp \quad \Rightarrow \quad 3x_3 \amp = 6</mrow>
			</md>.   
			Now we have reached the last equation. There's nothing to eliminate below it, so turn around and eliminate <em>upwards</em> instead: 
			<md>
				<mrow>  (x_1 - x_2 - x_3) + \frac{1}{3}(3x_3) \amp = 2  +\frac{1}{3}(6) \amp \quad \Rightarrow \quad x_1  -x_2 \amp = 4</mrow>
				<mrow> (x_2 + 3x_3) - (3x_3) \amp = 3 - (6)\amp \quad \Rightarrow \quad x_2 \amp = -3</mrow>
				<mrow> 3x_3  \amp = 6 \amp  \amp</mrow>
			</md>.   
			Continue moving back upwards, to the second equation, and use it to eliminate above: 
			<md>
				<mrow>  (x_1 - x_2 ) + (x_2) \amp = 4  + (-3)  \amp \quad \Rightarrow \quad x_1  \amp = 1</mrow>
				<mrow> x_2 \amp = -3 \amp \amp </mrow>
				<mrow> 3x_3  \amp = 6 \amp \amp </mrow>
			</md>.   
			Obviously, the (unique) solution is <m>x_1=1</m>, <m>x_2=-3</m>, <m>x_3=2</m>.
		</p>
		</example>
		<p>
			That was a mouthful. We can make it a little easier if we just manipulate the numbers without carrying around extra notation. We start with the <idx>augmented matrix</idx> <term>augmented matrix</term> <m>[\bA,\,\bb]</m>.
		</p> 
		<example>
			<p>
			We repeat the previous example but just with the relevant constants. The key is to use zeros as coefficients where there are <q>silent</q> variables. The starting augmented matrix is 
			<me>              
				\begin{bmatrix}
				 1 \amp 1 \amp -1 \amp 2 \\
				 -3 \amp -2 \amp 0 \amp 9 \\
				 1 \amp -2 \amp -1 \amp 5
				\end{bmatrix}
			</me>. 
			First elimination step uses the first row to eliminate below it: 
			<me>              
					\begin{bmatrix}
					 1 \amp 1 \amp -1 \amp 2 \\
					 0 \amp 1 \amp 3 \amp 3 \\
					 0 \amp -1 \amp 0 \amp 3
					\end{bmatrix}
			</me>. 
			Next we use the second row to eliminate below it: 
			<me>              
				\begin{bmatrix}
					1 \amp 1 \amp -1 \amp 2 \\
					0 \amp 1 \amp 3 \amp 3 \\
					0 \amp 0 \amp 3 \amp 6
				\end{bmatrix}
			</me>. 
			Having reached the last row, we turn around and eliminate above it.
			<me>              
				\begin{bmatrix}
					1 \amp -1 \amp 0 \amp 4 \\
					0 \amp 1 \amp 0 \amp -3 \\
					0 \amp 0 \amp 3 \amp 6
				\end{bmatrix}
			</me>. 
			We move up to the second row and eliminate above that:
			<me>              
					\begin{bmatrix}
						1 \amp 0 \amp 0 \amp 1 \\
						0 \amp 1 \amp 0 \amp -3 \\
						0 \amp 0 \amp 3 \amp 6
					\end{bmatrix}
			</me>. 
			Finally, to be super pedantic, we normalize each row: 
			<me>              
					\begin{bmatrix}
						1 \amp 0 \amp 0 \amp 1 \\
						0 \amp 1 \amp 0 \amp -3 \\
						0 \amp 0 \amp 1 \amp 2
					\end{bmatrix}
			</me>. 
			The last column is now the solution. 
			</p>   
		</example>
		<p>
			Before we generalize, let's look at an example that works out differently. 
		</p>
		<example>
			<p> 
			Let's solve the system having augmented matrix 
			<me>              
				\begin{bmatrix}
					1 \amp 1 \amp -1 \amp 4 \\
					1 \amp 2 \amp 2 \amp 3 \\
					2 \amp 1 \amp -5 \amp 9
				\end{bmatrix}
			</me>. 
			Use multiples of the first row to eliminate below it.
			<me>
				\begin{bmatrix}
				1 \amp 1 \amp -1 \amp 4 \\
				0 \amp 1 \amp 3 \amp -1 \\ 
				0 \amp -1 \amp -3 \amp 1 
				\end{bmatrix}
			</me>   
			Use a multiple of the second row to eliminate below that.   
			<me>
				\begin{bmatrix}
				1 \amp 1 \amp -1 \amp 4 \\
				0 \amp 1 \amp 3 \amp -1 \\ 
				0 \amp 0 \amp 0 \amp 0 
				\end{bmatrix}
			</me>   
			We have reached the last row, and it's time to turn around and eliminate upwards. But we can't, because the last row is entirely zero. So skip that and move up to the next row, eliminating upwards again in the second column: 
			<me>
				\begin{bmatrix}
				1 \amp 0 \amp -1 \amp 5 \\
				0 \amp 1 \amp 3 \amp -1 \\ 
				0 \amp 0 \amp 0 \amp 0 
				\end{bmatrix}
			</me>. 
			This is as simple as we can get things. The last row is telling us the astounding fact that <m>0=0</m>, i.e., nothing at all. The fact that there was nothing there to determine <m>x_3</m> means that we can set it to be anything we like, say <m>x_3=s</m> for arbitrary <m>s</m>. Now the second row says 
			<me>
				x_2 = -1 - 3s
			</me>,
			which is as specific as we can get. Finally, the first row says 
			<me>
				x_1 = 5 + s 
			</me>. 
			So <m>[5+s,-1-3s,s]</m> is a solution for every <m>s</m>, and we have infinitely many solutions.     
			</p>
		</example>
		<p>
		The process demonstrated in the previous two examples is best known as <idx>row elimination</idx> <term>Gauss-Jordan elimination</term>, though I prefer to call it <term>row elimination</term> (seeing how the basic idea was known at least in China thousands of years before those dudes were born). As seen in the examples, row elimination consists of two phases, one downward and one upward. The goal is to put the augmented matrix into a special form. In the following definition of that form, we use the term <idx>leading nonzero</idx> <term>leading nonzero</term> to mean the first (leftmost) entry of a row that is not zero. 
		<!--dl>
		<dt>Downward phase</dt> 
		<dd><p>For each <m>i=1,2,\ldots,m-1</m>:
		<ol>
		<li>If the <term>pivot</term> element in the <m>(i,i)</m> position is zero, swap row <m>i</m> with a row below it to get a nonzero pivot, if possible.</li>
		<li>If the pivot is now nonzero, use row <m>i</m> to put zeros below it in column <m>i</m>.</li>
		</ol>
		</p></dd>
		<dt>Upward phase</dt>
		<dd><p>
		For each <m>i=m,m-1,\ldots,2:</m>
		<ol>
		<li>If the pivot element is nonzero, use row <m>i</m> to put zeros above it in column <m>i</m>.</li>
		</ol>
		</p></dd>
		</dl-->
		</p>
		<definition>
		  <title>RREF</title>
		  <statement>
		  <p>A matrix is in <idx>RREF (row reduced echelon form)</idx> <term>RREF</term> (reduced row echelon form) if it meets these requirements:
		  <ol>
		  <li>Any rows of all zeros appear below all nonzero rows.</li>
		  <li>The leading nonzero of any row is a one.</li>
		  <li>Every leading one is the only nonzero in its column.</li>
		  </ol>
		  The columns that have leading ones are called <idx>pivot column</idx> <term>pivot columns</term>. The other columns are called <idx>free column</idx> <term>free columns</term>. 
		  </p>
		  </statement>
		</definition>
		<p>
		As seen in the examples above, an augmented matrix in RREF represents a linear system that we can solve by inspection:
		<ul>
		<li>The zero rows are ignored.</li>
		<li>If a leading one occurs in the last column, the system is inconsistent.</li>
		<li>Otherwise, each variable associated with a free column is considered a free parameter (e.g., <m>s</m>, <m>t</m>, etc.).</li>
		<li>The remaining pivot columns are used to solve for their corresponding variables in terms of the free parameters.</li>
		</ul>
		</p>
		<p>
		Finally, there are three <q>moves</q> or row operations that are allowed in row elimination:
		<ol> 
		<li>Swap any two rows.</li>
		<li>Multiply a row by a <em>nonzero</em> constant.</li>
		<li>Add a multiple of one row into another.</li>
		</ol>
		These are sufficient to transform any matrix into RREF.
		</p>
		<aside>
		<p>MATLAB has a command <c>rref</c> to put a matrix in RREF. However, it's not ideal, because MATLAB uses floating point and cannot always produce exact zeros. You can also <url href="https://www.wolframalpha.com/input/?i=rref%20%7B%7B1%2C2%2C3%7D%2C%7B4%2C5%2C6%7D%2C%7B7%2C8%2C9%7D%7D">try Wolfram Alpha</url> for this.
		</p>
		</aside>

	</subsection>
  </section>  

  <section xml:id="la-vector-algebra">
	  <title>Vector algebra</title>
	  <subsection xml:id="la-vectors">
	  <title>Linear combinations</title>
	  <p>
	  We use <m>\Rn{n}</m> to denote the set of all vectors with <m>n</m> real elements. (Every vector is assumed to have a single-column shape.) If <m>\bx</m> is in <m>\Rn{n}</m>, then for any real number <m>c</m> we define <m>c\bx</m> by elementwise multiplication. Because this just scales the magnitude of the vector, a real value is often called a <term>scalar</term><idx>scalar</idx>. If <m>\bx</m> and <m>\by</m> are in <m>\Rn{n}</m>, then we can add or subtract them elementwise. (Addition of a scalar and a vector, and addition of vectors of different lengths, are undefined.)
	  </p>
	  <p>
	  Scaling and addition are often rolled together into a singly namned operation. 
	  </p>
	  <definition>
		<title>Linear combination</title>
		<statement>
		<p>A <idx>linear combination</idx> <term>linear combination</term> of vectors <m>\ba_1,\ldots,\ba_k</m>, all in <m>\Rn{n}</m>, is         
		<men xml:id="la-va-lincomb">
		  x_1 \ba_1 + x_2 \ba_2 + \cdots + x_k \ba_k = \sum_{j=1}^k x_j \ba_j
		</men>,
		where the <m>x_j</m> are scalars known as <term>coefficients</term> (or <term>weights</term>). 
		</p></statement>
	  </definition>
	  <p>
	  One motivation for this definition is to express linear systems. 
	</p>
	  <example>
	  <p>
	  Consider the system
	  <md>
		<mrow>  x_1 - x_2  - x_3 \amp = 2 </mrow>
		<mrow> 3x_1 - 2x_2 \amp = 9 </mrow>
		<mrow> x_1 - 2x_2 - x_3 \amp = 5 </mrow>
	  </md>. 
	  Interpreting vector equalities elementwise, this is equivalent to 
	  <me>
		x_1 \threevec{1}{3}{1} + x_2 \threevec{-1}{-2}{-2} + x_3 \threevec{-1}{0}{-1} = 
		\threevec{2}{9}{5}
	  </me>.
	  We can identify the three vectors on the left side as the columns of the coefficient matrix <m>\bA</m>, and the one on the right is <m>\bb</m>. In an earlier example, we found the solution <m>\bx=[1,-3,2]</m>. Hence
	  <me>
		\ba_1 - 3 \ba_2 + 2\ba_3 = \bb
	  </me>,
	  where we gave names to the columns. 
	  </p></example>
	  </subsection>
	  <subsection>
	  <title>Matrix times vector</title>
	  <p>
		The linear combination definition <xref ref="la-va-lincomb"/> actually serves as the foundation of multiplication between a matrix and a vector. We use <m>\Rmn{m}{n}</m> to mean the set of all matrices with <m>m</m> rows and <m>n</m> columns. 
	  </p>
	  <definition>
		<title>Matrix times vector</title>
		<statement>
		<p>Given <m>\bA\in\Rmn{m}{n}</m> and <m>\bx\in\Rn{n}</m>, the product <m>\bA\bx</m> is defined as 
		<men xml:id="la-mv-matvec">
		  \bA\bx = x_1 \ba_1 + x_2 \ba_2 + \cdots + x_n \ba_n = \sum_{j=1}^n x_j \ba_j
		</men>.
		</p></statement>
	  </definition>
	  <p>
	  What justifies calling this opearation multiplication? In large part, it's the natural distributive properties
	  <md>
	  <mrow> \bA(\bx+\by) \amp =  \bA\bx + \bA\by</mrow>
	  <mrow> (\bA+\bB)\bx \amp =  \bA\bx + \bB\bx</mrow>
	  </md>
	  (where matrix addition is also defined elementwise), which can be checked with just a little determination. It's also true that <m>\bA(c\bx)=c(\bA\bx)</m>. However, <alert>the product <m>\bA\bx</m> is <em>not</em> commutative; <m>\bx\bA</m> is not even defined.</alert>
	   </p>
	   <p>
	   As a matter of fact, these properties combine in a way worth noting. We can define a function <m>L</m> on vectors by <m>L(\bx)=\bA\bx</m>. This is what we call a <idx>linear function</idx><term>linear function</term>, because <m>L(c\bx + \by)=cL(\bx)+L(\by)</m> for all <m>n</m>-vectors <m>\bx,\by</m> and scalars <m>c</m>. This would not be true for, for example, <m>L(\bx)=x_1x_2</m>. 
	   </p>
	  </subsection>
  </section>  

  <section xml:id="la-matrix-algebra">
	<title>Matrix algebra</title>
	<subsection>
	<title>Matrix multiplication</title>
	<p>Addition and scalar multiplication of matrices is exactly the same as for vectors—elementwise. Things get interesting when we get to matrix products.</p>
	<p>
		In order to define the product <m>\bA\bB</m>, we require that the number of <em>columns</em> in <m>\bA</m> is the same as the number of <em>rows</em> in <m>\bB</m>. We say that the <em>inner dimensions</em> must agree; otherwise, the product is considered undefined.  
	</p>
	<definition>
		<title>Matrix times matrix</title>
		<statement>
			<p>If <m>\bA</m> is <m>m\times n</m> and <m>\bB</m> is <m>n\times p</m>, then the product <m>\bA\bB</m> is defined as
				<men xml:id="la-eq-matmat">
					\bA\mathbf{B} =
						\bA \begin{bmatrix}
						\mathbf{b}_1 \amp \mathbf{b}_2 \amp \cdots \amp \mathbf{b}_p
						\end{bmatrix}
						= \begin{bmatrix}
						\bA\mathbf{b}_1 \amp \bA\mathbf{b}_2 \amp \cdots \amp \bA\mathbf{b}_p
						\end{bmatrix}
				</men>.
			</p>
		</statement>
	</definition>
	<p>In words, a matrix-matrix product is the horizontal concatenation of matrix-vector products involving the columns of the right-hand matrix. Note that when we multiply <m>m\times n</m> by <m>n\times p</m>, the result is <m>m\times p</m>; that is, the <em>outer dimensions</em> remain.</p>	
	<p>
	When we compute a matrix product by hand, we usually don't write out the above. Instead we use a more compact definition for the individual entries of <m>\bC = \bA\bB</m>,
	<men xml:id="la-eq-matmatinner">
		C_{ij} = \sum_{k=1}^n A_{ik}B_{kj}, \qquad i=1,\ldots,m, \quad j=1,\ldots,p
	</men>. 
	</p>
	<example>
		<p>
		Let
		<me>
			\bA = \begin{bmatrix}
				1 \amp -1 \\ 0 \amp 2 \\ -3 \amp 1
				\end{bmatrix}, \qquad
				\mathbf{B} = \begin{bmatrix}
				2 \amp -1 \amp 0 \amp 4 \\ 1 \amp 1 \amp 3 \amp 2
				\end{bmatrix}
		</me>.  
		Then 
		<md>
			<mrow>\bA\mathbf{B} \amp= \begin{bmatrix} 
				(1)(2) + (-1)(1) \amp (1)(-1) + (-1)(1) \amp (1)(0) + (-1)(3) \amp (1)(4) + (-1)(2) \\
				(0)(2) + (2)(1) \amp (0)(-1) + (2)(1) \amp (0)(0) + (2)(3) \amp (0)(4) + (2)(2) \\
				(-3)(2) + (1)(1) \amp (-3)(-1) + (1)(1) \amp (-3)(0) + (1)(3) \amp (-3)(4) + (1)(2)
				\end{bmatrix} </mrow>
			<mrow>\amp = \begin{bmatrix}
						1 \amp -2 \amp -3 \amp 2 \\ 2 \amp 2 \amp 6 \amp 4 \\ -5 \amp 4 \amp 3 \amp -10
					\end{bmatrix}</mrow>
		</md>.
		</p>
		<p>
		   Observe that 
			<me>
				\bA \begin{bmatrix} 2 \\ 1 \end{bmatrix} = 2 \begin{bmatrix} 1 \\ 0 \\ -3
					\end{bmatrix} + 1 \begin{bmatrix} -1 \\ 2 \\ 1 \end{bmatrix}
					= \begin{bmatrix} 1 \\ 2 \\ -5 \end{bmatrix}
			</me>, 
			and so on. 
		</p>
	 </example>
	 </subsection>
	 <subsection>
		<title>Properties</title>
		<p>
			 First, the bad news. In the previous subsection we computed the product <m>\bA\bB</m>, where the matrices are <m>3\times 2</m> and <m>2\times 4</m>, to get a result that is <m>3\times 4</m>. However, the product <m>\bB\bA</m> isn't even defined, because <m>4\neq 3</m>. Moreover, even when both <m>\bA\bB</m> and <m>\bB\bA</m> <em>are</em> defined, we cannot automatically expect them to be equal. You may have to unlearn some very old habits. 
		</p>
		<aside><p>In a product of matrices (and vectors), position matters. You cannot change the ordering without an explicit justification!</p></aside> 
		<fact>
			 <p>Matrix multiplication is not commutative. That is, the order of terms in a product affects the result.</p>
		</fact>
		<p>
		Fortunately, other familiar properties of multiplication do come along for the ride:
		<ol>
			<li><m>(\bA\bB)\bC=\bA(\bB\bC)</m></li>
			<li><m>\bA(\bB+\bC) = \bA\bB + \bA\bC</m></li>
			<li><m>(\bA+\bB)\bC = \bA\bC + \bB\bC</m></li>
			<!--li><m>\alpha(\bB+\bC) = \alpha\bB + \alpha\bC</m></li-->
		</ol>  
		</p>
	</subsection>
	<subsection>
	<title>Identity matrix</title>
		<p>
			It's pretty immediate that for any square matrix <m>\bA</m>, <m>\bA\boldsymbol{0}=\boldsymbol{0}\bA=\boldsymbol{0}</m> for the all-zero square matrix <m>\boldsymbol{0}</m> of approrpiate size. So we have a multiplicative zero. But the situation is a bit less obvious when it comes to a multiplicative unit element (the counterpart of the scalar 1). 
		</p>
		<definition>
			<title>Identity matrix</title>
			<statement>
			<p>The <m>n\times n</m> <idx>identity matrix</idx> <term>identity matrix</term> is 
				<me>
						\bI = \begin{bmatrix} 
						1 \amp 0 \amp 0 \amp \cdots \amp 0 \\
						0 \amp 1 \amp 0 \amp \cdots \amp 0 \\
							\vdots \amp  \amp \ddots \amp  \amp \vdots\\
							0 \amp \cdots \amp 0 \amp 1 \amp 0 \\
							0 \amp \cdots \amp 0 \amp 0 \amp 1
							\end{bmatrix}
				</me>.
			</p>
			</statement>
		</definition>
		<p>
		The definition of the identity is such that <m>\bA\bI=\bA</m> for all <m>\bA\in\Rmn{m}{n}</m> and <m>\bI\bB=\bB</m> for all <m>\bB\in\Rmn{n}{p}</m>. This includes vectors, i.e., <m>\bI\bx=\bx</m> for all <m>\bx \in \Rn{n}</m>.
		</p> 
</subsection>
</section>  

<section xml:id="la-general-solution">
	<title>General and particular solutions</title>
	<introduction>
		<p>
		Our next goal is to show that solutions to <m>\bA\bx=\bb</m> can all be expressed as <m>\bx=\bx_h+\bx_p</m>, with the two parts belonging to different sets. The same thing is going to happen when we study differential equations, pretty much for exactly the same reasons.
		</p>
	</introduction>
	<subsection>
		<title>Homogeneous solutions</title>
		<p>
		The linear system <m>\bA\bx=\bzero</m> plays a special role and is called a <idx><h>homogeneous</h> <h>linear system</h></idx> <term>homoegeneous linear system</term>. 
		</p>
		<theorem xml:id="la-gs-homolincomb">
			<title>Homogeneous linear systems</title>
			<statement><p>
				If <m>\bx_1,\ldots,\bx_k</m> are solutions of a homogeneous system <m>\bA\bx=\bzero</m>, then every linear combination of them is also a solution.
			</p></statement>
			<proof><p>
				Let <m>c_1\bx_1 + \cdots + c_k\bx_k</m> be such a linear combination. Then 
				<md>
					<mrow>\bA(c_1\bx_1 + \cdots + c_k\bx_k) \amp = \bA(c_1 \bx_1 ) + \cdots + \bA(c_k \bx_k) </mrow>
					<mrow>\amp = c_1\bA\bx_1 + \cdots + c_k\bA \bx_k </mrow>
					<mrow>\amp = \bzero</mrow>
				</md>.
			</p></proof>
		</theorem>
		<p>
			By itself this is nice, but what makes this theorem important is that <em>every</em> homogeneous solution can obtained this way. There is some geometric intuition in three dimensions. Each row of <m>\bA\bx=\bzero</m> is a plane through the origin. Typically three planes intersect at a single point (unique solution), but if two planes coincide, the intersection is a straight line through the origin. We can express such a line as the set of all possible <m>c_1\bx_1</m> for any vector <m>\bx_1</m> parallel to the line. Furthermore, if all three planes coincide, then the solution is just the common plane, which can be expressed as the set of alll possible <m>c_1\bx_1+c_2\bx_2</m> for any two nonparallel vectors in that plane.
		</p>
		<p>
			None of the geometry is necessary to apply the algebra.
		</p>
		<example xml:id="la-ex-33homog">
			<p>
				Suppose 
				<me>
					\bA = 
					\begin{bmatrix} 
					0 \amp -2 \amp 4 \\ 0 \amp 1 \amp -2 \\ 0 \amp 3 \amp -6
					\end{bmatrix}
				</me>. 
				The RREF of the augmented matrix <m>[\bA,\bzero]</m> is found to be 
				<me>
						\bA = 
						\begin{bmatrix} 
						0 \amp 1 \amp -2 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0
						\end{bmatrix}
				</me>. 
				(Note that row operations could never change the zeros in the augmented column.) Hence the only pivot column is column 2, and all solutions are of the form 
				<me>
					\threevec{s}{2t}{t} = s \threevec{1}{0}{0} + t \threevec{0}{2}{1}
				</me>,
				where <m>s</m> and <m>t</m> are arbitrary scalars. You can easily check that both of the constant vectors above are themselves homogeneous solutions. 
			</p>
		</example>
		<p>
			Motivated by the theorem and example above, we make a new definition. 
		</p>
		<definition xml:id="la-gs-homogen">
			<title>General solution of a linear system (homoegeneous)</title>
			<statement><p>
				We say that <m>\bx_h = c_1\bx_1 + \cdots + c_k\bx_k</m> is the <term>general solution</term><idx><h>general solution</h><h>of homogeneous linear system</h></idx> of <m>\bA\bx=\bzero</m> if every solution to the homogeneous system can be so written for some choice of the coefficients.
			</p></statement>
		</definition>
	</subsection>
	<subsection xml:id="la-gs-particular">
	<title>Particular solutions</title>
		<p>
			We now come to one of the simplest yet most confusing terms in the course. A <term>particular solution</term><idx><h>particular solution</h><h>of a linear system</h></idx> of the linear system <m>\bA\bx=\bb</m> is just any one solution of the problem. The only reason the term exists is to distinguish it from the <em>general</em> solution, which (as above) is an expression for <em>every possible</em> solution of the system. 
		</p>
		<theorem xml:id="la-gs-general">
			<title>General solution of alinear system</title>
			<statement><p>
				All solutions of <m>\bA\bx=\bb</m> may be written as 
				<me>\bx = \bx_h + \bx_p</me>, 
				where <m>\bx_h</m> is the general solution of <m>\bA\bx=\bzero</m> and <m>\bx_p</m> is any particular solution of <m>\bA\bx=\bb</m>. We call this the <term>general solution</term><idx><h>general solution</h><h>of a linear system</h></idx> of <m>\bA\bx=\bb</m>. 
			</p></statement>
			<proof><p>
				Let <m>\bx_p</m> be a particular solution, and suppose <m>\bu</m> is another solution. Then 
				<me>\bA(\bu-\bx_p)=\bA\bu - \bA\bx_p = \bb - \bb = \bzero</me>. 
				Hence <m>\bu-\bx_p</m> is a homogeneous solution, which means that we can write it as <m>\bx_h</m>.
			</p></proof>
		</theorem>
		<p>
			Thus, one way to attack <m>\bA\bx=\bb</m> is to first find the general homogeneous solution, then find any particular solution of the nonhomogeneous problem. You will do this many times over for differential equations in future chapters. 
		</p>
		<example>
			<p>
				We continue with the matrix <m>\bA</m> from <xref ref="la-ex-33homog"/>, now with <m>\bb=[-8,4,12]</m>. The RREF of the augmented matrix is now 
				<me> 
					\begin{bmatrix}
					0 \amp 1 \amp -2 \amp 4 \\ 0 \amp 0 \amp -0 \amp 0 \\ 0 \amp 0 \amp 0 \amp 0
					\end{bmatrix}
				</me>. 
				This contains all the information we need. If we imagine replacing all entries in the last column by zeros, then we get (as before) the general homogeneous solution 
				<me>\bx_h =	s \threevec{1}{0}{0} + t \threevec{0}{2}{1}</me>. 
				For the particular solution we again have free variables from columns 1 and 3. But we can use <em>any</em> particular solution, so it will be easiest if we set the free variables to zero. Then the first row tells us that <m>x_2=4</m>, and we get the general solution 
				<me>\bx = \threevec{0}{4}{0} + s \threevec{1}{0}{0} + t \threevec{0}{2}{1}</me>. 
			</p>
		</example>
		<p>
			This process doesn't really save us any algebraic work right now, because we would have gotten the same expression for the solution directly from the RREF. So you might be wondering what the point of this section is. Have patience, grasshopper. 
		</p>
	</subsection>
</section>  

<section xml:id="la-basis">
	<title>Basis and dimension</title>
	<subsection>
		<title>Span and independence</title>
		<p>In the last section we saw that solutions of <m>\bA\bx=\bzero</m> play an important role in all linear systems.</p>
		<definition xml:id="la-df-nullspace">
			<title>Nullspace</title>
			<statement><p>
				The <term>nullspace</term><idx>nullspace</idx> of a matrix <m>\bA\in\Rmn{m}{n}</m> is 				
				<men xml:id="la-eq-nullspace">
					\null(\bA) = \{ \bx\in\Rn{n} : \bA\bx=\bzero \} 
				</men>.
			</p></statement>
		</definition>
		<p>We also saw that every element of <m>\null(\bA)</m> can be expressed as a linear combination, <m>c_1\bx_1+\cdots+c_k\bx_k</m>.</p>
		<definition xml:id="la-df-span">
			<title>Span</title>
			<statement><p>
				The <term>span</term> of vectors <m>\bx_1,\ldots,\bx_k</m> is 
				<men xml:id="la-eq-span">
					\span(\bx_1,\ldots,\bx_k) = \{ c_1\bx_1+\cdots+c_k\bx_k : c_1,\ldots,c_k \in \mathbb{R} \}
				</men>.
			</p></statement>
		</definition>
		<p>
			The notion of span gives us a shorthand for expressing the nullspace (equivalently, the general homogeneous solution). For reasons we won't get into, every span is referred to as a space. However, it's important to know that there are infinitely many different ways to express the same space. For instance, 
			<me>\span\left(\bx_1\right) = \span\left(\bx_1,\bx_1,\bx_1\right)</me>. 
			That's kind of an obvious <q>cheat</q>. Here is a more subtle one: 
			<me>
				\span\left( \twovec{1}{0}, \twovec{0}{1}, \twovec{1}{1} \right) = \span\left( \twovec{1}{0}, \twovec{0}{1} \right)
			</me>. 
			The reasoning here is that the first two vectors already span all of <m>\Rn{2}</m>, so there's nothing more that the third vector can bring to the table. We have a term for these observations. 
		</p> 
		<definition xml:id="la-df-independence">
			<title>Linear independence</title>
			<statement><p>
				The vectors <m>\bx_1,\dots,\bx_k</m> are <term>linearly dependent</term> if there are scalars <m>c_1,\ldots,c_k</m>, not all zero, such that 
				<me>c_1\bx_1+\cdots+c_k\bx_k = \bzero</me>. 
				Otherwise the vectors are <term>linearly independent</term>. 
			</p></statement>
		</definition>
		<p>
			Why is this the right concept? Well, if the vectors are dependent, then by definition we can write 
			<me> c_j \bx_j = - \sum_{i\neq j} c_i \bx_i</me>, 
			where <m>c_j\neq 0</m>. If we divide this through by <m>c_j</m>, then we have shown that <m>\bx_j</m> lies within the span of all the other vectors, and therefore including it with linear combinations is redundant. 
		</p>
	</subsection> 
	<subsection xml:id="la-ba-basis">
	<title>Basis</title>
		<p>Put span and independence together and you get something good.</p>
		<definition xml:id="la-df-basis">
			<title>Basis</title>
			<statement><p>
				If <m>\bx_1,\dots,\bx_k</m> are independent, then they form a <term>basis</term> of <m>\span(\bx_1,\dots,\bx_k)</m>. 
			</p></statement>
		</definition>
		<p>It turns out that every basis of a given space has the same number of vectors in it. This number is called the <term>dimension</term> of the space. </p>
		<example>
				<p>We find the RREF of a <m>3\times 3</m> matrix: 
				<me>
					 \bA = \begin{bmatrix} 
					 1 \amp 1 \amp -1 \\
					 2 \amp 2 \amp 1 \\
					 -1 \amp -1 \amp 4   
					 \end{bmatrix} \mapsto 
					 \begin{bmatrix} 
					 1 \amp 1 \amp -1 \\
					 0 \amp 0 \amp 3 \\
					 0 \amp 0 \amp 3   
					 \end{bmatrix} \mapsto 
					 \begin{bmatrix} 
					 1 \amp 1 \amp 0 \\
					 0 \amp 0 \amp 1 \\
					 0 \amp 0 \amp 0   
					 \end{bmatrix}
				</me>. 
				Columns 1 and 3 are the pivot columns. We regard <m>x_2</m> as a free variable; say <m>x_2=s</m>. Then <m>x_3=0</m> and <m>x_1=-s</m>, and the homogeneous general solution is  
				<me>
					 \bx_h = \begin{bmatrix} -s \\ s \\ 0 \end{bmatrix} = s \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}
				</me>.
				Equivalently, we say that 
				<me>\null(\bA) = \span\left(\begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}\right)</me>,
				and in fact <m>[-1,1,0]</m>  is a basis of the nullspace, which is therefore one-dimensional. 
				</p>
		  </example>   
		  <example>
				<p>
					 Suppose the RREF of a matrix ends with 
					 <me>
					 \begin{bmatrix} 
					 1 \amp 0 \amp 0 \\
					 0 \amp 1 \amp 0 \\
					 0 \amp 0 \amp 1   
					 \end{bmatrix}
					 </me>
					 (i.e., an identity matrix). There are no free variables, and the only solution is <m>\bx=[0,0,0]</m>. Hence <m>\null(\bA) = \{\bzero\}</m>. This singleton set does not have a basis and is considered to have dimension zero. 
				</p>
		  </example> 
		  <p>
			  I emphasize that <em>we have done no new math here</em>. It's still just about linear systems and RREF matrices, with different and more precise terminology. 
		  </p>
	</subsection>
</section>

<!--section xml:id="la-subspace">
<title>Subspaces</title>
</section-->  

  <!--section xml:id="la-basis">
	  <title>Bases</title>
  </section--> 

<section xml:id="la-determinant">
	<title>Determinants</title>
	<p>
		 You should have seen some <m>2\times 2</m> and <m>3\times 3</m> determinants before now, in vector calculus. The <m>2\times 2</m> case is easy to describe: 
		 <me>\det\left( \twomat{a}{b}{c}{d} \right) = \twodet{a}{b}{c}{d} = ad-bc</me>. 
		 This can be extended to create a real-valued function for square matrices of any size. The formalities can be complicated, but we are going to use a practical definition. 
	</p>
	<aside><p>Here and for the rest of this chapter, we are going to deal with square matrices only.</p></aside>
	<definition xml:id="la-df-det">
		<title>Determinant</title>
		<statement><p>
			If <m>\bA\in\Rmn{n}{n}</m>, then 
			<me>
				\det(\bA) = \sum (-1)^{i+j} a_{ij} \det( \mathbf{M}_{ij} )
			</me>,
			where the sum is taken over any row or column of <m>\bA</m>, and <m>\mathbf{M}_{ij}</m> is the matrix that results from deleting row <m>i</m> and column <m>j</m> from <m>\bA</m>.
		</p></statement>
	</definition>
	<p>
		The definition, which is called <term>cofactor expansion</term>, is recursive: the <m>n\times n</m> case is defined in terms of the <m>(n-1)\times (n-1)</m> case, and so on all the way back down to <m>2\times 2</m>. 
	</p>
	<p>
		There are a few facts about determinants that are good to know. 
	</p>
	<fact>
		<p>
		Let <m>\bA</m> and <m>\bB</m> be <m>n\times n</m>, and let <m>c</m> be a scalar. Then 
		<ol>
			<li><m>\det(c\bA) = c^n \det(\bA)</m>,</li>
			<li><m>\det(\bA\bB) = \det(\bA)\det(\bB)</m>,</li>
			<li><m>\det(\bA)=0</m> if and only if <m>\bA</m> is singular, and</li>
			<li>If <m>\bA</m> is nonsingular, <m>\det(\bA^{-1})=1/\det(\bA)</m>.</li>
		</ol>
		</p>
	</fact>
	<p>The determinant condition is often the easiest way to check for singularity of a small matrix by hand.</p>
</section>  

<section xml:id="la-inverse">
		<title>Inverses</title>
		<subsection>
			<title>Inverse matrix</title>
			<p>
			For scalars, the multiplicative inverse of <m>a</m> is the reciprocal, <m>a^{-1}</m>, except that zero has no inverse. Now we investigate the same question for square matrices. 
			</p>
			<definition>
				<title>Inverse matrix</title>
				<statement>
					<p>The <term>inverse</term> of an <m>n\times n</m> matrix <m>\bA</m> is a matrix <m>\bC\in\Rmn{n}{n}</m> such that <m> \bA\bC = \bC\bA = \bI </m>. If such a <m>\bC</m> exists, we write <m>\bA^{-1}=\bC</m> and say that <m>\bA</m> is <term>invertible</term>; otherwise, it is <term>singular</term>.</p>
				</statement>
			</definition> 		
			<aside><p>Inverses are possible for square matrices only. The concept does not directly translate to rectangular matrices.</p></aside>
			<p>
			We are going to use without proof two important facts about matrix inverses. First, if an inverse exists, it is unique, so we can talk about <q>the</q> inverse of a matrix. And second, a left-inverse is automatically a right-inverse, and vice versa, so you can check just one of the equalities in the definition and the other comes for free.
			</p>
			<p>
			Because the zero matrix gives zero for every multiplicative partner, it cannot have an inverse. However, unlike scalars, <alert>there are nonzero matrices that are singular</alert>. The simplest is 
			<me>
				\bA = \begin{bmatrix}  0 \amp 1 \\ 0 \amp 0 \end{bmatrix}
			</me>. 
			(Proof: The second column of <m>\bC\bA</m> is zero for any square <m>\bC</m>.) 
			</p>
		</subsection>
		<subsection>
			<title>Inverses and linear systems</title>
			<p>
				If <m>\bA\in\Rmn{n}{n}</m> is invertible, then we can use its inverse to solve a linear system <m>\bA\bx=\bb</m>: 
				<me>
					\bA^{-1} \bb = \bA^{-1} (\bA \bx) = \bigl(  \bA^{-1} \bA \bigr) \bx = \bI \bx = \bx
				</me>. 
				The uniqueness of the inverse means that this solution is unique as well. We will see later on that the converse is also true: unique solution implies invertibility. 
			</p>
			<p>
				It's one thing to say <q>the inverse exists</q> and quite another to say <q>here it is.</q> Finding the inverse of a given matrix is in principle a matter of solving <m>n</m> linear systems. That's because if we let <m>\mathbf{e}_j</m> be the <m>j</m>th column of <m>\bI</m>, then according to the columnwise interpretation of <m>\bA\bA^{-1}</m>, the solution of <m>\bA \bx = \mathbf{e}_j</m> is the <m>j</m>th column of <m>\bA^{-1}</m>.
			</p>
			<p>
				Row elimination can be used to solve those systems simultaneously. However, it's all beside the point. If our goal is to solve a linear system <m>\bA\bx=\bb</m>, then there is little reason to solve <m>n</m> other systems first to get the inverse! The inverse tends to be much more important theoretically than for practical computation. 
			</p>
		</subsection>
		<subsection>
			<title><m>2\times 2</m> inverses and systems</title>
			<p>
				Happily, there is a simple formula for the matrix inverse in the <m>2\times 2</m> case:
				<men xml:id="la-eq-twotwoinv">
					\begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix}^{-1} = \frac{1}{ad-bc} \begin{bmatrix} d \amp -b \\ -c \amp a \end{bmatrix}
				</men>. 
				This formula breaks down if <m>ad=bc</m>, in which case <m>\det(\bA)=0</m> and the matrix is singular. You might remember that this is the same algebraic condition we derived for the linear system <m>\bA\bx=\bb</m> not having a unique solution. 
		  </p>
		  <p>
				Even though a 2x2 inverse is easy, it's still not the most convenient way to solve a linear system. There is an even faster equivalent shortcut known as <term>Cramer's Rule</term>:
				<md>
					 <mrow>x_1 \amp = \frac{ \twodet{b_1}{a_{12}}{b_2}{a_{22}} }{ \det(\bA) }</mrow>
					 <mrow>x_2 \amp = \frac{ \twodet{a_{11}}{b_1}{a_{21}}{b_2} }{ \det(\bA) }</mrow>   
				</md>.
		  </p>
		  <example>
				<p>
				For 
				<md>
					 <mrow>-x + 3y \amp = 1 </mrow>
					 <mrow>3x + y \amp = 7</mrow>
				</md>, 
				Cramer's Rule says
				<md>
					 <mrow>x \amp = \frac{ \twodet{1}{3}{7}{1} }{ \det(\bA) }=  \frac{ \twodet{1}{3}{7}{1} }{ \twodet{-1}{3}{3}{1} } = \frac{-20}{-10} = 2 </mrow>
					<mrow>y \amp = \frac{ \twodet{-1}{1}{3}{7} }{ \det(\bA) } = \frac{ \twodet{-1}{1}{3}{7} }{ \twodet{-1}{3}{3}{1} } = \frac{-10}{-10} = 1</mrow>            
				</md>.
				</p>
			</example>
			</subsection>
	</section>  
	
  <section xml:id="la-equivalent">
	  <title>Equivalent conditions</title>
	  <p>
		  You might have noticed by now that much of what we have covered is the same thing over and over with different notations and terminology. Here is where we pull all of that together.  
	  </p>
	  <theorem xml:id="la-thm-FTLA">
		  <title>Fundamental Theorem of Linear Algebra</title>
		  <statement><p>
			  Suppose <m>\bA\bx=\bb</m> is a linear system with an <m>n\times n</m> matrix <m>\bA</m>. Each of the following statements is equivalent to all of the others: 
			  <ol>
				  <li><m>\bA\bx=\bb</m> has a unique solution,</li>
				  <li>the linear system has no free variables,</li>
				  <li>the RREF of <m>\bA</m> is the <m>n\times n</m> identity matrix,</li>
				  <li><m>\bA</m> is invertible,</li>
				  <li>the columns of <m>\bA</m> are a basis of <m>\Rn{n}</m>,</li>
				  <li>the dimension of <m>\null(\bA)</m> is zero,</li>
				  <li><m>\det(\bA)\neq 0</m>, and</li>
				  <li><m>\bA</m> does not have a zero eigenvalue.</li>
			  </ol>
		  </p></statement>
	  </theorem>
	  <p>
		  The last equivalent statement is a placeholder in anticipation of <xref ref="la-eigen"/>. 
	  </p>
	  <p>
		  There are related statements that can be made regarding nonsquare matrices, but we aren't going to get into them. It is worth knowing, though, that the square case is the only one in which a unique solution is a possibility. If the matrix is singular, then that is ruled out, and there may be zero or an infinity of solutions, depending on both <m>\bA</m> and <m>\bb</m>. 
	  </p>
  </section>  

<section xml:id="la-complex">
	<title>Complex numbers</title>
	<introduction>
		<p>(For a really smart and entertaining introduction to complex numbers, I recommend <url href="https://youtu.be/T647CGsuOVU">this video series</url>.)</p>

		<p>We need to take a short but highly relevant detour to talk about complex numbers.</p> 

		<p>There's often a lot of uneasiness about complex numbers. Terminology is part of the reason. Using "real" and "imaginary" to label numbers is the residue of a value judgment that ruled mathematics for centuries. But complex numbers are actually just as "real" as so-called real numbers. If anything, they are actually <em>more</em> fundamental to the universe.</p>

		<p>As a practical matter, you can pretty much always replace a complex value with two real ones, and vice versa. But sometimes the manipulations are a lot easier in the complex form. In particular, you may be able to replace trigonometry with algebra.</p> 
	</introduction>
		
	<subsection>
		<title>The reality of imaginary numbers (optional)</title>

		<p>Let's rewind a bit. We can probably take for granted the positive integers 1, 2, 3, and so on, and we'll throw in zero too (though this too was controversial for centuries). It's not long before we want to solve a problem like <m>x+1=0</m>. Annoyingly, we can pose the problem using just nonnegative integers, but we can't solve it. So we accept the existence of the negative integers.</p>

		<p>I can imagine quite a bit of skepticism about this historically. ("Sure, Wei. Ever seen a negative goat?") But if you've ever taken out a loan, you know that negative numbers can have very real consequences.</p> 

		<p>Eventually, the negative integers seem "obvious" and perfectly natural. But then we run into a problem like
		<me>
		2x - 1 = 0
		</me>.
		We can pose this problem with integers, but we can't solve it. So we get used to accepting rational numbers, too.</p>

		<p>Rational numbers are pretty weird. Between any pair of them, you have infinitely more rational numbers! Yet it turns out they have (huge) gaps as well. You can't solve
		<me>
		x^2 - 2 = 0
		</me>
		using only rational numbers. So, you're willing to take on irrational numbers too. Talk about weird--every one of them has an infinite, non-repeating decimal expansion.</p> 

		<p>So much for the "real" numbers. At least we have filled in the so-called number line. But then you get to
		<me>
		x^2 + 1 = 0
		</me>,
		which is purely "real" but insolvable. Solutions to this equation were widely resisted for a very long time (say, the 18th century), to the point they were called "imaginary" (thanks, Descartes). </p>

		<p>Yet something amazing happens if you do accept imaginary numbers, and their expansion to the complex numbers. Namely, <em>The Fundamental Theorem of Algebra</em>, which states that if you write down a polynomial using complex numbers, it will have only complex numbers as solutions. So there's no infinite ladder of hypercomplex numbers that we have to ascend--just one rung past the "real" ones.</p>
	</subsection>
	<subsection xml:id="la-cx-basic">
		<title>Basic arithmetic</title>
		<p>
		We can write a complex number <m>z\in \mathbb{C}</m> as <m>z=x+iy</m>, where <m>i^2=-1</m> and <m>x</m> and <m>y</m> are real numbers known as the <term>real</term> and <term>imaginary</term> parts of <m>z</m>,
		<me>z = x+iy \quad \Leftrightarrow \quad x = \Re(z), \quad y = \Im(z)</me>.
		Writing a complex number this way is equivalent to using rectangular or Cartesian coordinates in the plane to specify a point <m>(x,y)</m>. Thus complex numbers are on a <q>number plane</q> rather than a number line.
		</p>
		<p>
			There is a generalization of absolute value to complex numbers known as the <term>modulus</term>, a real nonnegative quantity easily computed via
			<me>|z|^2 = [\text{Re}(z)]^2 + [\text{Im}(z)]^2</me>. 
			Like the absolute value, <m>|z|</m> is the distance from <m>z</m> to the origin, and <m>|w-z|</m> is the distance between two complex numbers. Here, though, the distances are in the plane rather than on a line. 
		</p>
		<p>
			An important operation on complex numbers that has no real counterpart is the <term>conjugate</term>,
			<me>\bar{z} =\text{Re}(z) - i \,\text{Im}(z)</me>.
			Geometrically this is a reflection across the real axis of the plane. No matter how complicated an expression is, you just replace <m>i</m> by <m>-i</m> everywhere to get the conjugate. 
		</p>
		<p>
			You add, subtract, and multiply complex numbers by applying the usual algebraic rules, applying <m>i^2=-1</m> as needed. They should give little trouble. Division can be a little trickier, even though the rules are always the same. One trick is to give a complex ratio a purely real denominator:
			<me>\frac{w}{z} = \frac{w \bar{z}}{z \bar{z}} = \frac{w \bar{z}}{|z|^2}</me>. 
			This is a lot like rationalizing a denominator with square roots. Memorize the special case <m>1/i = -i</m>. 
		</p>
		<p>Here are some more simple rules to know: 
		</p>
		<fact>
			<title>Complex arithmetic</title>
			<statement>
				<p>For complex numbers <m>w</m> and <m>z</m>, 
					<ol>
						<li><m>|\bar{z}| = |z|</m></li>
						<li><m>|wz| = |w|\cdot |z|</m></li>
						<li><m>|w+z|\le |w| + |z|</m> (triangle inequality)</li>
						<li><m>\left| \frac{1}{z} \right| = \frac{1}{|z|}</m></li>
						<li><m>\overline{wz}=\bar{w}\cdot \bar{z}</m></li>
						<li><m>\overline{w+z}=\bar{w}+\bar{z}</m></li>
						<li><m>\overline{\left(\frac{1}{z} \right)} = \frac{1}{\bar{z}}</m></li>
						<li><m>|z|^2 = z\cdot \bar{z}</m></li>
					</ol>
				</p>
			</statement>
		</fact>
	</subsection>
	<subsection xml:id="la-cx-matrices">
		<title>Complex-valued matrices</title>
		<p>
			Up to now we have dealt entirely with vectors and matrices whose entries are real numbers. What changes if we consider ones with complex entries? 
		</p>
		<p>
			For us, almost nothing. When we consider the definitions of span and nullspace, we have to allow complex coefficients in linear combinations and complex vectors in the nullspace, and a determinant can be complex-valued. But the rules of matrix algebra, and the solution of linear systems, are unaffected. TBH, the only reason we started with real matrices instead of complex ones is that complex numbers tend to freak people out a little. 
		</p>
	</subsection>
</section>  

<section xml:id="la-eigen">
	<title>Eigenvalues</title>
	<introduction>
		<p>
			The last stop on our whirlwind tour of linear algebra is a topic that is very important to the differential equations we study in future chapters. (I say <q>very important</q> here in the sense of oxygen being a very important part of breathing.)
		</p>
		<aside><p>Everything in this section still relates only to square matrices.</p></aside>
		<definition xml:id="la-df-eigenvalue">
			<title>Eigenvalue and eigenvector</title>
			<statement><p>
				Suppose <m>\bA\in\Cmn{n}{n}</m>. If there exist a scalar <m>\lambda</m> and a nonzero vector <m>\bv</m> such that 
				<me>\bA \bv = \lambda \bv</me>,
				then <m>\lambda</m> is an <term>eigenvalue</term> of <m>\bA</m> with associated <term>eigenvector</term> <m>\bv</m>. 
			</p></statement>
		</definition>
		<p>
			If you think of <m>\bA</m> as acting on vectors, then an eigenvector is a direction in which the action of <m>\bA</m> is the same as a scalar; we have found a little one-dimensional oasis in which the behavior of <m>\bA</m> is easy to comprehend. 
		</p>
	</introduction>
	<subsection xml:id="la-ei-eigenspace">
	<title>Eigenspaces</title>
		<p>
			An eigenvalue is a clean, well-defined target. Eigenvectors can be slipperier. For starters, if <m>\bA\bv=\lambda\bv</m>, then
			<me>
				\bA(c\bv) = c(\bA\bv)=c(\lambda\bv)=\lambda(c\bv)
			</me>. 
			Hence <alert>every nonzero multiple of an eigenvector is also an eigenvector for the same <m>\lambda</m></alert>. But there can be even more ambiguity than that. 
		</p>
		<example> 
			<p>Let <m>\bI</m> be an identity matrix. Then <m>\bI\bx=\bx</m> for any vector <m>\bx</m>, so every nonzero vector is an eigenvector!</p>
		</example>
		<p>
			Fortunately we already have the tools we need to describe a more robust target, based on the very simple reformulation <m>\bzero=\bA\bv-\lambda\bv=(\bA-\lambda\bI)\bv</m>. 
		</p>
		<definition xml:id="la-df-eigenspace">
			<title>Eigenspace</title>
			<statement><p>
				Let <m>\lambda</m> be an eigenvalue of <m>\bA</m>. The <term>eigenspace</term> associated with <m>\lambda</m> is <m>\null(\bA-\lambda\bI)</m>. 
			</p></statement>
		</definition>
		<p>
			Eigenspaces, unlike eigenvectors, are unique. We have to be a bit careful, though, because we usually express such spaces as the span of basis vectors, and those bases are not themselves unique. For instance, <m>\span\left(\twovec{1}{0},\twovec{0}{1}\right)</m> and <m>\span\left(\twovec{1}{1},\twovec{-1}{1}\right)</m> are both equal to <m>\Cn{2}</m>. 
		</p>
		<p>
		Note that if <m>\lambda</m> is <em>not</em> an eigenvalue, then (by definition) the only solution of <m>(\bA-\lambda\bI)\bv=\bzero</m> is <m>\bv=\bzero</m>. In other words, the dimension of <m>\null(\bA-\lambda\bI)</m> is zero, in which case (by <xref ref="la-thm-FTLA"/>) <m>\bA-\lambda\bI</m> is invertible. 
		</p>
		<fact><p><m>\lambda</m> is an eigenvalue of <m>\bA</m> if and only if <m>\bA-\lambda\bI</m> is singular.</p></fact>
		<p>
			In practice the most common way to find eigenvalues by hand is through the equivalent condition <m>\det(\bA-\lambda\bI)=0</m>. 
		</p>
		<example>
			<p>
				We look for eigenvalues of 
				<me>
					\bA = \begin{bmatrix} 1 \amp 1 \\ 4 \amp 1 \end{bmatrix}
				</me>. 
				Start by computing 
				<me>
					\left| \twomat{1}{1}{4}{1} - \twomat{\lambda}{0}{0}{\lambda} \right| = \twodet{1-\lambda}{1}{4}{1-\lambda} = (1-\lambda)^2 - 4 = \lambda^2-2\lambda-3
				</me>. 
				We find eigenvalues by setting this equal to zero, which means <m>\lambda_1=3</m> or <m>\lambda_2=-1</m>. These are the eigenvalues. 
			</p>
			<p>
				What about the eigenspaces? First consider <m>\lambda_1=3</m>: 
				<me>
					\bA-3 \bI = \twomat{-2}{1}{4}{-2} \overset{\text{RREF}}{\Longrightarrow} \twomat{1}{-1/2}{0}{0} 
				</me>. 
				Therefore this nullspace (the eigenspace) is <m>\span\left(\twovec{1/2}{1}\right)</m>. As for <m>\lambda_2=-1</m>, 
				<me>
						\bA+ \bI = \twomat{2}{1}{4}{2} \overset{\text{RREF}}{\Longrightarrow} \twomat{1}{1/2}{0}{0} 
				</me>, 
				leading to the eigenspace <m>\span\left(\twovec{-1/2}{1}\right)</m>. 
			</p>
		</example>
		<aside>
		<p>
			In the <m>2\times 2</m> case, suppose <m>\lambda</m> is already known to be an eigenvalue. Let the first row of <m>\bA-\lambda\bI</m> be <m>[\alpha,\beta]</m>. If <m>\alpha=\beta=0</m>, then the eigenspace is all of <m>\Cn{2}</m>. Otherwise, the eigenspace is <m>\span\left(\twovec{-\beta}{\alpha}\right)</m>. 
		</p>
		</aside>
		<p>
			In the general <m>n\times n</m> case, <m>\det(\bA-\lambda\bI)</m> is a polynomial of degree <m>n</m> whose roots are the eigenvalues of <m>\bA</m>. (This can be seen through the cofactor expansion definition of the determinant.) 
		</p>
		<definition xml:id="la-df-charpoly">
			<title>Characteristic polynomial</title>
			<statement><p>
				The <term>characteristic polynomial</term> of <m>\bA</m> is <m>\det(\bA-\lambda\bI)</m>.
			</p></statement>
		</definition>
		<p>
			Given what we know about roots of polynomials, there are some conclusions it's worth stating clearly. 
		</p>
		<theorem xml:id="la-thm-eigenvalues">
			<title>Eigenvalue properties</title>
			<statement><p>
				Suppose <m>\bA</m> is an <m>n\times n</m> matrix. Then
				<ol>
					<li><m>\bA</m> has at most <m>n</m> distinct complex eigenvalues, and</li> 
					<li>if <m>\bA</m> is real, then any complex eigenvalues occur in conjugate pairs, as do their assoicated eigenvectors.</li>
				</ol>
			</p></statement>
		</theorem>
		<example xml:id="la-ex-defective">
				<p>
					 Consider <m>\bA=\twomat{4}{0}{1}{4}</m>. The characteristic polynomial is 
					 <me>
						 \twodet{4-\lambda}{0}{1}{4-\lambda} = (4-\lambda)^2
					 </me>,
					 so <m>\lambda_1=4</m> is the only eigenvalue. Since 
					 <me>
						  \bA - 4\bI = \twomat{0}{0}{1}{0}
					 </me>, 
					 the nullspace consists of all vectors <m>\twovec{0}{s}</m>, i.e., it is <m>\span\left(\twovec{0}{1}\right)</m>. 
				</p>
		</example>
		<example>
			<p>
				We find the eigenvalues and eigenvectors of
				<me>
					\begin{bmatrix} 1 \amp -1 \\ 5 \amp -3 \end{bmatrix}
				</me>.  
				The characteristic polynomial is <m>\lambda^2 +2 \lambda +2</m>, with roots  <m>\lambda_{1,2} = -1 \pm 1i.</m>
			</p>
			<p>
				To find an eigenvector for <m>\lambda_1</m>, we note that the first row of <m> \bA - \lambda_1 \mathbf{I} </m> is <m>[(2-i),-1]</m>. Thus a basis for this eigenspace is the single eigenvector <m>\twovec{1}{2-i}</m>.
			</p>
			<p>
				We get a benefit here from the complex eigenvalues: the conjugate of an eigenvector for <m>\lambda_1</m> will be an eigenvector for <m>\lambda_2</m>. So we have <m>\span\left(\twovec{1}{2+i}\right)</m> to go with <m>\lambda_2</m>. 
			</p>
		</example>
	</subsection>
	<subsection xml:id="la-ei-multiplicity">
		<title>Multiplicity</title>
			<p>
				You should be familiar with the idea of multiplicity for roots of a polynomial. We can factor a characteristic polynomial <m>p</m> to get 
				<me>
					p(z) = (z-\lambda_1)^{m_1}(z-\lambda_2)^{m_2}\dots(z-\lambda_k)^{m_k}
				</me>
				for nonnegative integer exponents such that <m>m_1+\cdots+m_k=n</m>. These exponents are the mulitiplicites of the roots. 
			</p>
			<definition xml:id="la-df-algmult">
				<title>Algebraic multiplicity</title>
				<statement><p>
					The <term>algebraic multiplicity</term> of an eigenvalue is its multiplicity as a root of the characteristic polynomial. 
				</p></statement>
			</definition>
			<p>
				But eigenvalues have another significant notion of multiplicity as well. 
			</p>
			<definition xml:id="la-df-geommult">
				<title>Geometric multiplicity</title>
				<statement><p>
					The <term>geometric multiplicity</term> of an eigenvalue is the dimension of its associated eigenspace. 
				</p></statement>
			</definition>
			<fact>
				<p>
					The geometric multiplicity of an eigenvalue is less than or equal to its algebraic multiplicity. 
				</p>
			</fact>
			<p>
				If the geometric multiplicity of <m>\lambda</m> is strictly less than its algebraic multiplicity, we say that <m>\lambda</m> is <term>defective</term>. 
			</p>
			<example>
				<p>
					The <m>n\times n</m> identity matrix has characteristic polynomial <m>p(z)=(z-1)^n</m> (by recursive cofactor expansion). Thus its only eigenvalue is <m>\lambda=1</m>, with algebraic multiplicity <m>n</m>. All nonzero vectors are eigenvectors, so the dimension of the eigenspace is also <m>n</m>. Hence <m>\lambda=1</m> is nondefective. 
				</p>
			</example>
			<example>
					<p>
						<xref ref="la-ex-defective"/> showed that the matrix <m>\bA=\twomat{4}{0}{1}{4}</m> has eigenvalue <m>\lambda_1=4</m> with algebraic multiplicity 2 and geometric multiplicity 1. Thus this eigenvalue (and by extension the matrix itself) is defective.
					</p>
			</example>
			<p>
				Since multiplicities are always at least one, there is one simple but common case in which we are certain that there are no defective eigenvalues. 
			</p>
			<theorem xml:id="la-thm-distincteigs">
				<title>Distinct eigenvalues</title>
				<statement><p>
					If <m>\bA\in\Cmn{n}{n}</m> has <m>n</m> distinct eigenvalues, then <m>\bA</m> has no defective eigenvalues. 
				</p></statement>
			</theorem>
			<p>
				This is definitely not also an <q>only if</q> statement, though, as demonstrated by the identity matrix. 
			</p>
	</subsection>
</section>  

<section xml:id="la-diagonalization">
	<title>Diagonalization</title>
	<introduction>
		<p>
			Say <m>\bA</m> is <m>n\times n</m>, and that we write 
			<me>
				\bA \bv_1 = \lambda_1 \bv_1, \;
				\bA \bv_2 = \lambda_2 \bv_2,\; \dots, \;
				\bA \bv_n = \lambda_n \bv_n
			</me>,
			where <m>\lambda_1,\lambda_2,\dots,\lambda_n</m> are its eigenvalues (including multiplicities). Through some linear algebra magic, we can express this collection of equations more succinctly: 
			<md>
				<mrow>
						\begin{bmatrix} 
							\bA \bv_1 \amp \bA \bv_2 \amp \cdots \amp \bA \bv_n 
						\end{bmatrix} 
						\amp = 
						\begin{bmatrix} 
							\lambda_1\bv_1 \amp \lambda_2\bv_2 \amp \cdots \amp \lambda_n\bv_n 
						\end{bmatrix}
				</mrow>
				<mrow>\bA \begin{bmatrix} \bv_1 \amp \bv_2 \amp \cdots \amp \bv_n \end{bmatrix}            \amp = 
						\begin{bmatrix} \bv_1 \amp \bv_2 \amp \cdots \amp \bv_n \end{bmatrix} 
						\begin{bmatrix} \lambda_1 \amp 0 \amp \cdots \amp 0 \\
										0 \amp \lambda_2 \amp \cdots \amp 0 \\
										\amp \amp \ddots \amp \\
										0 \amp \cdots \amp 0 \amp \lambda_n
						\end{bmatrix}</mrow>
				<mrow>\bA \bV \amp = \bV \bD </mrow>           
			</md>, 
			where <m>\bD</m> is a <term>diagonal</term> matrix. 
		</p>
		<p>
			If <m>\bV</m> is nonsingular, we therefore can write 
			<men xml:id="la-eq-diagonalization">
				\bA = \bV \bD \bV^{-1}
			</men>, 
			which is called a <term>diagonalization</term> of <m>\bA</m>. A diagonalization is not unique; after all, we can take the eigenvalues in any order, and the eigenvectors themselves are not unique. 
		</p>
		<example>
			<title>A 2-by-2 diagonalization</title>
			<p>
				Let's find a diagonalization of 
				<me>
						\mathbf{A} =  \begin{bmatrix} 1 \amp 1 \\ 4 \amp 1 \end{bmatrix}
				</me>. 
				We found the eigenvalues and eigenvectors of this guy in the previous section. We got <m>\lambda_1=3</m> with <m>\bv_1 = \twovec{1}{2}</m>, and <m>\lambda_2=-1</m> with <m>\bv_2 =\twovec{1}{-2}</m>. So we paint by numbers to get
				<me>
						\mathbf{D} = \begin{bmatrix} 3 \amp 0 \\ 0 \amp -1 \end{bmatrix}, \qquad \bV = \begin{bmatrix} 1 \amp 1 \\ 2 \amp -2 \end{bmatrix}
				</me>. 
				We "just" need to invert <m>\bV</m>; fortunately, we're only at <m>2\times 2</m>, so this is a snap.
				<me> 
						\bV^{-1} = \frac{1}{-4} \begin{bmatrix} -2 \amp -1 \\ -2 \amp 1 \end{bmatrix} = \begin{bmatrix} 1/2 \amp 1/4 \\ 1/2 \amp -1/4 \end{bmatrix}
				</me>.  
				Hence
				<me> 
						\mathbf{A} = \bV \mathbf{D} \bV^{-1} = 
						\begin{bmatrix} 1 \amp 1 \\ 2 \amp -2 \end{bmatrix} 
						\begin{bmatrix} 3 \amp 0 \\ 0 \amp -1 \end{bmatrix} 
						\begin{bmatrix} 1/2 \amp 1/4 \\ 1/2 \amp -1/4 \end{bmatrix}
				</me>. 
			</p>
		</example>
		<p>
			Not every matrix has a diagonalization. The essential issue is whether <m>\bV</m> is invertible. We already have the condition needed to ensure this.  
		</p>
		<theorem xml:id="la-thm-diagonalizable">
			<title>Diagonalizability</title>
			<statement><p>
				A matrix is diagonalizable if and only if it has no defective eigenvalues.
			</p></statement>
		</theorem>
		<p>
			If we put that together with <xref ref="la-thm-distincteigs"/>, then we can also conclude that <alert>a matrix whose eigenvalues are all simple (multiplicity one) is diagonalizable</alert>. Any repeated eigenvalues might or might not be defective, and investigation of the eigenspace is required in order to determine which. In the <m>2\times 2</m> case, though, we can make a simpler determination.
		</p> 
		<fact>
			<p>A <m>2\times 2</m> matrix with repeated eigenvalue is either a multiple of the identity, in which case it is already diagonal, or it is defective.</p>
		</fact>
	</introduction>
	<subsection xml:id="la-di-basis">
		<title>Eigenvector basis (optional)</title>
		<p>
			Equation <xref ref="la-eq-diagonalization"/> probably looks quite strange. It was derived by writing out all of the eigenpair equations simulataneously, and we will see that it has some uses later on. But what is it about?  
		</p>
		<p>
			Focus first on the inner diagonal matrix <m>\bD</m>. Diagonal matrices are about as simple as matrices get: for any vector <m>\bu</m>,
			<me>
				\begin{bmatrix} \lambda_1 \amp \amp \amp \\ \amp \lambda_2 \amp \amp \\ \amp \amp \ddots \amp \\ \amp \amp \amp \lambda_n \end{bmatrix} 
				\bu = \begin{bmatrix} \lambda_1 u_1 \\ \lambda_2 u_2 \\ \vdots \\ \lambda_n u_n \end{bmatrix}
			</me>. 
			Notice how in the diagonal case <em>all the vector components are completely decoupled</em>e. Now, given a vector <m>\bx</m>, define <m>\bu=\bV^{-1}\bx</m>. Then clearly 
			<me>
				\bx = \bV\bu = u_1 \bv_1 +\cdots + u_n \bv_n
			</me>. 
			In words, the entries of <m>\bu</m> are the coefficients for expressing <m>\bx</m> as a linear combination of eigenvectors. We call these the <term>coordinates</term> of <m>\bx</m> in an  <term>eigenvector basis</term> (consider <xref ref="la-thm-FTLA"/>). 
		</p>
		<p>
			Let's put all this together: 
			<me>
				\bA \bx = (\bV \bD \bV^{-1})\bx = \bV \bD (\bV^{-1}\bx) 
			</me>,
			which implies that 
			<me>
				\bV^{-1} ( \bA\bx) = \bD (\bV^{-1}\bx) 
			</me>,
			which states that <alert>the action of <m>\bA</m> on any vector is diagonal when cast into eigenvector coordinates</alert>.
		</p>
	</subsection>
</section>
		
</chapter>
