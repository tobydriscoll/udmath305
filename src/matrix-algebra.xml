<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="matrix-algebra" xmlns:xi="http://www.w3.org/2001/XInclude">
	
<title>Matrix algebra</title>

<section xml:id="ma-rows-columns">
    <title>Rows and columns</title>
    <introduction>
        <p>
            Suppose we have a system of <m>n</m> linear ODEs, 
            <md>
                <mrow>\dd{x_1}{t} \amp = a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n - b_1 </mrow>
                <mrow>\dd{x_2}{t} \amp = a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n - b_2 </mrow>
                <mrow>\amp \vdots </mrow>
                <mrow>\dd{x_n}{t} \amp = a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nn} x_n - b_n </mrow>
            </md>. 
            A steady state of this system occurs when 
            <md>
                <mrow>a_{11} x_1 + a_{12} x_2 + \cdots + a_{1n} x_n \amp = b_1 </mrow>
                <mrow>a_{21} x_1 + a_{22} x_2 + \cdots + a_{2n} x_n \amp = b_2 </mrow>
                <mrow>\amp \vdots </mrow>
                <mrow>a_{n1} x_1 + a_{n2} x_2 + \cdots + a_{nn} x_n \amp = b_n </mrow>
            </md>, 
            which is a linear algebraic system of equations. We can summarize the system with an <m>n\times n</m> matrix <m>\bA</m> and an <m>n\times 1</m> vector <m>\bb</m>. Let's start slowly, with some examples in <m>n=2</m> dimensions.
        </p>
    </introduction>
    <subsection>
    <title>Nonparallel lines</title>
    <p>
        We can set the stage for matrices using a simple set of two linear equations in two variables, such as 
        <md>
            <mrow>3\amp x  + \amp y   = \amp 7</mrow>
            <mrow>-\amp x  + \amp 3y  = \amp 1</mrow>
        </md>. 
        This system happens to have the unique solution <m>x=2</m>, <m>y=1</m>. We can summarize it in terms of a coefficient matrix
        <me>
            \bA = \begin{bmatrix} 
            3 \amp 1 \\ -1 \amp 3
            \end{bmatrix}
        </me>
        and a vector
        <me>
            \bb = \twovec{7}{1}
        </me>. 
        We can interpret the system in two different ways, depending on whether we organize by rows (associated with equations) or columns (associated with variables). 
    </p>
    <p>
        Each row of the system is an equation describing a line in the <m>(x,y)</m> plane. The intersection of the lines is the point representing the solution of the system. 
    </p>
    <sidebyside>
        <program language="matlab" xml:id="ma_rc_rows">
        <input>
A = [ 3 1; -1 3 ];
b = [7 ; 1];

fimplicit(@(x,y) A(1,1)*x+A(1,2)*y-b(1),[-1 3])
hold on
fimplicit(@(x,y) A(2,1)*x+A(2,2)*y-b(2),[-1 3])

axis([-1 3 -1 3])
plot([0 0],[-1 3],'k-','linewid',2)
plot([-1 3],[0 0],'k-','linewid',2)

axis equal, grid on
xlabel('x'), ylabel('y')
        </input>
        </program>
        <image source="figures/ma_rc_rows.svg"/>	  
    </sidebyside>
    <p>
        If we were to choose a different vector <m>\bb</m>, the lines in the picture would keep their same slopes and just move up and down. Thus we expect one unique solution (intersection) for any given <m>\bb</m>.
    </p>

    <p> 
        The column viewpoint is less familiar. We interpret the system as an equivalent statement about plane vectors:
        <me> 
            x \twovec{3}{-1} + y \twovec{1}{3} = \twovec{7}{1}
        </me>. 
        We call this a <term>linear combination</term> of the columns of the matrix. 
        Vectors are equal only if all their components are equal, so the meaning is unchanged. But it invites a different picture, interpreting <m>x</m> and <m>y</m> as coefficients and making <m>\bb</m> the target.
    </p>
    <sidebyside>
        <program language="matlab" xml:id="ma_rc_cols">
        <input>
A = [ 3 1; -1 3 ];
b = [7 ; 1];

clr = get(gca,'colororder');
plot([0 2*A(1,1) b(1) A(1,2)],[0 2*A(2,1) b(2) A(2,2)],'--','color',[.5 .5 .5])
hold on
plot([0 A(1,1)],[0 A(2,1)],'color',clr(1,:))
plot([0 A(1,2)],[0 A(2,2)],'color',clr(2,:))
plot([0 b(1)],[0 b(2)])

axis equal, grid on
axis([-2 8 -5 5])
plot([0 0],[-5 5],'k-','linewid',2)
plot([-2 8],[0 0],'k-','linewid',2)

xlabel('b_1'), ylabel('b_2')
        </input>
        </program>
        <image source="figures/ma_rc_cols.svg"/>	  
    </sidebyside>
    <p>
        We might think of solving the system in this picture as expressing <m>\bb</m> in a different coordinate system defined by the columns of <m>\bA</m>. In fact, it's just a rotation and scaling of the default Cartesian system, so again from this perspective we expect a unique result for each <m>\bb</m>.
    </p>
    </subsection>
    <subsection>
        <title>Matrix times vector</title>
        <p>
            The column-oriented interpretation above is the foundation for our first algebraic operation with matrices. If <m>\bA</m> is an <m>n\times n</m> matrix and <m>\bx</m> is an <m>n\times 1</m> vector, then we define 
            <me>
                \bA\bx = x_1 \begin{bmatrix} a_{11}\\a_{21}\\\vdots\\a_{n1} \end{bmatrix} 
                + x_2 \begin{bmatrix} a_{12}\\a_{22}\\\vdots\\a_{n2} \end{bmatrix} 
                + \cdots + x_n \begin{bmatrix} a_{1n}\\a_{2n}\\\vdots\\a_{nn} \end{bmatrix} 
            </me>. 
            That is, <em>a matrix times a vector is a linear combination of the columns of the matrix</em>. 
        </p>
        <example>
            <p>
                We can compute
                <me>
                    \begin{bmatrix} 1 \amp 0 \amp -3 \\ 2 \amp -2 \amp 1 \\ 0 \amp 4 \amp 1 \end{bmatrix}
                    \begin{bmatrix} 2 \\ 1 \\ -2 \end{bmatrix} 
                    = 2 \begin{bmatrix} 1 \\ 2 \\ 0 \end{bmatrix} + 1 \begin{bmatrix} 0 \\ -2 \\ 4 \end{bmatrix} 
                    - 2 \begin{bmatrix} -3 \\ 1 \\ 1 \end{bmatrix} 
                    = \begin{bmatrix} 8 \\ 0 \\ 2 \end{bmatrix} 
                </me>. 
                Usually this sort of computation is not written out as a linear combination. Instead you just kind of zip together each row of <m>\bA</m> with the given vector:
                 <me>
                    \begin{bmatrix} 1 \amp 0 \amp -3 \\ 2 \amp -2 \amp 1 \\ 0 \amp 4 \amp 1 \end{bmatrix}
                    \begin{bmatrix} 2 \\ 1 \\ -2 \end{bmatrix} 
                    = \begin{bmatrix} (1)(2) + (0)(1) + (-3)(-2) \\ (2)(2) + (-2)(1) + (1)(-2) \\ (0)(2) + (4)(1) + (1)(-2) \end{bmatrix}
                </me>.  
            </p> 
        </example>
        <p>
            This definition of matrix times vector justifies writing the linear system of equations at the top of this section as 
            <me>
                \dd{\bx}{t} = \bA \bx - \bb
            </me>
            for the ODEs, and 
            <me>
                \bA \bx = \bb
            </me>
            for the linear algebraic system determining the steady state. 
        </p>
        <fact>
            <p>The linear system with coefficient matrix <m>\bA</m> and right-side vector <m>\bb</m> is about solving <m>\bA\bx=\bb</m> for <m>\bx</m>.</p>    
        </fact>

    </subsection>
    <subsection>
    <title>Parallel lines</title>
    <p> 
        Another instructive case is the system 
        <md>
            <mrow>\amp x  \amp+  2  y   = \amp 1</mrow>
            <mrow>\amp -2  x   \amp- 4  y  = \amp -1</mrow>
        </md>.  
    </p>
    <sidebyside>
        <program language="matlab" xml:id="ma_rc_parallel">
        <input>
A = [ 1 2; -2 -4 ];
b = [1 ; 1];

clf
fimplicit(@(x,y) A(1,1)*x+A(1,2)*y-b(1),[-2 2])
hold on
fimplicit(@(x,y) A(2,1)*x+A(2,2)*y-b(2),[-2 2])

axis equal, grid on
axis([-2 2 -2 2])
plot([0 0],[-2 2],'k-','linewid',2)
plot([-2 2],[0 0],'k-','linewid',2)
xlabel('x'), ylabel('y')
        </input>
        </program>
        <image source="figures/ma_rc_parallel.svg"/>	  
    </sidebyside>
    <p>
        The row picture makes it clear that we are seeking the intersection of two distinct parallel lines, an enterprise that is bound to disappoint us. Likewise, the column picture (not given) is about finding combinations of the column vectors <m>\twovec{1}{-2}</m> and <m>\twovec{2}{-4}</m>. These too are parallel, so all of their linear combinations lie on a single line through the origin. The given vector <m>\bb=\twovec{1}{1}</m> is not on this line, so no solution exists. 
    </p>
    <p> 
        However, a different vector on the right-hand side such as <m>\bb=\twovec{-3}{6}</m> does lie on this line, and we <em>can</em> solve the system in this case. In fact, there are infinitely many ways to do it:
        <md>
            <mrow>(1+2c) \twovec{1}{-2} - (2+c)\twovec{2}{-4} \amp = \left( \twovec{1}{-2} - 2 \twovec{2}{-4}  \right) + c \left( 2\twovec{1}{-2} - \twovec{2}{-4}  \right) </mrow>
            <mrow>\amp = \twovec{-3}{6} + c\cdot \textbf{0}</mrow>   
        </md>.
        In this situation, the corresponding row picture still has two parallel lines, but they conincide, also indicating the infinite set of solutions. For parallel lines, you can have zero or infinitely many solutions; there is nothing in between. 
    </p>

    </subsection>
    <subsection>
    <title>Singular matrices</title>
    <p>
        Although we have only examined two two-dimensional cases, they contain most of what we will see in general. A system of <m>n</m> linear equations in <m>n</m> variables with a particular matrix <m>\bA</m> will be one of two basic types: 
        <ol>
            <li>There is a unique solution for any choice of <m>\bb</m>, or</li>
            <li>Depending on <m>\bb</m>, there is no solution or infinitely many solutions.</li>
        </ol>
        In the second case we say the matrix <m>\bA</m> is <term> singular</term>, and in the first case we say it is <term>nonsingular</term> (or "invertible"). 
    </p>
    <p>
        The singular case has a familiar pattern to it. If there is a linear combination of the columns of <m>\bA</m> that gives a zero vector--that is, if the linear system has a null solution--then any multiple of that null solution can be added to a particular solution in order to get another particular solution. 
    </p>
 
    </subsection>
 </section>

 <section xml:id="ma-elimination">
    <title>Elimination for linear systems</title>

    <introduction>
       <p><em>From Section 4.2.</em></p>
        <p>
          Whenever we are faced with a linear system of <m>n</m> equations in <m>n</m> variables, we can solve it by transforming it step by step into an equivalent system that is easier to solve. This process is most transparent for the two-dimensional case, so we start there.           
        </p>
    </introduction>

    <subsection>
    <title>Two variables</title>
    <p>
        Consider the system
        <md>
            <mrow>-x + 3y \amp = 1 </mrow>
            <mrow>3x + y \amp = 7</mrow>
        </md>.  
        We can use the first equation to eliminate <m>x</m> from the second. Specifically, we multiply the first equation by <m>3</m> and add it to the second to get the system
        <md>
            <mrow>-x + 3y \amp = 1 </mrow>
            <mrow> 10 y \amp = 10</mrow>
        </md>.  
        It's now trivial to solve the second equation for <m>y=1</m>. With that known, the first equation can be solved to find <m>x=-(1-3)=2</m>.  
    </p>
    <p>
        The key to making the elimination work was the zero we forced into the second row of matrix representing the second system:
        <me>
            \begin{bmatrix} 
            -1 \amp 3 \\ 0 \amp 10
            \end{bmatrix}
        </me>. 
        This matrix has what we call and <term>upper triangular</term> structure. That structure is our goal, because then we can sort of unzip the solution by starting at the bottom and working our way up. 
    </p>
     </subsection>

     <subsection>
         <title>More variables</title>
         <p>
             Here's a system of four equations:
             <md>
                 <mrow>2x_1 + 0x_2 + 4x_3 + 3x_4 \amp = 4</mrow>
                 <mrow>-2x_1 + 0x_2 + 2x_3 - 13x_4 \amp = 40</mrow>
                 <mrow>x_1 + 15x_2 + 2x_3 - 4.5x_4 \amp = 29</mrow>
                 <mrow>-4x_1 + 5x_2 - 7x_3 - 10x_4 \amp = 9</mrow>
             </md>. 
             To begin with, all we really care about are the numbers (coefficients) in these equations. Everything else always remains the same. Let us define
            <me>
                \bA = 
                \begin{bmatrix}
                2 \amp 0 \amp 4 \amp 3 \\
                -2 \amp 0 \amp 2 \amp -13 \\
                1 \amp 15 \amp 2 \amp -4.5 \\
                -4 \amp 5 \amp -7 \amp -10   
                \end{bmatrix},
                \qquad \bb = 
                \begin{bmatrix} 4\\ 40\\ 29 \\9 \end{bmatrix}
            </me>. 
            Now we show how elimination proceeds on this matrix, but <url href="elimination.html">using matlab to handle the arithmetic</url> for us. 
         </p>
         <p>
             To summarize, we proceed through the rows <m>1,2,\ldots,n-1</m>. For row <m>i</m> we seek to put zeros below the <term>pivot element</term> in the <m>(i,i)</m> position. To do that we choose a multiplier for each <m>k>i</m> and add that multiple of row <m>i</m> to row <m>k</m>. The one thing that can stop us is if the pivot element is zero, because we can't find multipliers. In that event we swap row <m>i</m> with a lower row. After the eliminations are done, the system is in upper triangular form and can be solved by backward substitution.  
         </p>
     </subsection>

     <subsection>
         <title>Breakdown <m>=</m> singularity</title>
         <p>
             We have left one logical possibility unaddressed. What if we not only have a zero pivot, but all the candidate pivots below it are zero too? As a practical matter, this form of "breakdown" is benign; there is no need to eliminate the current variable because it is already eliminated! We can move down to the next row in the elimination process. To put it another way, we will end up with an upper triangular matrix with a zero in some <m>(i,i)</m> position (on the <term>diagonal</term> of the matrix). 
         </p>
         <p>
             But there is a crucial theoretical implication too.            
         </p>
         <theorem>
             <title>Elimination breakdown equals singularity</title>
             <p>
                 The elimination process is unable to find a nonzero pivot at some point in the algorithm if and only if the original system matrix is singular. 
             </p> 
         </theorem>
        <p>
            This behavior is manifested once we move on to the backward substitution phase. If some <m>(i,i)</m> element is zero in the triangular matrix, then we will be unable to solve for <m>x_i</m> when its turn comes. At that point, either the system holds true regardless of the value of <m>x_i</m> (which makes it a "free" variable), or the system is inconsistent and has no solution. 
        </p>
        <example>
            <title>Singular system of equations</title>
            <p> 
            Elimination for the singular system 
            <md>
                <mrow>\amp x  \amp+  2  y   = \amp 1</mrow>
                <mrow>\amp {-2}  x   \amp- 4  y  = \amp {-1}</mrow>
            </md>
            leads in one step to 
            <md>
                <mrow>\amp x  \amp+  2  y   = \amp 1</mrow>
                <mrow>\amp 0  x \amp + 0 y  = \amp 1</mrow>
            </md>. 
            The second equation is impossible to satisfy, hence there are no solutions. On the other hand, if we started with 
            <md>
                <mrow>\amp x  \amp+  2  y   = \amp {-3}</mrow>
                <mrow>\amp {-2}  x   \amp- 4  y  = \amp 6</mrow>
            </md>
            then we get 
            <md>
                <mrow>\amp x  \amp+  2  y   = \amp {-3}</mrow>
                <mrow>\amp 0  x \amp + 0 y  = \amp 0</mrow>
            </md>. 
            The second equation is satisfied no matter what we do. So we are "free" to choose <m>y</m>, and then the first equation tells us that <m>x=-3-2y</m>. In vector form, the system is solved by
            <me>
                \twovec{-3}{0} + y \twovec{-2}{1}
            </me>
            for any choice of <m>y</m>. We may interpret this as "particular plus null," because <m>\bA \twovec{-2}{1}=\boldsymbol{0}</m>. 
            </p>
        </example>
     </subsection>
 </section>

<section xml:id="ma-multiplication">
    <title>Multiplying matrices</title>
    <introduction>
        <p>From section 4.3 (skip "Elimination matrices"), with a little from 4.1 also.</p>
    <p>
        An <m>m\times n</m> matrix <m>\bA</m> is a rectangular <m>m</m>-by-<m>n</m> array of numbers called <term>elements</term> or <term>entries</term>.  The numbers <m>m</m> and <m>n</m> are called the <term>row dimension</term> and the <term>column dimension</term>, respectively; collectively they describe the <term>size</term> of <m>\bA</m>. We say <m>\bA</m> belongs to <m>\mathbb{R}^{m\times n}</m> if its entries are real or <m>\mathbb{C}^{m\times n}</m> if they are complex-valued.
    </p> 
    <p>
        A <term>square</term> matrix has equal row and column dimensions. A <term>row vector</term> has dimension <m>1\times n</m>, while a <term>column vector</term> has dimension <m>m \times 1</m>. By default, a vector is understood to be a column vector, and we use <m>\mathbb{R}^n</m> or <m>\mathbb{C}^n</m> to denote spaces of vectors.  An ordinary number may be called a <term>scalar</term>.
    </p>
    <p>
        We will mainly be dealing with scalars, vectors and square matrices. Please keep in mind that "scalar/vector" and "constant/variable" are separate attributes. All four combinations of them are possible. (And then there's "real/complex" as well.) 

        I use capital letters in bold to refer to matrices, and lowercase bold letters for vectors. The bold symbol <m>\boldsymbol{0}</m> may refer to a vector of all zeros or to a zero matrix, depending on context.
    </p>
    <p>
        To refer to a specific entry of a matrix, I do not use boldface. If you see a boldface character with one or more subscripts, then you know that it is a matrix or vector that belongs to a numbered collection.
    </p>
    <p>
        Adding and subtracting matrices and vectors is dead simple: if two things are the same size, you add/subtract them elementwise; otherwise, the operation is not permitted. (Caution: MATLAB <em>does</em> let you do that sometimes, and it's a great source of confusing, subtle bugs.) 
    </p>
    <p>
        However, multiplication and "division" are far more complicated for matrices than they are for scalars. 
    </p>
    </introduction>
    <subsection>
    <title>Dot products</title>
    <p>
        Suppose <m>\bu</m> and <m>\bv</m> are real column vectors of length <m>n</m>. Then their <term>dot product</term> (or <term>inner product</term>, or <term>scalar product</term> is 
        <me>
            \bu \cdot \bv = u_1 v_1 + u_2 v_2 + \cdots u_n v_n = \sum_i u_i v_i
        </me>.  
        This is equivalent to what you were used to doing in two and three dimensions in vector calculus. In fact, a well-known formula there holds here too:
        <me>
            \bu \cdot \bv = \| \bu \| \, \| \bv \| \, \cos(\theta)
        </me>,
        where <m>\|</m> is used to indicate Euclidean length (or <term>norm</term>) and <m>\theta</m> is the angle between the vectors. This is most important to us when the angle is a right angle, i.e., <m>\bu\cdot\bv = 0</m>, in which case we say the vectors are <term>orthogonal</term> (meaning perpendicular). 
    </p>
    <p>
        The dot product gives us a new way to describe the matrix-vector product <m>\bA\bx</m>. The product <m>\by=\bA\bx</m> is an <m>m\times 1</m> vector whose <m>i</m>th element is the dot product of <m>\bx</m> with the <m>i</m>th row of <m>\bA</m>. More symbolically,
        <me>
            y_i = \sum_{k=1}^n A_{ik} x_k, \qquad i=1,\ldots,m
        </me>. 
        This is, naturally, equivalent to the earlier definition of <m>\by</m> as a linear combination of the columns of <m>\bA</m>, with the combination coefficients coming from <m>\bx</m>.
    </p>
    </subsection>

    <subsection>
        <title>Matrix times matrix</title>
        <p>
            In order to define the product <m>\bA\bB</m>, we require that the number of columns in <m>\bA</m> is the same as the number of rows in <m>\bB</m>. Otherwise, the product is considered undefined. So let <m>\bA</m> be <m>m\times n</m> and <m>\bB</m> be <m>n\times p</m>.  One way to describe  <m>\bA\bB</m> is as a  generalization of the matrix-vector product:
            <me>
                \bA\mathbf{B} =
                    \bA \begin{bmatrix}
                    \mathbf{b}_1 \amp \mathbf{b}_2 \amp \cdots \amp \mathbf{b}_p
                    \end{bmatrix}
                    = \begin{bmatrix}
                    \bA\mathbf{b}_1 \amp \bA\mathbf{b}_2 \amp \cdots \amp \bA\mathbf{b}_p
                    \end{bmatrix}
            </me>.
            In words, a matrix-matrix product is the horizontal concatenation of matrix-vector products involving the columns of the right-hand matrix. Note that when we multiply <m>m\times n</m> by <m>n\times p</m>, the result is <m>m\times p</m>. 
        </p>
        <p>
            When we compute a matrix product by hand, we usually don't write out the above. Instead we use a more compact definition for the individual entries of <m>\bC = \bA\bB</m>,
            <me>
                C_{ij} = \sum_{k=1}^n A_{ik}B_{kj}, \qquad i=1,\ldots,m, \quad j=1,\ldots,p
            </me>. 
        </p>
        <example>
            <p>
                Let
                <me>
                    \bA = \begin{bmatrix}
                        1 \amp -1 \\ 0 \amp 2 \\ -3 \amp 1
                        \end{bmatrix}, \qquad
                        \mathbf{B} = \begin{bmatrix}
                        2 \amp -1 \amp 0 \amp 4 \\ 1 \amp 1 \amp 3 \amp 2
                        \end{bmatrix}
                </me>.  
                Then 
                <md>
                    <mrow>\bA\mathbf{B} \amp= \begin{bmatrix} 
                        (1)(2) + (-1)(1) \amp (1)(-1) + (-1)(1) \amp (1)(0) + (-1)(3) \amp (1)(4) + (-1)(2) \\
                        (0)(2) + (2)(1) \amp (0)(-1) + (2)(1) \amp (0)(0) + (2)(3) \amp (0)(4) + (2)(2) \\
                        (-3)(2) + (1)(1) \amp (-3)(-1) + (1)(1) \amp (-3)(0) + (1)(3) \amp (-3)(4) + (1)(2)
                        \end{bmatrix} </mrow>
                    <mrow>\amp = \begin{bmatrix}
                                1 \amp -2 \amp -3 \amp 2 \\ 2 \amp 2 \amp 6 \amp 4 \\ -5 \amp 4 \amp 3 \amp -10
                            \end{bmatrix}</mrow>
                </md>.
            </p>
            <p>
               Observe that 
                <me>
                    \bA \begin{bmatrix} 2 \\ 1 \end{bmatrix} = 2 \begin{bmatrix} 1 \\ 0 \\ -3
                        \end{bmatrix} + 1 \begin{bmatrix} -1 \\ 2 \\ 1 \end{bmatrix}
                        = \begin{bmatrix} 1 \\ 2 \\ -5 \end{bmatrix}
                </me>, 
                and so on. 
            </p>
         </example>
    </subsection>

    <subsection>
        <title>Properties</title>
        <p>
            First, the bad news. In the previous subsection we computed the product <m>\bA\bB</m>, where the matrices are <m>3\times 2</m> and <m>2\times 4</m>, to get a result that is <m>3\times 4</m>. However, the product <m>\bB\bA</m> isn't even defined, because <m>4\neq 3</m>. Moreover, even when both <m>\bA\bB</m> and <m>\bB\bA</m> <em>are</em> defined, we cannot automatically expect them to be equal. 
        </p>
        <fact>
            <p>Matrix multiplication is not commutative. That is, the order of terms in a product affects the result.</p>
        </fact>
        <p>
            In a product of matrices (and vectors), position matters. You cannot change the ordering without an explicit justification! You may have to unlearn some old habits.
        </p>
        <p>
            Fortunately, other familiar properties of multiplication do come along for the ride:
            <ol>
                <li><m>(\bA\bB)\bC=\bA(\bB\bC)</m></li>
                <li><m>\bA(\bB+\bC) = \bA\bB + \bA\bC</m></li>
                <li><m>(\bA+\bB)\bC = \bA\bC + \bB\bC</m></li>
                <!--li><m>\alpha(\bB+\bC) = \alpha\bB + \alpha\bC</m></li-->
            </ol>  
        </p>
    </subsection>
</section>

<section xml:id="ma-inverses">
    <title>Matrix inverses</title>
    <introduction>
    <p>From section 4.4.</p>
    <p>
        From now on, we're going to focus on square, <m>n\times n</m> matrices. 
    </p>
    <p>
        It's pretty immediate that for any square matrix <m>\bA</m>, <m>\bA\boldsymbol{0}=\boldsymbol{0}\bA=\boldsymbol{0}</m> for the all-zero square matrix <m>\boldsymbol{0}</m>. So we have a multiplicative zero matrix. But the situation is a bit less obvious when it comes to a multiplicative unit element. Some reflection on matrix multiplication properties reveals that if we define an <term>identity matrix</term> as the <m>n\times n</m> matrix
        <me>
            \bI = \begin{bmatrix} 
            1 \amp 0 \amp 0 \amp \cdots \amp 0 \\
            0 \amp 1 \amp 0 \amp \cdots \amp 0 \\
             \vdots \amp  \amp \ddots \amp  \amp \vdots\\
             0 \amp \cdots \amp 0 \amp 1 \amp 0 \\
             0 \amp \cdots \amp 0 \amp 0 \amp 1
             \end{bmatrix}
        </me>, 
        then <m>\bI\bA=\bA</m> and <m>\bA\bI=\bA</m>. Actually, we don't need to restrict <m>\bA</m> to be square here, and it's also true that <m>\bI\bx=\bx</m> for all <m>n\times 1</m> vectors <m>\bx</m>.
    </p> 
    </introduction>
    <subsection>
    <title>Inverse matrix</title>
    <p>
        With the multiplicative identity in hand, the next order of business is to look for multiplicative inverses. To wit, if we have a square matrix <m>\bA</m>, can we find another square matrix <m>\bC</m> such that <m> \bA\bC = \bC\bA = \bI </m>? 
        If such a matrix exists, we call it the <term>inverse</term> of <m>\bA</m> and write <m>\bA^{-1}=\bC</m>.  
    </p>
    <p>
        (Interestingly, while the lack of commutativity means we formally need to specify two products to define an inverse, it turns out that one can prove [not so easily!] that one of these products automatically implies the other; that is, a "left-inverse" is also a "right-inverse", and vice versa.)
    </p>
    <p>
        Because the zero matrix gives zero for every multiplicative partner, it cannot have an inverse. That's completely in line with what we know about the scalar zero. However, unlike scalars, there are nonzero matrices that do not have any inverse. The simplest is 
        <me>
            \bA = \begin{bmatrix}  0 \amp 1 \\ 0 \amp 0 \end{bmatrix}
        </me>. 
        (Proof: The second column of <m>\bC\bA</m> is zero for any square <m>\bC</m>.) We call such a matrix <term>singular</term>. A matrix in the opposite situation (having an inverse)is said to be <term>invertible</term>, or <term>nonsingular</term>. 
    </p>
    <p>
        This notion of singularity is the same one we encountered with 2-by-2 linear systems. For, if <m>\bA</m> is not singular, then it has an inverse (by definition), and the linear system <m>\bA\bx=\bb</m> is equivalent to 
        <me>
            \bA^{-1} \bb = \bA^{-1} (\bA \bx) = \bigl(  \bA^{-1} \bA \bigr) \bx = \bI \bx = \bx
        </me>. 
         That is, if <m>\bA</m> is nonsingular, the linear system has the unique solution <m>\bA^{-1} \bb</m>.  That the converse is also true (unique solution implies nonsingularity) is part of what's usually called the Fundamental Theorem of Linear Algebra, which is a little beyond our scope here.
    </p>
    <p>
        It's one thing to say "the inverse exists," and quite another to say "here it is." Finding the inverse of a given matrix is in principle a matter of solving <m>n</m> linear systems. That's because if we let <m>\mathbf{e}_j</m> be the <m>j</m>th column of <m>\bI</m>, then according to the columnwise interpretation of <m>\bA\bA^{-1}</m>, the solution of <m>\bA \bx = \mathbf{e}_j</m> is the <m>j</m>th column of <m>\bA^{-1}</m>. There is a well-known algorithm for solving all those systems simultaneously. However, it's beside the point. If our goal is to solve a linear system <m>\bA\bx=\bb</m>, then there is little reason to solve <m>n</m> other systems first to get the inverse! The inverse tends to be much more important theoretically than for practical computation. 
    </p>
    </subsection>
    <subsection>
    <title><m>2\times 2</m> inverses and systems</title>
    <p>
        One of the few circumstances in which it's easy to get our hands on a matrix inverse is in the <m>2\times 2</m> case:
        <me>
            \begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix}^{-1} = 
            \frac{1}{ad-bc} \begin{bmatrix} d \amp -b \\ -c \amp a \end{bmatrix}
        </me>.  
        It's even simpler to remember this if you note that <m>ad-bc</m> is the determinant of the matrix. 
    </p>
    <p>
        Even though a 2x2 inverse is easy, it's still not the most convenient way to solve a linear system. There is an even faster equivalent shortcut known as <term>Cramer's Rule</term>:
        <md>
            <mrow>x_1 \amp = \frac{ \twodet{b_1}{A_{12}}{b_2}{A_{22}} }{ \det(\bA) }</mrow>
            <mrow>x_2 \amp = \frac{ \twodet{A_{11}}{b_1}{A_{21}}{b_2} }{ \det(\bA) }</mrow>   
        </md>.
    </p>
    <example>
        <p>
            For 
        <md>
            <mrow>-x + 3y \amp = 1 </mrow>
            <mrow>3x + y \amp = 7</mrow>
        </md>, 
        Cramer's Rule says
        <md>
            <mrow>x \amp = \frac{ \twodet{1}{3}{7}{1} }{ \det(\bA) }=  \frac{ \twodet{1}{3}{7}{1} }{ \twodet{-1}{3}{3}{1} } = \frac{-20}{-10} </mrow>
           <mrow>y \amp = \frac{ \twodet{-1}{1}{3}{7} }{ \det(\bA) } = \frac{ \twodet{-1}{1}{3}{7} }{ \twodet{-1}{3}{3}{1} } = \frac{-10}{-10}</mrow>            
        </md>.
        </p>
     </example>
 
    </subsection>
    <subsection>
    <title>Determinants</title>
    <p>
        We're going to rely on a key fact (from the FTLA that I alluded to above).
    </p>
    <fact>
        <p>A square matrix <m>\bA</m> is singular if and only if its determinant is zero.</p>
    </fact>
    <p>
        Of course this result requires us to say what a determinant <em>is</em> beyond the 2-by-2 case. One way to compute a determinant is as the product of pivots in the elimination process, provided you flip the sign for each row swap. This makes the connection to singularity clear, and it's the best way for a computer to find the determinant, but most humans prefer a recursive method called <term>cofactor expansion</term>, in which
        <me>
            \det(\bA) = \sum (-1)^{i+j} A_{ij} \det( \mathbf{M}_{ij} )
        </me>,
        where the sum is taken over any row or column of <m>\bA</m>, and <m>\mathbf{M}_{ij}</m> is the matrix that results from deleting row <m>i</m> and column <m>j</m> from <m>\bA</m>. (You may have learned vector cross products as a 3-by-3 determinant using these cofactors across the top row.) 
    </p>
    </subsection>
</section>

<section xml:id="ma-nullspace">
    <title>Nullspaces</title>
    <introduction>
    <p><em>From section 5.2</em></p>
    <p>
        A little jargon we won't dwell on is a <term>vector space</term>. It's a set of vectors having a particular property: any linear combination of vectors in the set is also in the set (aka "closure" under addition and scalar multiplication). The space we will be interested in is called a <term>nullspace</term> of an <m>n\times n</m> matrix <m>\bA</m>, defined as 
        <me>
            \mathcal{N}(\bA) = \{ \text{all $n$-vectors $\bv$ such that $\bA\bv=\bzero$} \}
        </me>.
        To see that this has the required closure property, suppose <m>\bv_1,\dots,\bv_k</m> are in <m>\mathcal{N}(\bA)</m>. Then 
        <me>
            \bA( c_1 \bv_1 + \dots + c_k\bv_k ) = c_1 \bA\bv_1 + \cdots + c_k \bA \bv_k = \bzero
        </me>,
        so the linear combination is in <m>\mathcal{N}(\bA)</m>, too. 
    </p>
    </introduction>

    <subsection>
    <title>Nullspaces from elimination</title>
    <p>
        We can use regular elimination to find a nullspace. Since <m>\bb=\bzero</m> by definition, all the row operations leave it unchanged, and we can ignore it until the back substitution stage.
    </p>
    <example>
        <p>
        <me>
            \begin{bmatrix} 
            1 \amp 1 \amp -1 \\
            2 \amp 2 \amp 1 \\
            -1 \amp -1 \amp 4   
            \end{bmatrix} \mapsto 
            \begin{bmatrix} 
            1 \amp 1 \amp -1 \\
            0 \amp 0 \amp 3 \\
            0 \amp 0 \amp 3   
            \end{bmatrix} \mapsto 
            \begin{bmatrix} 
            1 \amp 1 \amp -1 \\
            0 \amp 0 \amp 3 \\
            0 \amp 0 \amp 0   
            \end{bmatrix}
        </me>. 
        The last row just says <m>0=0</m>, so we ignore it. The second row implies <m>x_3=0</m>, and then the first row implies <m>x_1+x_2=0</m>. We can regard <m>x_2</m> as a free variable; say <m>x_2=\alpha</m>. Then <m>x_1=-\alpha</m>, and the solution takes the form 
        <me>
            \bx = \begin{bmatrix} -\alpha \\ \alpha \\ 0 \end{bmatrix} = \alpha \begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}
        </me>.
        Thus the nullspace consists of all multiples of the constant vector <m>\begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}</m>. This representation isn't unique; any nonzero member of the space would serve as well. 
        </p>
    </example>   
    <example>
        <p>
            Suppose elimination ends with the matrix
            <me>
            \begin{bmatrix} 
            1 \amp 1 \amp -1 \\
            0 \amp 1 \amp 1 \\
            0 \amp 0 \amp 4   
            \end{bmatrix}
            </me>. 
            Now back substitution implies <m>x_3=0</m>, then <m>x_2=0</m>, then <m>x_1=0</m>. So <m>\mathcal{N}(\bA) = \{\bzero\}</m>.
        </p>
    </example> 
     <example>
        <p>
        <me>
            \begin{bmatrix} 
            0 \amp 2 \amp -3 \\
            0 \amp 4 \amp -6 \\
            0 \amp -2 \amp 3   
            \end{bmatrix} \mapsto 
            \begin{bmatrix} 
            0 \amp 2 \amp -3 \\
            0 \amp 0 \amp 0 \\
            0 \amp 0 \amp 0   
            \end{bmatrix}
        </me>. 
        The only condition here is <m>2x_2-3x_3=0</m>. Say <m>x_3=\alpha</m> and <m>x_1=\beta</m>. Then solutions take the form 
        <me>
            \bx = \begin{bmatrix} \beta \\ 3\alpha/2 \\ \alpha \end{bmatrix} = \alpha \begin{bmatrix} 0 \\ 3/2 \\ 1 \end{bmatrix} + \beta \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}
        </me>. 
        The nullspace consists of all linear combinations of the two constant vectors shown above.  Because two such vectors are needed, we say the nullspace is "two-dimensional."
        </p>
    </example>   
    </subsection>
    <subsection>
    <title>Row echelon form</title>
    <p>
        The target of row elimination in the case of a nullspace can be described by three rules:  
        <ol>
            <li>The leftmost nonzero in any row is a 1 (called a <term>leading one</term>).</li>
            <li>The leading one in any row is to the right of any leading ones in higher rows.</li> 
            <li>Any all-zero rows are at the bottom of the matrix.</li>
        </ol>
        We have not been following rule 1 up to now, but it's easy to satisfy; we just divide each row by its leading nonzero value. Such a matrix is said to be in <term>row-echelon form</term> (REF). 
    </p>
    <example>
        <p>
            REFs for the examples in the last subsection are 
            <me>
            \begin{bmatrix} 
            1 \amp 1 \amp -1 \\
            0 \amp 0 \amp 1 \\
            0 \amp 0 \amp 0   
            \end{bmatrix}, 
            \quad
            \begin{bmatrix} 
            1 \amp 1 \amp -1 \\
            0 \amp 1 \amp 1 \\
            0 \amp 0 \amp 1   
            \end{bmatrix},
            \quad 
            \begin{bmatrix} 
            0 \amp 1 \amp -3/2 \\
            0 \amp 0 \amp 0 \\
            0 \amp 0 \amp 0   
            \end{bmatrix}
            </me>. 
        </p>
    </example> 
    <p>
        Once the matrix has been transformed to REF, you can divide the components of the solution <m>\bx</m> into two groups: the <term>pivot variables</term> are those whose columns have a leading one somewhere, and the <term>free variables</term> are the rest. The number of free variables is the dimension of the nullspace (also called the nullity of <m>\bA</m>). You can then find a representation of the nullspace by setting each of the free variables to one in turn (making the others zero) and solving for the resulting pivot variables. The nullspace is the set of linear combinations of those vectors. 
    </p>
        <example>
        <p>
            Given the REF 
            <me>
            \begin{bmatrix} 
            1 \amp 1 \amp -1 \\
            0 \amp 0 \amp 1 \\
            0 \amp 0 \amp 0   
            \end{bmatrix}
            </me>,
            we see that <m>x_2</m> is the only free variable. When we set <m>x_2=1</m> during back substitution, we find that <m>x_3=0</m> and <m>x_1=-1</m>. Thus <m>\mathcal{N}(\bA)</m> consists of all linear combinations of <m>\begin{bmatrix} -1 \\ 1 \\ 0 \end{bmatrix}</m>, i.e., all multiples of that vector. 
            With the REF 
            <me>
            \begin{bmatrix} 
            1 \amp 1 \amp -1 \\
            0 \amp 1 \amp 1 \\
            0 \amp 0 \amp 1   
            \end{bmatrix}
            </me>,
            there are no free variables. The nullspace has dimension zero; i.e., it is the single point <m>\bzero</m>. Finally, with the REF
            <me>
            \begin{bmatrix} 
            0 \amp 1 \amp -3/2 \\
            0 \amp 0 \amp 0 \\
            0 \amp 0 \amp 0   
            \end{bmatrix}
            </me>,
            the free variables are <m>x_1</m>  and <m>x_3</m>. When <m>x_1=1</m> and <m>x_3=0</m>, we get <m>\begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}</m>, and when vice versa we get <m>\begin{bmatrix} 0 \\ 3/2 \\ 1 \end{bmatrix}</m>. 
        </p>
    </example> 
    </subsection>
</section>

<section xml:id="ma-eigenvalues">
    <title>Eigenvalues</title>
<introduction>
    <p><em>From section 6.1.</em></p>
    <p>
        Given a square matrix <m>\bA</m>, a solution of
        <me>
            \bA \bv = \lambda \bv
        </me>
        for a scalar <m>\lambda</m> and a nonzero vector <m> \bv</m>  means that <m>\lambda</m> is called an <term>eigenvalue</term> and <m>\bv</m> is an associated <term>eigenvector</term>. The first thing to note is that <m>c\bv</m> is then an eigenvector for <m>\lambda</m> for any nonzero <m>c</m>; that is, eigenvectors are determined only up to (at most) a multiplicative scalar. 
    </p>
    <p>
        Before going on to more generalities about eigenvalues, consider the implications for a linear ODE system <m>\by'=\bA\by</m>. We can insert the trial solution <m>\by(t)=g(t) \bv</m> for unknown time dependence and get 
        <me>
            g'(t) \bv  =  g(t) \bA \bv = g(t) \lambda \bv
        </me>. 
        This can be satisfied by requiring <m>g'=\lambda g</m>. So we conclude that <m>\by(t)=e^{\lambda t}\bv</m> solves the original linear system. Geometrically, the eigenvector condition means "<m>\bA\bv</m> is parallel to <m>\bv</m>." In a linear ODE, this makes <m>\by'</m> parallel to <m>\by</m>, so the direction never changes and the problem is essentially one-dimensional. 
    </p>

</introduction>

<subsection>
<title>Characteristic polynomial</title>

<p>
    The defining eigenpair condition is equivalent to <m> \bA \bv = \lambda \mathbf{I} \bv, </m> or 
    <me>
        (\bA-\lambda \mathbf{I})  \bv = \boldsymbol{0}
    </me>. 
     Now suppose <m>\lambda</m> is a number such that <m>(\bA-\lambda \mathbf{I})</m> is a nonsingular matrix. It follows that the linear system above has a unique solution, namely <m>\bv = \boldsymbol{0}</m>. So, we can find no eigenvector at that value of <m>\lambda</m>. In other words,
</p>
<theorem>
    <p><m>\lambda</m> is an eigenvalue if and only if <m>\bA-\lambda \mathbf{I}</m> is a singular matrix. All nonzero members of the nullspace of <m>\bA-\lambda \mathbf{I}</m> are eigenvectors associated with <m>\lambda</m>.</p>
</theorem>
<p>
    This connection gives the most common way to compute eigenvalues by hand. Recall that a matrix is singular if and only if its determinant is zero. Thus an alternative algebraic condition for an eigenvalue is <m>\det(\bA-\lambda \mathbf{I})=0</m>. The eigenvectors associated with <m>\lambda</m> are the nonzero members of the nullspace of <m>\bA-\lambda\bI</m>. For an eigenvalue this space is at least one-dimensional. 
</p>
<p>
    In the <m>n\times n</m> case, <m>\det(\bA-\lambda\bI)</m> is a polynomial of degree <m>n</m> called the <term>characteristic polynomial</term> of <m>\bA</m>. Thus there are <m>n</m> eigenvalues, if we count multiplicities, i.e.,
    <me>
        \det(\bA-\lambda\bI) = (\lambda-\lambda_1)^{m_1} (\lambda-\lambda_2)^{m_2} \cdots 
        (\lambda-\lambda_k)^{m_k}  
    </me>,
    where <m>k\le n</m> and the positive integers <m>m_i</m>, called the <term>algebraic multiplicities</term> of the eigenvalues, sum to <m>n</m>. 
</p>
<p>
    Thanks to cofactor expansion of the determinant, there is one type of matrix for which finding the eigenvalues is trivial.
</p>
<theorem>
    <p>
        The eigenvalues of a triangular matrix are the diagonal entries of the matrix.
    </p>
</theorem>
<p>
    Diagonal matrices are included in the theorem. 
</p>
</subsection>
<subsection>
<title>Eigensystems in the <m>2\times 2</m> case</title>
<p>
    The characteristic polynomial in the <m>2\times 2</m> situation will return us to familiar territory.
    <md>
        <mrow>\det( \bA -\lambda \mathbf{I} ) \amp = \det\left( \begin{bmatrix} a \amp b \\ c \amp d \end{bmatrix} - \lambda \begin{bmatrix} 1 \amp 0 \\ 0 \amp 1 \end{bmatrix} \right)</mrow>
        <mrow>\amp = \begin{vmatrix} a-\lambda \amp b \\ c \amp d-\lambda \end{vmatrix} = (a-\lambda)(d-\lambda)-bc=\lambda^2 - T \lambda +D</mrow>
    </md>,
    where <m>T</m> and <m>D</m> are the trace and determinant of <m>\bA</m>. This is what I stated when we were analyzing the stability of a 2-by-2 linear ODE system. 
</p>
<p>
    Since an eigenvalue makes <m>\bA-\lambda\bI</m> singular, one step of elimination on that matrix will make the second row all zeros. Thus we only need to pay attention to the condition imposed by the first row, <m>(a-\lambda)v_1 + b v_2 =0</m>. One solution of this equation is <m>\bv=\twovec{b}{\lambda-a}</m>, which will be a representation of <m>\mathcal{N}(\bA)</m>. The only exception is if <m>\lambda-a=b=0</m>, in which case we need to swap rows before the elimination step. 
</p>
<example>
    <p>
        Let's find the eigenvalues and eigenvectors of
        <me>
            \begin{bmatrix} 1 \amp 1 \\ 4 \amp 1 \end{bmatrix}
        </me>. 
        The associated polynomial is <m>\lambda^2-2\lambda-3</m>, which has roots  <m>\lambda_1=3</m> and <m>\lambda_2=-1</m>. These are the eigenvalues. 
    </p>
    <p>
        Next, the first row of the linear system <m>( \bA -3 \mathbf{I}) \bv = \boldsymbol{0} </m> is <m>-2v_1 + 1v_2=0</m>, which has nonzero solution <m>\bv_1=\twovec{1}{2}</m>. This eigenvector represents the entire eigenspace for <m>\lambda_1</m>. Similarly, for <m>\lambda_2=-1</m> the first row of the null system is <m>2v_1+v_2=0</m>, so <m>\bv_2 = \twovec{1}{-2}</m> represents that eigenspace.
    </p>
</example>
<p>
    Quick note: We often speak of "finding the eigenvalues and eigenvectors" of a matrix. The eigenvalues are unique, but eigenvectors are not. What's really meant is to find minimal representations of the eigenspaces, which are uniquely determined, but that's just considered understood.
</p>
<example>
    <p>
        Consider <m>\bA=\twomat{4}{0}{1}{4}</m>. This matrix is (lower) triangular, so we immediately know <m>\lambda_1=\lambda_2=4</m>. Now, 
        <me>
            \bA - 4\bI = \twomat{0}{0}{1}{0}
        </me>. 
        If we swap the rows, the matrix is in RE form. Hence <m>v_2</m> is free and <m>v_1=0</m>. The eigenspace is one-dimensional, fully represented by <m>\twovec{0}{1}</m>. 
    </p>
</example>

</subsection>
<subsection>
<title>Complex eigenvalues</title>
<p>
    The connection to polynomials means that if <m>\bA</m> is a real matrix, it might have eigenvalues that occur in complex conjugate pairs. This makes the eigenvectors have the same structure. 
</p>
<example>
    <p>
        We find the eigenvalues and eigenvectors of
        <me>
            \begin{bmatrix} 1 \amp -1 \\ 5 \amp -3 \end{bmatrix}
        </me>.  
        The characteristic polynomial is <m>\lambda^2 +2 \lambda +2</m>, with roots  <m>\lambda_{1,2} = -1 \pm 1i.</m>
    </p>
    <p>
        To find an eigenvector for <m>\lambda_1</m>, we use the first row of <m> \bA - \lambda_1 \mathbf{I} </m> to conclude <m>(2-i)v_1 -v_2 = 0</m>. Thus any nonzero multiple of <m>\bv_1=\twovec{1}{2-i}</m> will do.
    </p>
    <p>
        We get a benefit here from the complex eigenvalues: the conjugate of an eigenvector will be an eigenvector for the conjugated eigenvalue. So we have <m>\bv_2=\twovec{1}{2+i}</m> to go with <m>\lambda_2</m>. 
    </p>
</example>
</subsection>
<subsection>
<title>Some tips</title>
<p>
    Here are a couple of things to know: the eigenvalues of <m>\bA+\bB</m> are <em>not</em>  the eigenvalues of <m>\bA</m> plus the eigenvalues of <m>\bB</m>, and the eigenvalues of <m>\bA\bB</m> are <em>not</em>  the eigenvalues of <m>\bA</m> times the eigenvalues of <m>\bB</m>. The main exception to this latter observation is for powers of a matrix,
    <me>
        \bA \bv = \lambda \bv \quad \Rightarrow \quad \bA^k \bv = \lambda^k \bv
    </me>,
    for any positive integer power <m>k</m>.  Actually, this holds for inverses too,
    <me>
        \bA \bv = \lambda \bv \quad \Rightarrow \quad \bA^{-1} \bv = \lambda^{-1} \bv
    </me>.  
    (Note that if zero is an eigenvalue, then <m>\bA-0\bI</m> is singular, so <m>\bA</m> is singular.) 
</p>
<p>
    Two other handy facts hold for matrices of all sizes.
</p>
<fact>
    <p>The sum of the eigenvalues of <m>\bA</m> is the <term>trace</term> of <m>\bA</m> (sum of the diagonal entries).</p>
</fact>
<fact>
    <p>The product of the eigenvalues is the determinant.</p>
</fact>

</subsection>
</section>

<section xml:id="ma-diagonalization">
<title>Diagonalization</title>
<introduction>
    <p><em>From section 6.2.</em></p>
    <p>
        Say <m>\bA</m> is <m>n\times n</m>, and that we write 
        <me>
            \bA \bv_1 = \lambda_1 \bv_1, \;
            \bA \bv_2 = \lambda_2 \bv_2,\; \dots, \;
            \bA \bv_n = \lambda_n \bv_n
        </me>,
        where <m>\lambda_1,\lambda_2,\dots,\lambda_n</m> are its eigenvalues. Through some linear algebra magic, we can express this collection of equations more succinctly: 
        <md>
            <mrow>
                 \begin{bmatrix} 
                    \bA \bv_1 \amp \bA \bv_2 \amp \cdots \amp \bA \bv_n 
                \end{bmatrix} 
                \amp = 
                \begin{bmatrix} 
                    \lambda_1\bv_1 \amp \lambda_2\bv_2 \amp \cdots \amp \lambda_n\bv_n 
                \end{bmatrix}
            </mrow>
            <mrow>\bA \begin{bmatrix} \bv_1 \amp \bv_2 \amp \cdots \amp \bv_n \end{bmatrix}            \amp = 
                \begin{bmatrix} \bv_1 \amp \bv_2 \amp \cdots \amp \bv_n \end{bmatrix} 
                \begin{bmatrix} \lambda_1 \amp 0 \amp \cdots \amp 0 \\
                             0 \amp \lambda_2 \amp \cdots \amp 0 \\
                             \amp \amp \ddots \amp \\
                             0 \amp \cdots \amp 0 \amp \lambda_n
                \end{bmatrix}</mrow>
            <mrow>\bA \bV \amp = \bV \bD </mrow>           
        </md>, 
        where <m>\bD</m> is a <term>diagonal</term> matrix. 
    </p>
    <p>
        If <m>\bV</m> is nonsingular, we therefore can write 
        <me>
            \bA = \bV \bD \bV^{-1}
        </me>, 
        which is called a <term>diagonalization</term> of <m>\bA</m>. A diagonalization is not unique; after all, we can take the eigenvalues in any order, and the eigenvectors themselves are not unique. 
    </p>
    <example>
        <title>A 2-by-2 diagonalization</title>
        <p>
            Let's find a diagonalization of 
            <me>
                \mathbf{A} =  \begin{bmatrix} 1 \amp 1 \\ 4 \amp 1 \end{bmatrix}
            </me>. 
            We found the eigenvalues and eigenvectors of this guy in the previous section. We got <m>\lambda_1=3</m> with <m>\bv_1 = \twovec{1}{2}</m>, and <m>\lambda_2=-1</m> with <m>\bv_2 =\twovec{1}{-2}</m>. So we paint by numbers to get
            <me>
                \mathbf{D} = \begin{bmatrix} 3 \amp 0 \\ 0 \amp -1 \end{bmatrix}, \qquad \bV = \begin{bmatrix} 1 \amp 1 \\ 2 \amp -2 \end{bmatrix}
            </me>. 
            We "just" need to invert <m>\bV</m>; fortunately, we're only at <m>2\times 2</m>, so this is a snap.
            <me> 
                \bV^{-1} = \frac{1}{-4} \begin{bmatrix} -2 \amp -1 \\ -2 \amp 1 \end{bmatrix} = \begin{bmatrix} 1/2 \amp 1/4 \\ 1/2 \amp -1/4 \end{bmatrix}
            </me>.  
            Hence
            <me> 
                \mathbf{A} = \bV \mathbf{D} \bV^{-1} = 
                \begin{bmatrix} 1 \amp 1 \\ 2 \amp -2 \end{bmatrix} 
                \begin{bmatrix} 3 \amp 0 \\ 0 \amp -1 \end{bmatrix} 
                \begin{bmatrix} 1/2 \amp 1/4 \\ 1/2 \amp -1/4 \end{bmatrix}
            </me>. 
        </p>
    </example>
    <p>
        Every eigenvalue has an eigenspace of dimension at least one. If the <m>n</m> eigenvalues are distinct, the dimensions add up to <m>n</m>, and it can be proved that the matrix <m>\bV</m> is invertible.
    </p>
    <theorem>
        <p>
            If the eigenvalues of a matrix are all distinct, the matrix is diagonalizable.
        </p> 
    </theorem>
    <p>
        The "only if" version of this theorem is <em>not</em> true. For example, the identity matrix is trivially diagonalizable with <m>\bV=\bI</m>, but the eigenvalues are both equal to one. 
    </p>
</introduction>
<subsection>
<title>Diagonalization of a linear ODE</title>
<p>
    Suppose <m>\mathbf{y}'=\mathbf{A} \mathbf{y}</m>. Given a diagonalization of <m>\bA</m>, we compute
    <md>
        <mrow> \mathbf{y}'\amp =\mathbf{V} \mathbf{D} \mathbf{V}^{-1} \mathbf{y}</mrow>
        <mrow>\mathbf{V}^{-1} \mathbf{y}' \amp=\mathbf{D} \mathbf{V}^{-1} \mathbf{y}</mrow>
        <mrow>(\mathbf{V}^{-1} \mathbf{y})' \amp=\mathbf{D} (\mathbf{V}^{-1} \mathbf{y})</mrow>
    </md>. 
    This suggests a change of variable, <m>\mathbf{u} = \mathbf{V}^{-1} \mathbf{y}.</m> Then
    <me> 
        \mathbf{u}' = \mathbf{D} \mathbf{u} 
        = \begin{bmatrix} \lambda_1 u_1 \\ \lambda_2 u_2 \\ \vdots \\ \lambda_n u_n \end{bmatrix}
    </me>. 
    In terms of the new variable, the ODE system is <m>u_1'=\lambda_1 u_1,\, \dots,\, u_n'=\lambda_n u_n</m>, which are completely decoupled–that is, they evolve independently. 
</p>
<p>
    How are we to interpret <m>\mathbf{u} = \mathbf{V}^{-1} \mathbf{y}</m>? Suppose we write it as <m>\bV \bu = \by</m>, which has the form of a linear system. One of our interpretations of this system is to find an expression of the vector <m>\by</m> in terms of coordinates based on combining the columns of <m>\bV</m>. Thus, if we change an ODE to eigenvector coordinates, the resulting ODE for the coordinate values is diagonal. 
</p>
</subsection>

<subsection>
<title>Repeated eigenvalues</title>
<p>
    As we have seen time and again, repeated roots complicate our conclusions. The same is true for eigenvalues.
</p>
<p>
    As stated above, when the eigenvalues are all distinct, a diagonalization is always possible. When there are repeated eigenvalues, though, that may or may not be the case. We saw above that an identity matrix is diagonalizable. However, the matrix <m>\bA=\twomat{4}{0}{1}{4}</m> shown in an example above is impossible to diagonalize. The only candidates for the columns of <m>\bV</m> are eigenvectors, and those are all nonzero multiples of <m>\twovec{0}{1}</m>. Hence 
     <me>
        \bV = \begin{bmatrix} 0 \amp 0 \\ \alpha \amp \beta \end{bmatrix}
    </me>, 
    which is necessarily singular (determinant is zero). Therefore no diagonalization is possible, and we say <m>\bA</m> is <term>defective</term>. Another way of saying this is that the dimensions of the eigenspaces add up to less than <m>n</m>, or that the matrix does not have a "complete independent set of eigenvectors."
</p>
<p>
    A full description of defectiveness is beyond our scope. For the <m>2\times 2</m> case, we've essentially seen all the possibilities already: a matrix with repeated eigenvalues is either a multiple of the identity, in which case it has a diagonalization with <m>\bV=\bI</m>, or it's defective. 
</p>
</subsection>
</section>

<section xml:id="ma-linearODE">
<title>Linear ODE systems</title>
<introduction>
    <p><em>From section 6.3.</em></p>
    <p>
        Now we are ready to take on <m>\by'=\bA\by</m>, at least in the diagonalizable and defective <m>2\times 2</m> cases. 
    </p>
    <p>
        In the previous section I argued that if <m>\bA=\bV\bD\bV^{-1}</m>, then we can define <m>\bu=\bV^{-1}\by</m> and get the diagonal system <m>\bu'=\bD\bu</m>. This diagonal system is trivial to solve,
        <me>
            \bu(t) = \begin{bmatrix} 
            e^{\lambda_1 t} u_1(0) \\ 
            e^{\lambda_2 t} u_2(0) \\ 
            \vdots \\
            e^{\lambda_n t} u_n(0)  
            \end{bmatrix} 
        </me>. 
        In terms of the original variable <m>\by=\bV\bu</m>, this solution is 
        <me>
            \by(t) = \bV \begin{bmatrix} 
            e^{\lambda_1 t} c_1 \\ 
            e^{\lambda_2 t} c_2 \\ 
            \vdots \\
            e^{\lambda_n t} c_n  
            \end{bmatrix}
            = c_1 e^{\lambda_1 t} \bv_1 + \cdots + c_n e^{\lambda_n t} \bv_n
        </me>, 
        where we defined <m>c_i=u_i(0)</m> for all <m>i</m>. This is one form of the general solution.
     </p>
<example>
    <p>
        In an earlier section we found the eigenstuff for 
        <me>
            \bA = \begin{bmatrix} 1 \amp 1 \\ 4 \amp 1 \end{bmatrix}
        </me> 
        as <m>\lambda_1=3</m>, <m>\bv_1 = \twovec{1}{2}</m>, and <m>\lambda_2=-1</m>, <m>\bv_2=\twovec{1}{-2}</m>. This means the general solution of <m>\by'=\bA\by</m> is 
        <me>
            \by(t) = c_1 e^{3t} \twovec{1}{2} + c_2 e^{-t} \twovec{1}{-2} 
            = \twovec{ c_1 e^{3t} + c_2 e^{-t}}{2c_1 e^{3t} - 2 c_2e^{-t}}
        </me>.
    </p>
</example>
</introduction>

<subsection>
<title>Complex eigenvalues</title>
<p>
    Nothing startling happens for complex eigenvalues. You just need to use complex arithmetic.
</p>
<example>
    <p>
        Let's look at the solution of 
        <me>
            \mathbf{y}' = \begin{bmatrix} 1 \amp -1 \\ 5 \amp -3 \end{bmatrix} \mathbf{y}
        </me>. 
        The eigenvalues are <m>1\pm i</m>, with eigenvectors <m>\twovec{1}{2\mp i}</m>. So one form of the general solution is 
        <me>
            c_1 e^{t} e^{it} \twovec{1}{2-i} + c_2 e^{t} e^{-it} \twovec{1}{2+i}
        </me>. 
        With some work, we could convert this to a purely real form. Or, suppose we also have the initial condition <m>\by(0)=\twovec{1}{-2}</m>. This gives 
        <md> 
            <mrow>c_1 \twovec{1}{2-i} + c_2 \twovec{1}{2+i} \amp = \twovec{1}{-2}</mrow>
            <mrow>\twomat{1}{1}{2-i}{2+i} \twovec{c_1}{c_2} \amp = \twovec{1}{-2}</mrow>
        </md>. 
        Using Cramer's rule, we get 
        <md>
            <mrow>c_1 \amp = \frac{ \twodet{1}{1}{-2}{2+i} }{ \twodet{1}{1}{2-i}{2+i} } = \frac{4+i}{2i}=\frac{-1-4i}{2}</mrow>
            <mrow>c_2 \amp = \frac{ \twodet{1}{1}{2-i}{-2} }{ \twodet{1}{1}{2-i}{2+i} } = \frac{-4+i}{2i}=\frac{-1+4i}{2}</mrow>            
        </md>.
    </p>
</example>
</subsection>

<subsection>
<title>Defective <m>2\times 2</m></title>
<p>
    In the case <m>n=2</m>, <m>\lambda_1=\lambda_2=\lambda</m>, and <m>\bA</m> is not diagonal, there is no diagonalization possible. Every eigenvalue does have at least one eigenvector, so there is a <m>\bv</m> with <m>\bA\bv = \lambda \bv</m>, and <m>c_1 e^{\lambda t}\bv</m> is still a null solution. 
</p> 
<p>
    Based on our experiences to date, we try an ODE solution of the form 
    <m>
        te^{\lambda t} \mathbf{w}
    </m>
    for an unknown <m>\bw</m>. We plug this into the ODE and get 
    <me>
        (\lambda t e^{\lambda t} + e^{\lambda t}) \mathbf{w} = t e^{\lambda t} \mathbf{A} \mathbf{w}
    </me>. 
    We don't have enough terms to balance both the <m>t^0</m> and <m>t^1</m> groups. A better guess is 
    <me> 
        te^{\lambda t} \mathbf{v} + e^{\lambda t} \mathbf{w}
    </me>. Then
    <me>
        (\lambda t e^{\lambda t} + e^{\lambda t}) \mathbf{v} + \lambda e^{\lambda t} \mathbf{w} = t e^{\lambda t} \mathbf{A} \mathbf{v} + e^{\lambda t} \mathbf{A} \mathbf{w}
    </me>.
    Grouping together like terms, we have
    <me>
         t( \mathbf{A} \mathbf{v} - \lambda \mathbf{v} ) + (  \mathbf{A} \mathbf{w} - \lambda \mathbf{w} ) = \mathbf{v}.
    </me>
    The term multiplying <m>t</m> is zero, by the eigenvector property. So we need only requiire
    <me>
        (  \mathbf{A} - \lambda \mathbf{I} )  \mathbf{w} = \mathbf{v} 
    </me>,
    which can be interpreted as a definition of a <term>generalized eigenvector</term> <m>\bw</m>. Even though the matrix here is singular, a solution for <m>\bw</m> is always possible. 
</p>
<example>
    <p>
        Let us find the general solution of
        <me>
            \mathbf{y}' = \begin{bmatrix} -1 \amp 1 \\ 0 \amp -1 \end{bmatrix} \mathbf{y}.
        </me>
        You can easily check that <m>\lambda=-1</m> is a double eigenvalue, and that <m>[1,0]</m> is an eigenvector. The system for the generalized eigenvector is 
        <me>
            \begin{bmatrix} 0 \amp 1 \\ 0 \amp 0 \end{bmatrix} \mathbf{w} = \begin{bmatrix} 1 \\ 0 \end{bmatrix}
        </me>. 
        The first row of this system tells us <m>w_2=1</m>. The second row makes no restrictions. We may as well let <m>w_1=0</m>, since it's not forbidden.
        </p>
        <p>
        The general solution is therefore
        <me>
            c_1 e^{-t} \mathbf{v} + c_2 e^{-t}(t\mathbf{v}+\mathbf{w})
            = c_1 e^{-t} \begin{bmatrix} 1 \\ 0 \end{bmatrix} + c_2 e^{-t} \begin{bmatrix} t \\ 1 \end{bmatrix}
        </me>.
    </p>
</example>
</subsection>
<subsection>
<title>Second-order equations</title>
<p>
    Time to tie up one small loose end. What does the eigenvalue approach tell us about our beloved second-order equation, <m>ay''+by'+cy=0</m>? Remember, we define <m>x_1=y</m>, <m>x_2=y'</m> to get the companion matrix <m>\twomat{0}{1}{-c/a}{-b/a}</m>. The characteristic polynomial of this matrix is 
    <me>
        \lambda^2 + \frac{b}{a}\lambda + \frac{c}{a}
    </me>,
    which is just <m>1/a</m> times what we called the characteristic polynomial of the original problem. So, what we called characteristic roots back then were really eigenvalues, and we wrote out the solutions using only the first component <m>x_1</m>. 
</p>
</subsection>
</section>

<section xml:id="ma-exponential">
    <title>Matrix exponential</title>
    <introduction>
    <p><em>From section 6.4.</em></p>
    <p>
        Let's take another look at <m>\by'=\bA\by</m> in the diagonalizable case with <m>\bA=\bV\bD\bV^{-1}</m>. Again we define <m>\bu=\bV^{-1}\by</m> and get the diagonal system <m>\bu'=\bD\bu</m>, with solution 
        <me>
            \bu(t) = \begin{bmatrix} 
            e^{\lambda_1 t} u_1(0) \\ 
            e^{\lambda_2 t} u_2(0) \\ 
            \vdots \\
            e^{\lambda_n t} u_n(0)  
            \end{bmatrix} 
            = \begin{bmatrix} e^{\lambda_1 t} \amp  \amp \amp  \\
                              \amp e^{\lambda_2 t} \amp  \amp  \\
                             \amp \amp \ddots \amp \\
                             \amp  \amp  \amp e^{\lambda_n t}
                \end{bmatrix} \bu(0)
        </me>. 
        Diagonal matrices are a snap to multiply:
        <me>
            \bD^k = \fourdiag{\lambda_1^k}{\lambda_2^k}{\lambda_n^k}, \qquad k=1,2,3,\ldots
        </me>. 
        From here we can make a remarkable leap to a matrix power series,
        <me> 
            \bI + \bD + \frac{1}{2!}\bD^2 + \frac{1}{3!}\bD^3 + \cdots = 
             \fourdiag{e^\lambda_1}{e^\lambda_2}{e^\lambda_n}
        </me>.
        It's irresistable to conclude that we have found the exponential of the diagonal matrix <m>\bD</m>:
        <me> 
            e^\bD = \fourdiag{e^\lambda_1}{e^\lambda_2}{e^\lambda_n}
        </me>.
        What's more, we can now write simply,
        <me>
            \bu(t) = e^{t\bD} \bu(0)
        </me>. 
    </p>
    <p>
       Returning to the original ODE variable <m>\by=\bV\bu</m> leads to
        <me> 
            \by(t) = \bV e^{t\bD} \bV^{-1} \by(0) 
        </me>.    
        Could it be, then, that
        <me>
            e^{t\bA} = \bV e^{t\bD} \bV^{-1}
        </me>? 
        Let's consult the power series:
        <md>
            <mrow>e^{t\bA} \amp = \bI + t\bA + \frac{1}{2!}t^2\bA^2 + \frac{1}{3!}t^3\bA^3 + \cdots</mrow>
            <mrow>\amp = \bV\bV^{-1} + t(\bV\bD\bV^{-1}) + \frac{1}{2!}t^2 (\bV\bD\bV^{-1})(\bV\bD\bV^{-1}) +  \frac{1}{3!}t^3 (\bV\bD\bV^{-1})(\bV\bD\bV^{-1})(\bV\bD\bV^{-1}) + \cdots </mrow>
            <mrow>\amp = \bV \bigl( \bI + t\bD + \frac{1}{2!}t^2\bD^2 + \frac{1}{3!}t^3\bD^3 + \cdots \bigr) \bV^{-1}</mrow>
        </md>, 
        and we have arrived. Therefore, 
        <me>
            \by(t) = e^{t\bA} \by(0)
        </me>
        is the solution of <m>\by'=\bA\by</m>.  
    </p>
    <example>
    <p>
        We look again at an example fron the previous section, 
        <me>
            \bA = \begin{bmatrix} 1 \amp 1 \\ 4 \amp 1 \end{bmatrix}
        </me>, 
        in which <m>\lambda_1=3</m>, <m>\bv_1 = \twovec{1}{2}</m>, and <m>\lambda_2=-1</m>, <m>\bv_2=\twovec{1}{-2}</m> are eigenpairs. We define 
        <me>
            \bV = \twomat{1}{1}{2}{-2}
        </me>
        and compute
        <me>
            \bV^{-1} = \frac{1}{4}\twomat{2}{1}{2}{-1}
        </me>. 
        Hence 
        <me> 
            \by(t) = \frac{1}{4} \twomat{1}{1}{2}{-2} \twodiag{e^{3t}}{e^{-t}} \twomat{2}{1}{2}{-1} \by(0)
        </me>. 
    </p>
    </example>
    </introduction>
    <subsection>
    <title>Properties</title>
    <p>
        Many of the things you know about scalar exponential functions hold true for matrices as well.
        <ul>
            <li><m>e^\bzero = \bI</m></li>
            <li><m>\bigl(e^{t\bA}\bigr)^{-1} = e^{-t\bA}</m></li>
            <li><m>\dd{}{t} e^{t\bA} = \bA e^{t\bA} = e^{t\bA} \bA</m></li>
        </ul> 
        However, there is one property we have to let go of, at least partly:
        <me>
            e^{\bA+\bB} \neq e^{\bA} e^{\bB} \quad \text{ unless }\quad  \bA\bB=\bB\bA
        </me>. 
        This has some far-reaching consequences that we won't get a chance to explore. 
    </p>
    </subsection>
    <subsection>
    <title>Complex and defective cases</title>
    <example>
        <p>
            It's routine to show that 
            <me>
                \bA = \twomat{0}{1}{-1}{0} = \twomat{1}{1}{i}{-i} \twodiag{i}{-i} \frac{1}{2} \twomat{1}{-i}{1}{i}
            </me>. 
            From here we find that 
            <md>
                <mrow>e^{t\bA} \amp = \twomat{1}{1}{i}{-i} \twodiag{e^{it}}{e^{-it}} \frac{1}{2} \twomat{1}{-i}{1}{i}</mrow>
                <mrow>\amp = \frac{1}{2} \twomat{e^{it}}{e^{-it}}{ie^{it}}{-ie^{-it}} \twomat{1}{-i}{1}{i} </mrow>
                <mrow>\amp = \frac{1}{2} \twomat{2\cos(t)}{-i[2i\sin(t)]}{i[2i\sin(t)]}{2\cos(t)}</mrow>
                <mrow>\amp = \twomat{\cos(t)}{\sin(t)}{-\sin(t)}{\cos(t)}</mrow>
            </md>. 
            This matrix applies a rotation of an angle <m>t</m> about the origin to all vectors in the plane. Thus, the phase portrait consists of concentric circles and the origin is a center. 
        </p>
    </example>
    <example>
        <p>
        The problem <m>\mathbf{y}' = \begin{bmatrix} -1 \amp 1 \\ 0 \amp -1 \end{bmatrix} \mathbf{y}</m> has a defective matrix, so we cannot use a diagonalization to compute the matrix exponential. Surprisingly, though, the result can be even easier to find!  
        </p>
        <p>
            We will use the fact that 
            <me>
                \exp(\bA t+\bI t) = \exp(\bA t) \exp(\bI t) = e^{t} e^{\bA t}
            </me>. 
            Again, we cannot do the first step above for <em>general</em> matrices, but it is clearly true that <m>(\bA t)(\bI t)=(\bI t)(\bA t)</m>, so it's okay here. The second step above just uses <m>e^{t\bI} = e^t \bI</m>.
        </p>
        <p>
            Now for the fun part. By the power series definition,
            <me>
                \exp(\bA t+\bI t) = \bI + t (\bA+\bI) + \frac{1}{2}t^2 (\bA+\bI)^2 + \cdots
            </me>. 
            But note that 
            <me> 
                (\bA+\bI)^2 = \twomat{0}{1}{0}{0} \twomat{0}{1}{0}{0} = \bzero
            </me>. 
            So all the higher matrix powers vanish too, and
            <me>
                \exp(\bA t+\bI t) = \bI + t (\bA+\bI)  = \twomat{1}{t}{0}{1}
            </me>. 
            Finally, then,
            <me>
              e^{t\bA} = e^{-t} e^{t\bA+t\bI} = e^{-t} \twomat{1}{t}{0}{1}
            </me>. 
            We had a double eigenvalue at <m>-1</m> and, to nobody's surprise, <m>te^{-t}</m> came out to play. 
        </p>
    </example>
    </subsection>
    <subsection>
    <title>Equation with forcing term</title>
    <p>
        Suppose we now have <m>\by'=\bA\by + \bff(t)</m> for a constant matrix <m>\bA</m>. We can solve this problem using our old friend, the integrating factor. Define <m>\mathbf{M}(t) = e^{-t \bA}</m>, and multiply through by it. Then 
        <md>
            <mrow>\dd{}{t} \bigl[ e^{-t \bA} \by(t) \bigr] \amp = e^{-t \bA} \bff(t)</mrow>
        </md>,
        and we solve to get 
        <me>
            \by(t) = e^{t \bA}\by(0) + \int_0^t e^{(t-s) \bA} \bff(s)\, ds
        </me>, 
        which is a lot like our growth-factor formula for first-order scalar equations. 
    </p>
    <p>
        This seems like a good place to stop with the matrices.
    </p>
    </subsection>
</section>
</chapter>


