<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="first-systems" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>First-order linear systems</title>

<section xml:id="fs-structure">
<title>Structure of linear ODE systems</title>
	<p>
		Most phenomena in the real world do not have just one dependent variable. 
	</p>
	<example xml:id="fs-ex-twotanks">
		<p>
			Consider two connected tanks holding brine. Tank 1 holds 100 L, has an input of 4 L/hr of brine of concentration 3 kg/L, and an output of 6 L/hr. Tank 2 holds 200 L, has an input of 7 L/hr of brine of concentration 5 kg/L, and an output of 5 L/hr. Tank 1 pumps 1 L/hr into tank 2, and tank 2 pumps 3 L/hr into tank 1. 
		</p>
		<p>
			Let <m>x_i(t)</m> be the mass of salt in tank <m>i</m>. The statements above imply 
			<md>
				<mrow> \dd{x_1}{t} \amp = 4\cdot 3 - 6\cdot \frac{x_1}{100} - 1\cdot \frac{x_1}{100} + 3 \cdot \frac{x_2}{200}  </mrow>
				<mrow> \dd{x_2}{t} \amp = 7\cdot 5 - 5\cdot \frac{x_2}{200} - 3\cdot \frac{x_2}{200} +  1\cdot \frac{x_1}{100}</mrow>
			</md>.
			This is neatly expressed using linear algebra: 
			<me>
				\dd{}{t} \twovec{x_1}{x_2} = \frac{1}{200}\twomat{-14}{3}{2}{-8} \twovec{x_1}{x_2} + \twovec{12}{35} 
			</me>. 
			Here we have naturally extended the derivative of a vector to mean the derivative of each component.
		</p>
	</example>
	<p>
		The model in <xref ref="fs-ex-twotanks"/> is of an important general form. 
	</p>
	<definition xml:id="fs-df-linsystem">
		<title>Linear system of ODEs</title>
		<statement><p>
			A <term>linear system of ODEs</term> is an equation of the form
			<men xml:id="fs-eq-system">
				\bx' = \bA(t)\bx + \bff(t)
			</men>,
			where <m>\bx</m> is an <m>n</m>-dimensional <term>state vector</term>, <m>\bA</m> is an <m>n\times n</m> <term>coefficient matrix</term>, and <m>\bff(t)</m> is an <m>n</m>-dimensional <term>forcing function</term>. If given, an <term>initial condition</term> of the system is a time <m>t_0</m> and vector <m>\bx_0</m> such that <m>\bx(t_0)=\bx_0</m>.
		</p></statement>
	</definition>
	<p>
		Because the matrix in <xref ref="fs-ex-twotanks"/> is constant in time, we would call its model a <term>constant-coefficient</term> system. 
	</p>
	<p>
		It's not a stretch to say that virtually all of the general statements we made about the scalar linear problem <m>x'=a(t)x+f(t)</m> can be remade with boldface/capital letters for the linear system <m>\bx'=\bA(t)\bx+\bff(t)</m>. Those statements relied mainly on linearity, which remains present. So for example, 
	</p>
	<theorem xml:id="fs-general">
		<title>General solutions (first-order linear systems)</title>
		<statement><p>
			Every solution of <m>\bx'=\bA(t)\bx+\bff(t)</m> can be written in the form <m>\bx=\bx_h+\bx_p</m>, where <m>\bx_h</m> is the general solution of <m>\bx'=\bA(t)\bx</m> and <m>\bx_p</m> is any solution of <m>\bx'=\bA(t)\bx+\bff(t)</m>.
		</p></statement>
	</theorem>	
	<p>
		Once again, then, we look first at the homogeneous system (no forcing term), and then for particular solutions of the nonhomogeneous problem.
	</p>
</section>

<section xml:id="fs-homogeneous">
	<title>Homogeneous systems</title>
	<p>
		Given 
		<me>
			\bx' = \bA(t)\bx, 
		</me>
		where <m>\bx\in\Rn{n}</m> and <m>\bA\in\Rmn{n}{n}</m>, we can easily show in the usual way that any linear combination of solutions is also a solution. Thus our real task is to find a basis for the solution space. 
	</p>
	<aside>
		<p>
			You need to read and write carefully when in vector-land. In particular, note that <m>\bx_1</m> refers to the first vector of a collection, while <m>x_1</m> means the first component of a vector <m>\bx</m>.
		</p>
	</aside>
	<p>
		Suppose <m>\bx_1,\ldots,\bx_m</m> are homogeneous solutions of the ODE, and we use a linear combination of them to satisfy an initial condition <m>\bx(t_0)=\bx_0</m>: 
		<me>
			c_1 \bx_1 + \cdots + c_m \bx_m = \bx_0
		</me>. 
		One of our go-to facts will be the equivalence of a linear combination of vectors and matrix-vector multiplication. Here that means we define the <m>n\times m</m> matrix
		<me>
			\bX(t) = \bigl[ \bx_1(t) \: \bx_2(t) \: \cdots \: \bx_m(t)  \bigr]
		</me>
		so that 
		<me>
				\bX(t_0) \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_m \end{bmatrix} = \bx_0
		</me>. 
		This is a linear algebraic system for the coefficients <m>c_j</m>. We can expect a unique solution for any <m>\bx_0</m> if and only if <m>m=n</m> and <m>\bX(t_0)</m> is invertible-or equivalently, when its columns are a basis for <m>\Rn{n}</m>. 
	</p>
	<definition xml:id="fs-df-fundamental">
		<title>Fundamental matrix</title>
		<statement><p>
			The <m>n\times n</m> matrix <m>\bX(t)</m> is a <idx>fundamental matrix</idx><term>fundamental matrix</term> of the homogeneous system <m>\bx' = \bA(t)\bx</m> if its columns satisfy <m>\bx_j'=\bA\bx_j</m> at all times and they form a basis at time <m>t=t_0</m>. 
		</p></statement>
	</definition>
	<p>
		Because the nonsingularity of a fundamental matrix is one of its key properties, its determinant plays an important role.
	</p>
	<definition xml:id="ls-df-wronskian">
		<title>Wronskian</title>
		<statement><p>
			The <idx>Wronskian</idx><term>Wronskian</term> of a collection of homogeneous solutions <m>\bx_1,\ldots,\bx_n</m> is 
			<me>
				W(t) = \det\Bigl( \bigl| \bx_1(t) \: \cdots \: \bx_n(t)  \bigr| \Bigr)
			</me>.
		</p></statement>
	</definition>
	<p>
		Thus, solutions with a nonzero Wronskian are the columns of a fundamental matrix. There is a remarkable result known as <em>Abel's formula</em> that greatly simplifies this issue. 
	</p>
	<theorem xml:id="fs-thm-abel">
		<title>Homogeneous solution independence</title>
		<statement><p>
			Suppose that <m>\bA(t)</m> is continuous on an open interval <m>I</m>, and let <m>\bx_1,\ldots,\bx_n</m> be solutions of <m>\bx'=\bA\bx</m> whose Wronskian is <m>W(t)</m>. Then if <m>W</m> is nonzero at any time in <m>I</m>, it is nonzero at all times in <m>I</m>. 
		</p></statement>
	</theorem>
</section>

<section xml:id="fs-constcoeff">
	<title>Constant-coefficient systems</title>
	<p>
		The solutions of <m>\bx'=\bA(t)\bx</m> largely boil down to finding a fundamental matrix, which satisfies  
		<me>
			\bX' = \bigl[ \bx_1' \: \bx_2' \: \cdots \: \bx_n'  \bigr] = \bigl[ \bA\bx_1 \: \bA\bx_2 \: \cdots \: \bA\bx_n  \bigr] = \bA \bX
		</me>. 
		With such a matrix in hand, the general solution is <m>\bx(t)=\bX(t)\bc</m> for an arbitrary <q>integration constant</q> <m>\bc</m>. 
	</p>
	<p>
		The good news for the general case pretty much ends there; in most problems we cannot get a reasonably simple expression for <m>\bX</m>. But there is a very important special case in which we can say a lot: when <m>\bA(t)</m> is a constant matrix. 
	</p>
	<p>
		We begin the journey with an eigenpair of <m>\bA</m>,
		<me>
			\bA \bv = \lambda \bv
		</me>. 
		If we posit that <m>\bx(t)=g(t)\bv</m>, then 
		<me>
			g'\bv = \bA(g\bv) = g(\bA\bv) = g\lambda \bv
		</me>,
		which can be satisfied if <m>g'=\lambda g</m>, or <m>g(t)=ce^{\lambda t}</m>. That is, each eigenpair gives us a homogeneous solution! (Yes, this the moment all the eigenstuff was preparing us for.)
	</p>
	<p>
		Counting algebraic multiplicities, we know that <m>\bA</m> has eigenvalues <m>\lambda_1,\ldots,\lambda_n</m>. Say that we have eigenvectors <m>\bv_1,\ldots,\bv_n</m> to go with them. Then we have <m>n</m> homogeneous solutions
		<me>
			\bx_j(t) = e^{\lambda_j t} \bv_j, \quad j=1,\ldots,n
		</me>. 
		To determine whether they form a fundamental matrix, we need to determine whether they form a basis. According to <xref ref="fs-thm-abel"/>, we can ask that question at any value of <m>t</m>, including <m>t=0</m>. So the key issue is the independence of the eigenvectors. 
	</p>
	<theorem xml:id="fs-thm-eigensolution">
		<title>Solution by eigenvectors</title>
		<statement><p>
			Let <m>\bA</m> have eigenvalues <m>\lambda_1,\ldots,\lambda_n</m> and corresponding eigenvectors <m>\bv_1,\ldots,\bv_n</m>. If the eigenvectors are independent (that is, they form a basis), then 
			<men xml:id="fs-eq-eigensolution">
				\bX(t) = \begin{bmatrix} 
				e^{\lambda_1 t} \bv_1 \amp \cdots \amp e^{\lambda_n t} \bv_n
				\end{bmatrix}
			</men>
			is a fundamental matrix for <m>\bx'=\bA\bx</m>.
		</p></statement>
	</theorem>
	<p>
		This situation occurs when <m>\bA</m> has no defective eigenvalues, i.e., it is a diagonalizable matrix (<xref ref="la-diagonalization"/>). One important case when that is guaranteed is when the eigenvalues are all distinct (<xref ref="la-thm-distincteigs"/>). 
	</p>
	<example>
		<p>
			We can find the eigenstuff of 
			<me>
				\bA = \begin{bmatrix} 1 \amp 1 \\ 4 \amp 1 \end{bmatrix}
			</me> 
			as <m>\lambda_1=3</m>, <m>\bv_1 = \twovec{1}{2}</m>, and <m>\lambda_2=-1</m>, <m>\bv_2=\twovec{1}{-2}</m>. This gives us solutions of <m>\bx'=\bA\bx</m>:
			<me>
				\bx_1(t) = e^{3t} \twovec{1}{2},\quad \bx_2(t) = e^{-t} \twovec{1}{-2} 
			</me>.
			Because the eigenvalues are distinct, we know that these are independent solutions. The general homogeneous solution is 
			<me>
				\bx(t) = \bX(t)\bc = c_1 e^{3t} \twovec{1}{2} + c_2 e^{-t} \twovec{1}{-2} 
				= \twovec{ c_1 e^{3t} + c_2 e^{-t}}{2c_1 e^{3t} - 2 c_2e^{-t}}
			</me>.
		</p>
	</example>
	<p>
		Matrices with real, distinct eigenvalues will always look like that example. Things get trickier with complex eigenvalues and defective eigenvalues, and we need to spend some time with each. 
	</p>
</section>

<section xml:id="fs-complex-exponentials">
	<title>Complex exponentials</title>
	<introduction>
		<p>
			How are we going to make sense of <m>e^{\lambda t}\bv</m> when <m>\lambda</m> is not a real number? This question leads to some really profound and useful mathematics. 
		</p>
		<p>
			Let's start with the simplest, scalar case, <m>e^{it}</m>. This should be the solution of <m>x'=ix</m>, <m>x(0)=1</m>. If we assume that <m>x(t)</m> is complex-valued, let us write it as 
			<me>
				x(t) = u(t) + i v(t)
			</me>,
			for real <m>u,v</m>. Then <m>x'=ix</m> becomes
			<me>
				u' + iv' = i(u+iv) = i^2v + iu = -v + iu
			</me>.
			This leads to the coupled real equations
			<me>
				u' = -v, \quad v'=u
			</me>,
			together with <m>u(0)=1</m>, <m>v(0)=0</m>. It won't take you long to verify that <m>u(t)=\cos(t)</m>, <m>v(t)=\sin(t)</m> is the solution. So this implies the identity 
		</p>
		<fact>
			<title>Euler's identity</title>
			<p>
			<idx>Euler's identity</idx>
			<men xml:id="fs-eq-euler">
				e^{it} = \cos(t) + i\sin(t)
			</men>.
			</p>
		</fact>
		<p>
			This is one of the big and fundamental surprises of applied math: exponential functions, when you go in an imaginary direction, become sin/cos oscillations. We can illustrate this with MATLAB, which handles complex values routinely. Note the use of <c>1i</c> to get the imaginary unit.
		</p>
		<sidebyside>
			<program xml:id="exp_complex" language="matlab">
			<input>
				c = @(t) real(exp(1i*t));
				s = @(t) imag(exp(1i*t));
				fplot({c,s},[0,4*pi])
				xlabel('t'), ylabel('e^{it}')
				title('Complex exponential')
				legend('Re part','Im part')
			</input>
			</program>
			<image source="matlab/exp_complex.svg"/>
		</sidebyside>
		<!--p>
			In fact, the path of <m>e^{it}</m> in the complex plane for real <m>t</m> is just a unit circle. 
		</p>
		<sidebyside>
			<program language="matlab" xml:id="exp_circle">
			<input>
				c = @(t) real(exp(1i*t));
				s = @(t) imag(exp(1i*t));
				fplot(c,s,[0,2*pi])
				axis equal, axis(1.05*[-1 1 -1 1])
				xlabel('Re'), ylabel('Im')
				title('Complex exponential = Unit circle')
			</input>
			</program>
			<image source="matlab/exp_circle.svg"/>
		</sidebyside-->
	</introduction>
	<subsection xml:id="fs-complex-polar">
		<title>Polar form of complex numbers</title>
		<p>
			Euler's identity <xref ref="fs-eq-euler"/> gives us an alternative to the rectangular decomposition <m>z=u+iv</m>. If <m>r</m> is a positive number, and we write
			<me>
				re^{i\theta} = (r\cos \theta)+i(r\sin \theta)
			</me>,
			then it is clear that <m>(r,\theta)</m> represent the polar coordinates of the point. Furthermore, the polar <m>r</m> is just the modulus <m>|z|</m>. (The angle <m>\theta</m> might variously be called the <term>argument</term> or <term>phase</term> of <m>z</m>.) 
		</p>
		<p>
			In rectangular form, addition and subtraction of complex numbers is easy and naturally interpreted as vector addition in the plane. In polar form, it's multiplication and division that become easy:
			<me>
				(re^{i\theta})(se^{i\phi}) = rse^{i(\theta+\phi)},  \qquad   \frac{re^{i\theta}}{se^{i\phi}} = \frac{r}{s}e^{i(\theta-\phi)}
			</me>. 
			Geometrically, multiplication by a complex number scales the magnitude and then rotates around the origin. 
		</p>
		<!--p>
			More fun: observe that if <m>t</m> is real, 
			<me>
				\overline{e^{it}} = e^{-it} = \frac{1}{e^{it}} 
			</me>.
			Exponentials can replace some trig identities. For instance, 
			<me>
				e^{2it} = [e^{it}]^2 = (\cos^2(t) - \sin^2(t)) + i (2\cos(t)\sin(t))
			</me>,
			which gives two double-angle formulas in one. 
		</p-->
	</subsection>
</section>

<section xml:id="fs-complex-eigenvalues">
	<title>Complex solutions</title>
	<p>
		For a real matrix <m>\bA</m>, complex eigenvalues and their corresponding eigenvectors occur in conjugate pairs. 
	</p>
	<example>
		<p>
			We eariler found the eigenvalues and eigenvectors of
			<me>
				\bA = \begin{bmatrix} 1 \amp -1 \\ 5 \amp -3 \end{bmatrix}
			</me>. 
			The result is 
			<me>
				\lambda_{1,2} = -1 \pm 1i, \qquad \bv_{1,2} = \twovec{1}{2\mp i}
			</me>.
			This gives a general solution in the form 
			<me>
				\bx(t) = c_1 e^{(-1+i)t} \twovec{1}{2- i} + c_2 e^{(-1-i)t} \twovec{1}{2+i}
			</me>. 
			It's implied that <m>c_{1,2}</m> are complex constants. 
		</p>
		<p>
			There is more to the story. A purely real problem, with real initial conditions, will lead to a solution that is real at all times. In that case it works out that <m>c_2=\overline{c_1}</m>. But an alternative way to express the solution is to take the real and imaginary parts of 
			<md>
				<mrow>e^{(-1+i)t} \twovec{1}{2-i} \amp = e^{-t} [\cos(t)+i\sin(t)] \left( \twovec{1}{2} + i\twovec{0}{-1} \right)</mrow>
				<mrow>\amp = e^{-t} \left( \cos(t)\twovec{1}{2} - \sin(t) \twovec{0}{-1} \right)</mrow>
				<mrow> \amp \qquad + i e^{-t} \left( \cos(t)\twovec{0}{-1} + \sin(t) \twovec{1}{2} \right) </mrow>
			</md>. 
			That is, the general solution can also be written as 
			<me>
				\bx(t) = b_1 e^{-t} \twovec{\cos(t)}{2\cos(t)+\sin(t)} + b_2 e^{-t}\twovec{\sin(t)}{2\sin(t)-\cos(t)} 
			</me>,
			where here <m>b_1</m> and <m>b_2</m> are real. 
		</p>
	</example>
	<p>
		As you can see, converting from the complex to the real form is quite an algebraic slog. It's easier to become comfortable with the complex exponentials themselves. Observe that 	<men xml:id="fs-eq-complex-exp">
			e^{(a+i \omega)t} =  e^{at} (\cos \omega t + i \sin \omega t)
		</men>.
		Reading from right to left, we have the product of <m>e^{i\omega t}</m>, which goes around and around the unit circle at constant angular velocity <m>\omega</m> , and <m>e^{at}</m>, which is an amplitude that exponentially grows or decays (or stays fixed if <m>a=0</m>).  
	</p>
	<example>
		<p> 
			Here are some looks at the function <m>e^{\lambda t}</m> for different complex values of <m>\lambda</m>. First, when <m>\lambda</m> is purely imaginary, the function values stay on the unit circle in the complex plane. Taking the real and imaginary parts of the function give cosine and sine, respectively. 
		</p>
		<sidebyside>
			<program language="matlab" xml:id="so_cexp_neutral">
				<input>
a = 0;  om = 1;
t = linspace(0, 25, 500);
f = exp((a+1i*om)*t);

plot3(t, real(f), imag(f), 'LineWidth',2)
hold on
plot3(t,real(f), 0*t-1.5)
plot3(t, 0*t+2, imag(f))
plot3(0*t+30,real(f),imag(f),'k')
axis([0 30 -2  2 -1.5 1.5])
title(sprintf('a = %.2f, omega = %.1f',a,om))
grid on, xlabel('Time')
ylabel('Real Axis')
zlabel('Imag Axis')	
set(gca,'dataaspect',[6,1,1])    			
				</input>
			</program>
			<image source="matlab/so_cexp_neutral.svg"/>	  
		</sidebyside>
		<p>
			If <m>\Re(\lambda) \gt 0</m>, the magnitude of the function grows exponentially. The result is an outward spiral. The imaginary part of <m>\lambda</m> controls the frequency or "tightness" of the spiral. 
		</p>
		<sidebyside>
			<program language="matlab" xml:id="so_cexp_grow">
				<input>
a = 0.01;  om = 2;
t = linspace(0, 25, 500);
f = exp((a+1i*om)*t);

plot3(t, real(f), imag(f), 'LineWidth',2)
hold on
plot3(t,real(f), 0*t-1.5)
plot3(t, 0*t+2, imag(f))
plot3(0*t+30,real(f),imag(f),'k')
axis([0 30 -2  2 -1.5 1.5])
title(sprintf('a = %.2f, omega = %.1f',a,om))
grid on, xlabel('Time')
ylabel('Real Axis')
zlabel('Imag Axis')	  
set(gca,'dataaspect',[6,1,1])  			
				</input>
			</program>
			<image source="matlab/so_cexp_grow.svg"/>	  
		</sidebyside>
		<p>
			Finally, if <m>\Re(\lambda) \lt 0</m>, the spiral is a decaying one. The real and imaginary parts are attenuated oscillations. 
		</p>
		<sidebyside>
			<program language="matlab" xml:id="so_cexp_decay">
				<input>
a = -0.1;  om = 0.8;
t = linspace(0, 25, 500);
f = exp((a+1i*om)*t);

plot3(t, real(f), imag(f), 'LineWidth',2)
hold on
plot3(t,real(f), 0*t-1.5)
plot3(t, 0*t+2, imag(f))
plot3(0*t+30,real(f),imag(f),'k')
axis([0 30 -2  2 -1.5 1.5])
title(sprintf('a = %.2f, omega = %.1f',a,om))
grid on, xlabel('Time')
ylabel('Real Axis')
zlabel('Imag Axis')	 
set(gca,'dataaspect',[6,1,1])   			
				</input>
			</program>
			<image source="matlab/so_cexp_decay.svg"/>	  
		</sidebyside>
	</example>
	<p>
		The upshot is that in the function <m>e^{\lambda t}</m>, the real part of <m>\lambda</m> determines exponential growth or decay, and the imaginary part of <m>\lambda</m> determines frequency of oscillation. All real exponentials, and all harmonic sines and cosines, are just special cases.
	</p>
</section>

<section xml:id="fs-phaseplane">
	<title>Phase plane</title>
	<introduction>
		<p>
			It's useful to discuss more specifically what can happen with the homogeneous equation <m>\bx'=\bA\bx</m> in the two-dimensional, constant-matrix case. Not only is this the simplest possible non-scalar case, but it's easily visualized using the <idx>phase plane</idx><term>phase plane</term>, which is a plot of trajectories and directions in the <m>(x_1,x_2)</m> plane. 
		</p>
		<p>
			Of special interest is the steady state at the origin, <m>\bx=\bzero</m>. In scalar problems we found the stability of a steady state to be of interest, and that applies here as well. In higher dimensions we often call a steady state a <idx><h>fixed point</h><seealso>steady state</seealso></idx><term>fixed point</term>. The behavior near a fixed point depends strongly on the eigenvalues <m>\lambda_1,\lambda_2</m>. The major cases to be considered are summarized in <xref ref="fs-tb-stability"/>, leaving out special <q>edge cases</q> of zeros or equalities. 
		</p>
		<table xml:id="fs-tb-stability">
			<title>Eigenvalues and fixed point</title>
			<tabular>
			<row>
				<cell><term>Eigenvalues</term></cell>
				<cell><term>Type</term></cell>
				<cell><term>Stability</term></cell>
			</row>
			<row>
				<cell><m>\lambda_1 \lt \lambda_2 \lt 0</m></cell>
				<cell>node</cell>
				<cell>stable</cell>
			</row>
			<row>
				<cell><m>\lambda_1 \lt 0 \lt \lambda_2 </m></cell>
				<cell>saddle</cell>
				<cell>unstable</cell>
			</row>
			<row>
					<cell><m>0 \lt \lambda_1 \lt \lambda_2 </m></cell>
					<cell>node</cell>
					<cell>unstable</cell>
			</row>
			<row>
				<cell><m>\lambda = a\pm ib,\: a\lt 0</m></cell>
				<cell>spiral</cell>
				<cell>stable</cell>
			</row>
			<row>
				<cell><m>\lambda = a\pm ib,\: a\gt 0</m></cell>
				<cell>spiral</cell>
				<cell>unstable</cell>
			</row>
			</tabular>
		</table>		
	</introduction>
	<subsection xml:id="fs-pp-node">
		<title>Node</title>
		<p>
			When the eigenvalues are real and have the same sign, the fixed point is called a <idx>node</idx><term>node</term>. If the sign is negative, all trajectories head into the fixed point and it is stable; conversely, in the positive case it is unstable. 
		</p>
		<example>
			<p>
				Here is a phase plane plot for <m>\bA=\twomat{-4}{-1}{-2}{-5}</m>, which has eigenvalues <m>-3</m> and <m>-6</m>. 
			</p>
			<sidebyside>
				<program language="matlab" xml:id="fs_pp_nodestable">
				<input>
					A = [-4 -1; -2 -5];
					[V,D] = eig(A);
					lam = diag(D);
					
					clf
					plot(0,0,'r.','markersize',18)
					hold on
					
					% plot directions
					[x1,x2] = meshgrid(-1:.2:1);
					v = A*[x1(:)';x2(:)'];
					sz = size(x1);
					quiver(x1,x2,reshape(v(1,:),sz),reshape(v(2,:),sz))
					
					% plot eigenvectors
					plot(5*[-V(1,:);V(1,:)],5*[-V(2,:);V(2,:)],'k','linew',2)
					
					% plot trajectories
					colr = get(gca,'colororder');
					t = linspace(0,10,300);
					for theta = 2*pi*(0:23)/24
						for j = 1:length(t)
							X = expm(t(j)*A);
							x(:,j) = X*[ cos(theta); sin(theta) ];
						end
						plot(x(1,:),x(2,:),'linew',1.5,'color',colr(1,:))
					end
					
					axis([-1.1 1.1 -1.1 1.1]), axis square
					title('stable node')
					xlabel('x_1')
					ylabel('x_2')		
				</input>
				</program>
				<image source="matlab/fs_pp_nodestable.svg"/>
			</sidebyside>
			<p>
				The black lines show the directions of the eigenvectors. An initial state on one of those lines stays on it forever. Other initial conditions follow a path tending to become parallel to eigenvector <m>\bv_1</m>, since the other component decays out more quickly. The plot for <m>-\bA</m> would just reverse all of the arrows and show an unstable fixed point. 
			</p>
	</example>
	</subsection>
	<subsection xml:id="fs-pp-saddle">
		<title>Saddle</title>
		<p>
			When the eigenvalues are real and of different signs, the fixed point is called a <idx>saddle</idx><term>saddle</term>. A saddle point is always unstable. The part of the solution along the negative eigenvector decays away, but the contribution from the positive eigenvector grows with time. 
		</p>
		<example>
			<p>
				Here is a phase plane plot for <m>\bA=\twomat{-2}{-3}{-6}{-5}</m>, which has eigenvalues <m>1</m> and <m>-8</m>. 
			</p>
			<sidebyside>
				<program language="matlab" xml:id="fs_pp_saddle">
				<input>
						A = [-2 -3; -6 -5];
						[V,D] = eig(A);
						lam = diag(D);
						
						clf
						plot(0,0,'r.','markersize',18)
						hold on
						
						% plot directions
						[x1,x2] = meshgrid(-1:.2:1);
						v = A*[x1(:)';x2(:)'];
						sz = size(x1);
						quiver(x1,x2,reshape(v(1,:),sz),reshape(v(2,:),sz))
						
						% plot eigenvectors
						plot(5*[-V(1,:);V(1,:)],5*[-V(2,:);V(2,:)],'k','linew',2)
						
						% plot trajectories
						colr = get(gca,'colororder');
						t = linspace(0,10,300);
						for theta = 2*pi*(0:23)/24
							for j = 1:length(t)
								X = expm(t(j)*A);
								x(:,j) = X*[ cos(theta); sin(theta) ];
							end
							plot(x(1,:),x(2,:),'linew',1.5,'color',colr(1,:))
						end
						
						axis([-1.1 1.1 -1.1 1.1]), axis square
						title('saddle')
						xlabel('x_1')
						ylabel('x_2')					
					</input>
				</program>
				<image source="matlab/fs_pp_saddle.svg"/>
			</sidebyside>
			<p>
				An initial condition exactly on the stable black line (eigenvector) will approach the origin, but anything else ends up shooting away more or less in the direction of the unstable eigenvector.  
			</p>
		</example>
	</subsection>
	<subsection xml:id="fs-pp-spiral">
		<title>Spiral</title>
		<p>
			When the eigenvalues are complex conjugates, the fixed point is called a <idx>spiral</idx><term>spiral</term>. If the eigenvalues are <m>a \pm ib</m>, then all solutions contain <m>e^{at}e^{\pm ibt}</m>, or equivalently, <m>e^{at}\cos{bt}</m> and <m>e^{at}\sin{bt}</m>. The real part causes growth and instability if <m>a\gt 0</m>, or decay and stability if <m>a \lt 0</m>. The imaginary part determines the angular speed of the spiral. 
		</p>
		<example>
			<p>
			Here is a phase plane plot for <m>\bA=\twomat{1}{-2}{4}{-3}</m>, which has eigenvalues <m>-1\pm 2i</m>. 
			</p>
			<sidebyside>
				<program language="matlab" xml:id="fs_pp_spiral">
				<input>
						A = [1 -2; 4 -3];
						[V,D] = eig(A);
						lam = diag(D);
						
						clf
						plot(0,0,'r.','markersize',18)
						hold on
						
						% plot directions
						[x1,x2] = meshgrid(-1:.2:1);
						v = A*[x1(:)';x2(:)'];
						sz = size(x1);
						quiver(x1,x2,reshape(v(1,:),sz),reshape(v(2,:),sz))
						
						% plot trajectories
						colr = get(gca,'colororder');
						t = linspace(0,7,300);
						for theta = 2*pi*(0:6)/7
							for j = 1:length(t)
								X = expm(t(j)*A);
								c = exp(lam(1)*t(j)-1i*theta);
								x(:,j) = X*[ cos(theta); sin(theta) ];
							end
							plot(x(1,:),x(2,:),'linew',1.5,'color',colr(1,:))
						end
						
						axis([-1.2 1.2 -1.2 1.2]), axis square
						title('stable spiral')
						xlabel('x_1')
						ylabel('x_2')
					</input>					
				</program>
				<image source="matlab/fs_pp_spiral.svg"/>
			</sidebyside>
			<p>
				The eigenvectors are complex and don't show up on the plot; there are no purely linear trajectories.   
			</p>
		</example>
	</subsection>
</section>

<section xml:id="fs-matrix-exp">
	<title>Matrix exponential</title>
	<introduction>
		<p>
			A solution of <m>x'=ax</m> for constant <m>a</m> is <m>x=e^{at}</m>. We know the Taylor series 
			<me>
				e^{at} = 1 + at + \frac{1}{2!}(at)^2 + \frac{1}{3!} (at)^3 + \cdots
			</me>. 
			This series is something we could generalize to a square matrix <m>\bA</m>, for which integer powers are possible:<idx>matrix exponential</idx>
			<men xml:id="fs-eq-matrixexp">
				e^{t\bA} = \bI + t\bA + \frac{1}{2!}t^2 \bA^2 + \frac{1}{3!} t^3 \bA^3 + \cdots
			</men>. 
			Let's not worry too much about whether this converges (it does). What are its properties? 
		</p>
		<theorem xml:id="fs-thm-matrixexp">
			<title>Matrix exponential</title>
			<statement><p>
				Let <m>\bA,\bB</m> be <m>n\times n</m> matrices. Then 
				<ol>
					<li><m>e^{t\bA}=\bI</m> if <m>t=0</m>,</li>
					<li><m>\dd{}{t}e^{t\bA} = \bA e^{t\bA} = e^{t\bA}\bA</m>,</li>
					<li><m>[e^{t\bA}]^{-1} = e^{-t\bA}</m>, and</li>
					<li>If <m>\bA\bB=\bB\bA</m>, then <m>e^{\bA+\bB} = e^{t\bA}e^{t\bB} = e^{t\bB}e^{t\bA}</m>.</li>
				</ol>
			</p></statement>
		</theorem>
		<p>
			The first three conclusions of the theorem follow quickly from the series definition <xref ref="fs-eq-matrixexp"/>. (The last is a deeper fact that is important below as well as in other areas such as theoretical physics.) They establish some important facts.  
		</p>
		<fact>
			<p>
				<ol>
					<li>For any constant vector <m>\bc</m>, <m>\bx(t)=e^{t\bA}\bc</m> is a solution of <m>\bx'=\bA\bx</m>.</li>
					<li><m>\bX(t)=e^{t\bA}</m> is a fundamental matrix for <m>\bx'=\bA\bx</m> with <m>\bX(0)=\bI</m>.</li>
				</ol>
			</p>
		</fact>
		<p>
			Obviously, summing an infinite series of matrices doesn't seem like a great prospect. But we have two ways around that.
		</p>
	</introduction>
	<subsection xml:id="fs-me-diagonal">
		<title>Nondefective matrix</title>
		<p>
			In section <xref ref="la-ei-multiplicity"/> we saw the definition of a defective eigenvalue. A matrix with any defective eigenvalues is also called defective. One characterization of a defective <m>n\times n</m> matrix is that it has less than <m>n</m> independent eigenvectors. (That is, the sum of the eigenspace dimensions is less than <m>n</m>.) It's easy to spot when <m>n=2</m>:
		</p>
		<fact>
			<p>A <m>2\times 2</m> matrix with repeated eigenvalue is either diagonal or defective.</p>
		</fact>
		<p>
			For a nondefective <m>\bA</m>, <xref ref="fs-thm-eigensolution"/> guarantees that 
			<me>
				\bX(t) = \begin{bmatrix} e^{t\lambda_1}\bv_1 \amp e^{t\lambda_2}\bv_2 \amp \cdots \amp e^{t\lambda_n}\bv_n \end{bmatrix}  
			</me>
			is a fundamental matrix for the ODE <m>\bx'=\bA\bx</m>, where the <m>(\lambda_j,\bv_j)</m> are eigenpairs. If we now define 
			<me>
				\bY(t) = \bX(t) \bX(0)^{-1}
			</me>, 
			then 
			<me>
				\bY'(t) = \bX'(t) \bX(0)^{-1} = \bA \bX(t) \bX(0)^{-1} = \bA \bY(t)
			</me>,
			so <m>\bY(t)</m> is a fundamental matrix as well. But also <m>\bY(0)=\bX(0)\bX(0)^{-1}=\bI</m>, so in fact <m>\bY(t)</m> is the matrix exponential. 
		</p>
		<p>
			When this argument is fully unraveled, it can be summarized as follows. Given a complete set of eigenvectors, i.e.,
			<me>
				\bA \bv_1 = \lambda_1 \bv_1, \;
				\bA \bv_2 = \lambda_2 \bv_2,\; \dots, \;
				\bA \bv_n = \lambda_n \bv_n
			</me>,
			define 
			<me>
				\bV = \begin{bmatrix} \bv_1 \amp \bv_2 \amp \cdots \amp \bv_n \end{bmatrix}.
			</me>
			(In the notation above, this is <m>\bX(0)</m>.) Then 
			<men xml:id="fs-matrixexp">
				e^{t\bA} = \bV \begin{bmatrix} 
				e^{t\lambda_1} \amp \amp \amp \\ 
				\amp e^{t\lambda_2} \amp \amp \\ 
				\amp \amp \ddots \amp \\ 
				\amp \amp \amp e^{t\lambda_n} 
				\end{bmatrix}  \bV^{-1}
			</men>.
			This is one of those cases in which you're probably better off working from the formula than going each time through the process we used to get it. 
		</p>
		<example>
			<p>
			Let's find the exponential of 
			<me>
				\bA = \twomat{1}{1}{4}{1}
			</me>,
			given <m>\lambda_1=3</m>, <m>\bv_1 = \twovec{1}{2}</m>, and <m>\lambda_2=-1</m>, <m>\bv_2=\twovec{1}{-2}</m>. We identify  
			<me>
				\bV = \twomat{1}{1}{2}{-2}
			</me>
			and compute 
			<md>
				<mrow> e^{t\bA} \amp = \twomat{1}{1}{2}{-2} \twomat{e^{3t}}{0}{0}{e^{-t}} \twomat{1}{1}{2}{-2}^{-1} </mrow>
				<mrow>  \amp = -\frac{1}{4} \twomat{1}{1}{2}{-2} \twomat{e^{3t}}{0}{0}{e^{-t}} \twomat{-2}{-1}{-2}{1} </mrow>
				<mrow>  \amp = -\frac{1}{4} \twomat{e^{3t}}{e^{-t}}{2e^{3t}}{-2e^{-t}} \twomat{-2}{-1}{-2}{1} </mrow>
				<mrow>  \amp = \frac{1}{4} \twomat{2e^{3t}+2e^{-t}}{e^{3t}-e^{-t}}{4e^{3t}-4e^{-t}}{2e^{3t}+2e^{-t}} </mrow>
			</md>. 
			Not exactly a walk in the park, but doable. 
		</p>
		</example>
		<p>
			Because the matrix exponential is a fundamental matrix, the general solution to <m>\bx'=\bA\bx</m> can always be written in the form <m>e^{t\bA}\bc</m>. This isn't the easiest way to get a general solution, because the <m>\bX(t)</m> above requires less work. But this form is handy with an IVP, because 
			<me>
				\bx_0 = \bx(0) = e^{0\bA}\bc = \bc
			</me>. 
			That is, <m>e^{t\bA}\bx_0</m> solves the IVP.  
		</p>
		<example>
			<p>
				Suppose 
				<me>
					\bA = \twomat{1}{-2}{5}{-3}
				</me>, 
				and we are given or work out that 
				<me>
					\lambda_1 = -1+i, \; \lambda_2 = -1-i, \qquad \bV = \twomat{1}{1}{2-i}{2+i}
				</me>.
				Then 
				<me>
					\bV^{-1} = \frac{1}{2+i-2+i} \twomat{2+i}{-1}{-2+i}{1} = \frac{i}{2} \twomat{-2-i}{1}{2-i}{-1}
				</me>,
				and then 
				<md>
					<mrow> e^{t \bA} \amp = e^{-t}\frac{i}{2}\twomat{1}{1}{2-i}{2+i} \twomat{e^{it}}{0}{0}{e^{-it}} \twomat{-2-i}{1}{2-i}{-1} </mrow>
					<mrow>  \amp = e^{-t}\frac{i}{2}\twomat{e^{it}}{e^{-it}}{(2-i)e^{it}}{(2+i)e^{-it}} \twomat{-2-i}{1}{2-i}{-1} </mrow>
					<mrow>  \amp = e^{-t}\frac{i}{2} \twomat{-(2+i)e^{it}+(2-i) e^{-it}}{e^{it}-e^{-it}}{-(2-i)(2+i)e^{it}+(2+i)(2-i) e^{-it}}{(2-i)e^{it}-(2+i) e^{-it}} </mrow>
				</md>. 
				Each entry is written in the form of a complex number minus its conjugate, which gives <m>2i</m> times the imaginary part. This helps a lot.
				<me>
					e^{t \bA} = -e^{-t} \twomat{-\cos(t)-2\sin(t)}{\sin(t)}{-5\sin(t)}{-\cos(t)+2\sin(t)}
				</me>.
			</p>
		</example>
	</subsection>
	<subsection xml:id="fs-me-defective">
		<title>Defective matrix</title>	
		<p>
			In the 2-by-2 case, a defective matrix has a double eigenvalue but is not diagonal. In this case we have no clear way to construct the fundamental <m>\bX(t)</m> that gave us the matrix exponential above. Fortunately, we have another reasonable path for the <m>2\times 2</m> situation. 
		</p>
		<example>
			<p>
				The matrix 
				<me>
					\bA = \twomat{-1}{1}{0}{-1}
				</me>
				has a double eigenvalue of <m>\lambda=-1</m> and is defective. Now consider 
				<me>
					\bB = \bA - \lambda \bI = \twomat{0}{1}{0}{0}
				</me>. 
				This matrix has the property that <m>\bB^2=\bzero</m>. That means the exponential series for it is just 
				<me>
					e^{t\bB} = \bI + t\bB = \twomat{1}{t}{0}{1}
				</me>. 
				Finally, we can use <xref ref="fs-thm-matrixexp"/> to calculate
				<me> 
					e^{t\bA} = e^{t\lambda\bI}e^{t(\bA-\lambda\bI)} = e^{-t}\twomat{1}{t}{0}{1}
				</me>. 
			</p>
		</example>
		<p>
			Matters are not so simple for larger matrices, but we won't be going there. 
		</p>
	</subsection>
</section>

<section xml:id="fs-varparam">
	<title>Variation of parameters</title>
	<p>
		We return to the forced problem <m>\bx'=\bA\bx+\bf</m>. We can use the same technique of variation of parameters as in <xref ref="fl-varparam"/> and get very similar formulas. 
	</p>
	<theorem xml:id="fs-thm-varparam">
		<title>Variation of parameters</title>
		<statement><p>
			Let <m>\bX(t)</m> be a fundamental matrix of the homogeneous problem <m>\bx'=\bA(t)\bx</m>. Then the general solution of <m>\bx'=\bA(t)\bx+\bf(t)</m> is 
			<men xml:id="fs-eq-varparam1">
				\bx(t) = \bx_h(t) + \bx_p(t) = \bX(t)\bc + \bX(t) \int \bX(t)^{-1} \bff(t) \, dt
			</men>. 
			The solution with initial condition <m>\bx(0)=\bx_0</m> is 
			<men xml:id="fs-eq-varparam2">
				\bx(t) = \bX(t)\bX(0)^{-1}\bx_0 + \bX(t) \int_0^t \bX(s)^{-1}\bff(s) \, ds
			</men>.	
			In the case of a constant matrix <m>\bA</m>, one option is <m>\bX(t)=e^{t\bA}, and</m>
			<men xml:id="fs-eq-varparam3">
				\bx(t) = e^{t\bA}\bx_0 + \int_0^t e^{(t-s)\bA} \bff(s) \, ds
			</men>.
		</p></statement>
	</theorem>
	<p>
		Given how tedious it is to calculate even a small fundamental matrix by hand, it's no mean feat to apply these formulas to a specific example. We're going to save that for when we really need it later.
	</p>
</section>

</chapter>