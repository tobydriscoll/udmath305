<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch3-first-order-systems" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>First-order linear systems</title>

<section xml:id="fs-structure">
<title>Structure of linear ODE systems</title>
	<p>
		Most phenomena in the real world do not have just one dependent variable. 
	</p>
	<example xml:id="fs-ex-twotanks">
		<p>
			Consider two connected tanks holding brine. Tank 1 holds 100 L, has an input of 4 L/hr of brine of concentration 3 kg/L, and an output of 6 L/hr. Tank 2 holds 200 L, has an input of 7 L/hr of brine of concentration 5 kg/L, and an output of 5 L/hr. Tank 1 pumps 1 L/hr into tank 2, and tank 2 pumps 3 L/hr into tank 1. 
		</p>
		<p>
			Let <m>x_i(t)</m> be the mass of salt in tank <m>i</m>. The statements above imply 
			<md>
				<mrow> \dd{x_1}{t} \amp = 4\cdot 3 - 6\cdot \frac{x_1}{100} - 1\cdot \frac{x_1}{100} + 3 \cdot \frac{x_2}{200}  </mrow>
				<mrow> \dd{x_2}{t} \amp = 7\cdot 5 - 5\cdot \frac{x_2}{200} - 3\cdot \frac{x_2}{200} +  1\cdot \frac{x_1}{100}</mrow>
			</md>.
			This is neatly expressed using linear algebra: 
			<me>
				\dd{}{t} \twovec{x_1}{x_2} = \frac{1}{200}\twomat{-14}{3}{2}{-8} \twovec{x_1}{x_2} + \twovec{12}{35} 
			</me>. 
			Here we have naturally extended the derivative of a vector to mean the derivative of each component.
		</p>
	</example>
	<p>
		The model in <xref ref="fs-ex-twotanks"/> is of an important general form. 
	</p>
	<definition xml:id="fs-df-linsystem">
		<title>Linear system of ODEs</title>
		<statement><p>
			A <term>linear system of ODEs</term> is an equation of the form
			<men xml:id="fs-eq-system">
				\bx' = \bA(t)\bx + \bff(t)
			</men>,
			where <m>\bx</m> is an <m>n</m>-dimensional <term>state vector</term>, <m>\bA</m> is an <m>n\times n</m> <term>coefficient matrix</term>, and <m>\bff(t)</m> is an <m>n</m>-dimensional <term>forcing function</term>. If given, an <term>initial condition</term> of the system is a time <m>t_0</m> and vector <m>\bx_0</m> such that <m>\bx(t_0)=\bx_0</m>.
		</p></statement>
	</definition>
	<p>
		Because the matrix in <xref ref="fs-ex-twotanks"/> is constant in time, we would call its model a <term>constant-coefficient</term> system. 
	</p>
	<p>
		It's not a stretch to say that virtually all of the general statements we made about the scalar linear problem <m>x'=a(t)x+f(t)</m> can be remade with boldface/capital letters for the linear system <m>\bx'=\bA(t)\bx+\bff(t)</m>. Those statements relied mainly on linearity, which remains present. So for example, 
	</p>
	<theorem xml:id="fs-general">
		<title>General solutions (first-order linear systems)</title>
		<statement><p>
			Every solution of <m>\bx'=\bA(t)\bx+\bff(t)</m> can be written in the form <m>\bx=\bx_h+\bx_p</m>, where <m>\bx_h</m> is the general solution of <m>\bx'=\bA(t)\bx</m> and <m>\bx_p</m> is any solution of <m>\bx'=\bA(t)\bx+\bff(t)</m>.
		</p></statement>
	</theorem>	
	<p>
		Once again, then, we look first at the homogeneous system (no forcing term), and then for particular solutions of the nonhomogeneous problem.
	</p>
	<exercises xml:id="fs-exer-linsysmodels">
		<title>Practice for linear systems of ODEs</title>
		<exercise>
			<title>Heating a house</title>
			<statement>
			  <p>
				Here is an (overly) simple model for heating in a house. Let <m>b(t)</m> be the temperature of the basement, <m>m(t)</m> be the temperature of the main living area, and <m>a(t)</m> be the temperature of the attic. Suppose the ground is at a constant 10 degrees C. We use a Newtonian model to describe how the temperature of the basement evolves due to interactions with the earth and the main floor:
				<me>
					\frac{db}{dt} = -k_b (b - 10) - k_{mb} (b-m). 
				</me>
				Similarly, the attic interacts with the air, which we will hold at 2 degrees, and the main floor:
				<me>
					\frac{da}{dt} = -k_a (a - 2) - k_{ma} (a-m). 
				</me>
				Finally, suppose the main area interacts mostly with the other levels and experiences input from a heater:
				<me>
					\frac{dm}{dt} = -k_{mb} (m - b) - k_{ma} (m - a) + h(t).
				</me>
				Using the vector variable <m>\bx = \bigl[b,m,a\bigr]</m>, write the entire system in the form <m>\bx' = \bA \bx + \bff(t)</m>. That is, identify what <m>\bA</m> and <m>\bff(t)</m> must be based on the above equations. 
			  </p>
			  <p>
				  Now suppose that the heater is shut off and the three temperatures settle into a steady state (i.e., stop changing in time). Write out an algebraic linear system of equations for the temperatures. 
			  </p>
			</statement>
			<answer>
			  <p>
				<me>
					\frac{d}{dt} \threevec{b}{m}{a} = 
					\begin{bmatrix} -k_b-k_{mb} \amp k_{mb} \amp 0 \\ k_{mb} \amp -k_{mb}-k_{ma} \amp k_{ma} \\  0 \amp k_{ma} \amp -k_{ma}-k_a \end{bmatrix}\threevec{b}{m}{a} 
					+ \threevec{10k_b}{h(t)}{2k_a}
				</me>
				<me>
					\begin{bmatrix} -k_b-k_{mb} \amp k_{mb} \amp 0 \\ k_{mb} \amp -k_{mb}-k_{ma} \amp k_{ma} \\  0 \amp k_{ma} \amp -k_{ma}-k_a \end{bmatrix}\threevec{b}{m}{a} 
					= - \threevec{10k_b}{0}{2k_a}
				</me>
			  </p>
			</answer>
		</exercise>
	</exercises>
	<solutions divisional="solution"></solutions>
</section>

<section xml:id="fs-homogeneous">
	<title>Homogeneous systems</title>
	<p>
		Given 
		<me>
			\bx' = \bA(t)\bx, 
		</me>
		where <m>\bx\in\Rn{n}</m> and <m>\bA\in\Rmn{n}{n}</m>, we can easily show in the usual way that any linear combination of solutions is also a solution. Thus our real task is to find a basis for the solution space. 
	</p>
	<aside>
		<p>
			You need to read and write carefully when in vector-land. In particular, note that <m>\bx_1</m> refers to the first vector of a collection, while <m>x_1</m> means the first component of a vector <m>\bx</m>.
		</p>
	</aside>
	<p>
		Suppose <m>\bx_1,\ldots,\bx_m</m> are homogeneous solutions of the ODE, and we use a linear combination of them to satisfy an initial condition <m>\bx(t_0)=\bx_0</m>: 
		<me>
			c_1 \bx_1 + \cdots + c_m \bx_m = \bx_0
		</me>. 
		One of our go-to facts will be the equivalence of a linear combination of vectors and matrix-vector multiplication. Here that means we define the <m>n\times m</m> matrix
		<me>
			\bX(t) = \bigl[ \bx_1(t) \: \bx_2(t) \: \cdots \: \bx_m(t)  \bigr]
		</me>
		so that 
		<me>
				\bX(t_0) \begin{bmatrix} c_1 \\ c_2 \\ \vdots \\ c_m \end{bmatrix} = \bx_0
		</me>. 
		This is a linear algebraic system for the coefficients <m>c_j</m>. We can expect a unique solution for any <m>\bx_0</m> if and only if <m>m=n</m> and <m>\bX(t_0)</m> is invertible-or equivalently, when its columns are a basis for <m>\Rn{n}</m>. 
	</p>
	<definition xml:id="fs-df-fundamental">
		<title>Fundamental matrix</title>
		<statement><p>
			The <m>n\times n</m> matrix <m>\bX(t)</m> is a <idx>fundamental matrix</idx><term>fundamental matrix</term> of the homogeneous system <m>\bx' = \bA(t)\bx</m> if its columns satisfy <m>\bx_j'=\bA\bx_j</m> at all times and they form a basis at time <m>t=t_0</m>. 
		</p></statement>
	</definition>
	<aside>
		<p>
			The Kapitula book uses <m>\Phi(t)</m> as the notation for a fundamental matrix. Pot-ay-to, pot-ah-to.
		</p>
	</aside>
	<p>
		Because the nonsingularity of a fundamental matrix is one of its key properties, its determinant plays an important role.
	</p>
	<definition xml:id="ls-df-wronskian">
		<title>Wronskian</title>
		<statement><p>
			The <idx>Wronskian</idx><term>Wronskian</term> of a collection of homogeneous solutions <m>\bx_1,\ldots,\bx_n</m> is 
			<me>
				W(t) = \det\Bigl( \bigl[ \bx_1(t) \: \cdots \: \bx_n(t)  \bigr] \Bigr)
			</me>.
		</p></statement>
	</definition>
	<p>
		Thus, solutions with a nonzero Wronskian are the columns of a fundamental matrix. There is a remarkable result known as <em>Abel's formula</em> that greatly simplifies this issue. 
	</p>
	<theorem xml:id="fs-thm-abel">
		<title>Homogeneous solution independence</title>
		<statement><p>
			Suppose that <m>\bA(t)</m> is continuous on an open interval <m>I</m>, and let <m>\bx_1,\ldots,\bx_n</m> be solutions of <m>\bx'=\bA\bx</m> whose Wronskian is <m>W(t)</m>. Then if <m>W</m> is nonzero at any time in <m>I</m>, it is nonzero at all times in <m>I</m>. 
		</p></statement>
	</theorem>
	<exercises xml:id="fs-exer-homog">
		<title>Practice for homogeneous ODE systems</title>
		<exercise>
			<title>Wronskian of exponentials</title>
			<statement>
				<p>
					Suppose <m>\bx_1(t)=e^{\lambda_1 t}\bv_1</m> and <m>\bx_2(t)=e^{\lambda_2 t}\bv_2</m>, where <m>\bv_1</m> and <m>\bv_2</m> are linearly independent vectors in two dimensions. Find the Wronskian of <m>\bx_1,\bx_2</m> at any time, and explain why it is nonzero. 
				</p>
			</statement>
			<hint>
				<p>
					You don't know the elements of <m>\bv_1,\bv_2</m>, so give them arbitrary names. The Wronskian has a common factor that you can pull out. 
				</p>
			</hint>
			<answer>
				<p>
					<m>W(t) = e^{t(\lambda_1+\lambda_2)} \det\Bigl(\bigl[\bv_1\: \bv_2\bigr]\Bigr)</m>
				</p>
			</answer>
			<solution>
				<p>
					Say <m>\bv_1=[a,\,c]</m> and <m>\bv_2=[b,\,d]</m>. Then 
					<md>
						<mrow>W(t) \amp = \det\Bigl( \bigl[ \bx_1(t) \: \bx_2(t)  \bigr] \Bigr)</mrow>
						<mrow>  \amp = \twodet{ae^{\lambda_1 t}}{be^{\lambda_2 t}}{ce^{\lambda_1 t}}{de^{\lambda_2 t}}</mrow>
						<mrow>  \amp = e^{t(\lambda_1+\lambda_2)} (ad-bc) =  e^{t(\lambda_1+\lambda_2)} \det\Bigl(\bigl[\bv_1\: \bv_2\bigr]\Bigr) </mrow>
					  </md>
					  The exponential function is never zero, and, because of the independence of <m>\bv_1,\bv_2</m>, <xref ref="la-thm-FTLA"/> guarantees that the last determinant above is nonzero as well.
				</p>
			</solution>
		</exercise>
	</exercises>
	<solutions divisional="solution"></solutions>
</section>

<section xml:id="fs-constcoeff">
	<title>Constant-coefficient systems</title>
	<p>
		The solutions of <m>\bx'=\bA(t)\bx</m> largely boil down to finding a fundamental matrix, which satisfies  
		<me>
			\bX' = \bigl[ \bx_1' \: \bx_2' \: \cdots \: \bx_n'  \bigr] = \bigl[ \bA\bx_1 \: \bA\bx_2 \: \cdots \: \bA\bx_n  \bigr] = \bA \bX
		</me>. 
		With such a matrix in hand, the general solution is <m>\bx(t)=\bX(t)\bc</m> for an arbitrary <q>integration constant</q> <m>\bc</m>. 
	</p>
	<p>
		The good news for the general case pretty much ends there; in most problems we cannot get a reasonably simple expression for <m>\bX</m>. But there is a very important special case in which we can say a lot: when <m>\bA(t)</m> is a constant matrix. 
	</p>
	<p>
		We begin the journey with an eigenpair of <m>\bA</m>,
		<me>
			\bA \bv = \lambda \bv
		</me>. 
		If we posit that <m>\bx(t)=g(t)\bv</m>, then 
		<me>
			g'\bv = \bA(g\bv) = g(\bA\bv) = g\lambda \bv
		</me>,
		which can be satisfied if <m>g'=\lambda g</m>, or <m>g(t)=c e^{\lambda t}</m>. That is, each eigenpair gives us a homogeneous solution! (Yes, this the moment all the eigenstuff was preparing us for.)
	</p>
	<p>
		Counting algebraic multiplicities, we know that <m>\bA</m> has eigenvalues <m>\lambda_1,\ldots,\lambda_n</m>. Say that we have eigenvectors <m>\bv_1,\ldots,\bv_n</m> to go with them. Then we have <m>n</m> homogeneous solutions
		<me>
			\bx_j(t) = e^{\lambda_j t} \bv_j, \quad j=1,\ldots,n
		</me>. 
		To determine whether they form a fundamental matrix, we need to determine whether they form a basis. According to <xref ref="fs-thm-abel"/>, we can ask that question at any value of <m>t</m>, including <m>t=0</m>. So the key issue is the independence of the eigenvectors. 
	</p>
	<theorem xml:id="fs-thm-eigensolution">
		<title>Solution by eigenvectors</title>
		<statement><p>
			Let <m>\bA</m> have eigenvalues <m>\lambda_1,\ldots,\lambda_n</m> and corresponding eigenvectors <m>\bv_1,\ldots,\bv_n</m>. If the eigenvectors are independent (that is, they form a basis), then 
			<men xml:id="fs-eq-eigenfundamental">
				\bX(t) = \begin{bmatrix} 
				e^{\lambda_1 t} \bv_1 \amp \cdots \amp e^{\lambda_n t} \bv_n
				\end{bmatrix}
			</men>
			is a fundamental matrix for <m>\bx'=\bA\bx</m>. Hence the general solution can be expressed as 
			<men xml:id="fs-eq-eigengeneral">
				\bx(t) = \bX(t)\bc = c_1 e^{\lambda_1 t} \bv_1 + \cdots + c_n e^{\lambda_n t} \bv_n
			</men>.
		</p></statement>
	</theorem>
	<example xml:id="fs-ex-eigen-real">
		<title>System with real, distinct eigenvalues</title>
		<p>
			We can find the eigenstuff of 
			<me>
				\bA = \begin{bmatrix} 1 \amp 1 \\ 4 \amp 1 \end{bmatrix}
			</me> 
			as <m>\lambda_1=3</m>, <m>\bv_1 = \twovec{1}{2}</m>, and <m>\lambda_2=-1</m>, <m>\bv_2=\twovec{1}{-2}</m>. This gives us solutions of <m>\bx'=\bA\bx</m>:
			<me>
				\bx_1(t) = e^{3t} \twovec{1}{2},\quad \bx_2(t) = e^{-t} \twovec{1}{-2} 
			</me>.
			Because the eigenvalues are distinct, we know that these are independent solutions. The general homogeneous solution is 
			<me>
				\bx(t) = \bX(t)\bc = c_1 e^{3t} \twovec{1}{2} + c_2 e^{-t} \twovec{1}{-2} 
				= \twovec{ c_1 e^{3t} + c_2 e^{-t}}{2c_1 e^{3t} - 2 c_2e^{-t}}
			</me>.
		</p>
	</example>
	<p>
		Matrices with real, distinct eigenvalues will always look like <xref ref="fs-ex-eigen-real"/>. But there are two other situations to consider: repeated eigenvalues and complex eigenvalues. 
	</p>
	<p>
		The situation of repeated eigenvalues splits into two subcases. In section <xref ref="la-ei-multiplicity"/> we saw the definition of a <term>defective</term> eigenvalue, whose eigenspace has dimension lower than its multiplicity in the characteristic polynomial. A matrix with any defective eigenvalues is also called defective, and the opposite situation is . One characterization of a defective <m>n\times n</m> matrix is that it has less than <m>n</m> independent eigenvectors. (That is, the sum of the eigenspace dimensions is less than <m>n</m>.) It's easy to spot when <m>n=2</m>:
	</p>
	<definition xml:id="ls-def-defectivematrix">
		<title>Defective matrix</title>
		<statement><p>
			A matrix with one or more defective eigenvalues is called <idx>defective</idx><term>defective</term>. A matrix with no defective eigenvalues is called <idx>nondefective</idx><term>nondefective</term>. 
		</p></statement>
	</definition>
	<p>
		Using this new language, <xref ref="fs-thm-eigensolution"/> applies when <m>\bA</m> is nondefective, even when it has some repeated eigenvalues. In the two-dimensional case this is easy to spot. 
	</p>
	<fact>
		<p>A <m>2\times 2</m> matrix with a repeated eigenvalue is either diagonal or defective.</p>
	</fact>
	<p>
		The situation of a defective matrix will be touched on in <xref ref="fs-matrix-exp"/>. 
	</p>
	<p>	
		With complex eigenvalues the algebra gets nastier, but the principles don't change. In fact <xref ref="fs-eq-eigengeneral"/> remains a valid solution expression. However, we do have to come to understand in <xref ref="fs-complex-exponentials"/> what it means to take the exponential of a complex number. 
	</p>
</section>

<section xml:id="fs-complex-exponentials">
	<title>Complex eigenvalues</title>
	<introduction>
		<p>
			How are we going to make sense of <m>e^{\lambda t}\bv</m> when <m>\lambda</m> is not a real number? This question leads to some really profound and useful mathematics. 
		</p>
		<p>
			Let's start with the simplest, scalar case, <m>e^{it}</m>. This should be the solution of <m>x'=ix</m>, <m>x(0)=1</m>. If we assume that <m>x(t)</m> is complex-valued, let us write it as 
			<me>
				x(t) = u(t) + i v(t)
			</me>,
			for real <m>u,v</m>. Then <m>x'=ix</m> becomes
			<me>
				u' + iv' = i(u+iv) = i^2v + iu = -v + iu
			</me>.
			This leads to the coupled real equations
			<me>
				u' = -v, \quad v'=u
			</me>,
			together with <m>u(0)=1</m>, <m>v(0)=0</m>. It won't take you long to verify that <m>u(t)=\cos(t)</m>, <m>v(t)=\sin(t)</m> is the solution. So this implies the identity 
		</p>
		<fact>
			<title>Euler's identity</title>
			<p>
			<idx>Euler's identity</idx>
			<men xml:id="fs-eq-euler">
				e^{it} = \cos(t) + i\sin(t)
			</men>.
			</p>
		</fact>
		<p>
			This is one of the big and fundamental surprises of applied math: exponential functions, when you go in an imaginary direction, become sin/cos oscillations. We can illustrate this with MATLAB, which handles complex values routinely. Note the use of <c>1i</c> to get the imaginary unit.
		</p>
		<sidebyside>
			<listing xml:id="exp_complex">
				<caption>The parts of a complex exponential.</caption>
				<program language="matlab">
				<input>
					c = @(t) real(exp(1i*t));
					s = @(t) imag(exp(1i*t));
					fplot({c,s},[0,4*pi])
					xlabel('t'), ylabel('e^{it}')
					title('Complex exponential')
					legend('Re part','Im part')
				</input>
				</program>
			</listing>
			<image source="figures/exp_complex.svg"/>
		</sidebyside>
		<!--p>
			In fact, the path of <m>e^{it}</m> in the complex plane for real <m>t</m> is just a unit circle. 
		</p>
		<sidebyside>
			<program language="matlab" xml:id="exp_circle">
			<input>
				c = @(t) real(exp(1i*t));
				s = @(t) imag(exp(1i*t));
				fplot(c,s,[0,2*pi])
				axis equal, axis(1.05*[-1 1 -1 1])
				xlabel('Re'), ylabel('Im')
				title('Complex exponential = Unit circle')
			</input>
			</program>
			<image source="figures/exp_circle.svg"/>
		</sidebyside-->
	</introduction>
	<subsection xml:id="fs-complex-polar">
		<title>Polar form of complex numbers</title>
		<p>
			Euler's identity <xref ref="fs-eq-euler"/> gives us an alternative to the rectangular decomposition <m>z=u+iv</m>. If <m>r</m> is a positive number, and we write
			<me>
				re^{i\theta} = (r\cos \theta)+i(r\sin \theta)
			</me>,
			then it is clear that <m>(r,\theta)</m> represent the polar coordinates of the point. Furthermore, the polar <m>r</m> is just the modulus <m>|z|</m>. (The angle <m>\theta</m> might variously be called the <term>argument</term> or <term>phase</term> of <m>z</m>.) 
		</p>
		<p>
			In rectangular form, addition and subtraction of complex numbers is easy and naturally interpreted as vector addition in the plane. In polar form, it's multiplication and division that become easy:
			<me>
				(re^{i\theta})(se^{i\phi}) = r s e^{i(\theta+\phi)},  \qquad   \frac{re^{i\theta}}{se^{i\phi}} = \frac{r}{s}e^{i(\theta-\phi)}
			</me>. 
			Geometrically, multiplication by a complex number scales the magnitude and then rotates around the origin. 
		</p>
		<!--p>
			More fun: observe that if <m>t</m> is real, 
			<me>
				\overline{e^{it}} = e^{-it} = \frac{1}{e^{it}} 
			</me>.
			Exponentials can replace some trig identities. For instance, 
			<me>
				e^{2it} = [e^{it}]^2 = (\cos^2(t) - \sin^2(t)) + i (2\cos(t)\sin(t))
			</me>,
			which gives two double-angle formulas in one. 
		</p-->
	</subsection>
	<subsection xml:id="ls-cx-complexexp">
		<title>Complex exponentials</title>
		<p>
			There is a key connection between the rectangular form of a complex number and the polar form of its exponential:
			<men xml:id="fs-eq-complex-exp">
				e^{(a+i \omega)t} =  e^{at} e^{i \omega t} = e^{at} (\cos \omega t + i \sin \omega t)
			</men>.
			The first term in the product is the familiar real <m>e^{at}</m>, which is an amplitude that exponentially grows or decays in time (or stays fixed if <m>a=0</m>). The other term has modulus one and can be interpreted as a parameterization of the unit circle in the complex plane with constant angular velocity <m>\omega</m>.  
		</p>
		<example>
			<p> 
				Here are some looks at the function <m>e^{\lambda t}</m> for different complex values of <m>\lambda</m>. First, when <m>\lambda</m> is purely imaginary, the function values stay on the unit circle in the complex plane. Taking the real and imaginary parts of the function give cosine and sine, respectively. 
			</p>
			<sidebyside>
				<listing  xml:id="fs_cexp_neutral">
					<caption>Traveling around the unit circle.</caption>
					<program language="matlab">
						<input>
						a = 0;  om = 1;
						t = linspace(0, 25, 500);
						f = exp((a+1i*om)*t);

						plot3(t, real(f), imag(f), 'LineWidth',2)
						hold on
						plot3(t,real(f), 0*t-1.5)
						plot3(t, 0*t+2, imag(f))
						plot3(0*t+30,real(f),imag(f),'k')
						axis([0 30 -2  2 -1.5 1.5])
						title(sprintf('a = %.2f, omega = %.1f',a,om))
						grid on, xlabel('Time')
						ylabel('Real Axis')
						zlabel('Imag Axis')	
						set(gca,'dataaspect',[6,1,1])    			
						</input>
					</program>
				</listing>
				<image source="figures/fs_cexp_neutral.svg"/>	  
			</sidebyside>
			<p>
				If <m>\Re(\lambda) \gt 0</m>, the magnitude of the function grows exponentially. The result is an outward spiral. The imaginary part of <m>\lambda</m> controls the frequency or "tightness" of the spiral. 
			</p>
			<sidebyside>
				<listing xml:id="fs_cexp_grow">
					<caption>A growing spiral.</caption>
					<program language="matlab">
						<input>
						a = 0.02;  om = 2;
						t = linspace(0, 25, 500);
						f = exp((a+1i*om)*t);

						plot3(t, real(f), imag(f), 'LineWidth',2)
						hold on
						plot3(t,real(f), 0*t-1.5)
						plot3(t, 0*t+2, imag(f))
						plot3(0*t+30,real(f),imag(f),'k')
						axis([0 30 -2  2 -1.5 1.5])
						title(sprintf('a = %.2f, omega = %.1f',a,om))
						grid on, xlabel('Time')
						ylabel('Real Axis')
						zlabel('Imag Axis')	  
						set(gca,'dataaspect',[6,1,1])  			
						</input>
					</program>
				</listing>
				<image source="figures/fs_cexp_grow.svg"/>	  
			</sidebyside>
			<p>
				Finally, if <m>\Re(\lambda) \lt 0</m>, the spiral is a decaying one. The real and imaginary parts are attenuated oscillations. 
			</p>
			<sidebyside>
				<listing  xml:id="fs_cexp_decay">
					<caption>A decaying spiral.</caption>
					<program language="matlab">
						<input>
						a = -0.1;  om = 0.8;
						t = linspace(0, 25, 500);
						f = exp((a+1i*om)*t);

						plot3(t, real(f), imag(f), 'LineWidth',2)
						hold on
						plot3(t,real(f), 0*t-1.5)
						plot3(t, 0*t+2, imag(f))
						plot3(0*t+30,real(f),imag(f),'k')
						axis([0 30 -2  2 -1.5 1.5])
						title(sprintf('a = %.2f, omega = %.1f',a,om))
						grid on, xlabel('Time')
						ylabel('Real Axis')
						zlabel('Imag Axis')	 
						set(gca,'dataaspect',[6,1,1])   			
						</input>
					</program>
				</listing>
				<image source="figures/fs_cexp_decay.svg"/>	  
			</sidebyside>
		</example>
		<fact>
			<p>
				In the function <m>e^{\lambda t}</m>, the real part of <m>\lambda</m> determines exponential growth or decay of amplitude, and the imaginary part of <m>\lambda</m> determines constant frequency of oscillation.
			</p>
		</fact>
		<p>
			Both purely real exponentials and purely harmonic sines and cosines are just special cases of the complex exponential! 
		</p>
	</subsection>
	<subsection xml:id="fs-cx-real">
		<title>Converting to real solutions</title>
		<p>
			An applied mathematician is perfectly happy with a solution using complex numbers, even if it represents something that is actually real. But an engineer might get fired for not explaining things in real form to the sales team. (And let's be honest, for not using very small words.)
		</p>		
		<p>
			Let's consider the <m>2\times 2</m> case for simplicity. The general solution is 
			<me>
				\bx(t) = \bX(t)\bc = c_1 e^{\lambda_1 t} \bv_1 + c_2 e^{\lambda_2 t} \bv_2
			</me>. 
			If <m>\bA</m> is real, then <m>\lambda_2=\overline{\lambda_1}</m> and <m>\bv_2=\overline{\bv_1}</m>. If the initial conditions are also real, then it works out that <m>c_2=\overline{c_1}</m> as well, and the entire solution can be expressed by expanding each item into real and imaginary parts. This is, as we say in the biz, not fun. 
		</p>
		<p>
			The pain is lessened if we use a different form of the general solution by writing out the real and imaginary parts of <m>e^{\lambda_1 t} \bv_1</m>. If <m>\lambda_1=a+i\omega</m>, then write 
			<me>
				e^{\lambda_1 t} \bv_1 = e^{at} \bigl[\cos(\omega t)+i\sin(\omega t) \bigr] \bv_1 =  e^{at} \bigl[ u(t) + i w(t) \bigr]
			</me>. 
			Then an alternative expression for the general solution is <m>\bx(t)=b_1 e^{at} u(t) + b_2 e^{at} w(t)</m>, where everything in that formula is now real-valued. 
		</p>
		<example>
			<p>
				We earlier found the eigenvalues and eigenvectors of
				<me>
					\bA = \begin{bmatrix} 1 \amp -1 \\ 5 \amp -3 \end{bmatrix}
				</me>
				to be  
				<me>
					\lambda_{1,2} = -1 \pm 1i, \qquad \bv_{1,2} = \twovec{1}{2\mp i}
				</me>.
				This gives a general solution in the complex form 
				<me>
					\bx(t) = c_1 e^{(-1+i)t} \twovec{1}{2- i} + c_2 e^{(-1-i)t} \twovec{1}{2+i}
				</me>. 
				It's implied that <m>c_{1,2}</m> are complex constants. 
			</p>
			<p>
				To find an alternative real form, we write 
				<md>
					<mrow>e^{(-1+i)t} \twovec{1}{2-i} \amp = e^{-t} [\cos(t)+i\sin(t)] \left( \twovec{1}{2} + i\twovec{0}{-1} \right)</mrow>
					<mrow>\amp = e^{-t} \left( \cos(t)\twovec{1}{2} - \sin(t) \twovec{0}{-1} \right)</mrow>
					<mrow> \amp \qquad + i \left( \cos(t)\twovec{0}{-1} + \sin(t) \twovec{1}{2} \right) </mrow>
				</md>. 
				Thus the general solution can also be written as 
				<me>
					\bx(t) = b_1 e^{-t} \twovec{\cos(t)}{2\cos(t)+\sin(t)} + b_2 e^{-t}\twovec{\sin(t)}{2\sin(t)-\cos(t)} 
				</me>,
				where here <m>b_1</m> and <m>b_2</m> are real. 
			</p>
		</example>
	</subsection>
</section>

<section xml:id="fs-phaseplane">
	<title>Stability in the phase plane</title>
	<introduction>
		<p>
			The homogeneous system <m>\bx'=\bA\bx</m> for a constant matrix <m>\bA</m> clearly has the constant function <m>\bx(t)\equiv \bzero</m> as a solution. We call this a <idx>steady-state</idx><term>steady-state</term> or <idx>equilibrium</idx><term>equilibrium</term> solution. Not all equilibria are the same, however. 
		</p>
		<p>
			Imagine holding a broom by two fingers, with the bristles pointing straight down at the floor. You will not have much difficulty keeping the broom in approximately this position for a long time, even if an annoying friend comes by and jostles you a bit. Now imagine holding it inverted, with the bristles pointing toward the ceiling. If you were able to <em>perfectly</em> balance the center of gravity of a symmetric broom above your fingers, then in principle it ought to stay there. But the broom is not perfect, and your arm and fingers can't stay perfectly still. You might keep the broom upright for a few seconds, but it will soon crash to the floor. 
		</p>
		<p>
			Both the downward and upward positions of the broom are equilibria. The property that differentiates them is <idx>stability</idx><term>stability</term>. Stability refers to the response of the system to a perturbation (small change) of its state away from an equilibrium. If the system stays very (arbitrarily) close to the equilibrium after perturbation, we say the equilibrium is <idx>stable</idx><term>stable</term>. (What happens near equilibrium, stays near equilibrium.) Otherwise it is <idx>unstable</idx><term>unstable</term>. 
		</p>
		<p>
			Stability of the origin depends on the eigenvalues of <m>\bA</m>. This is most easily summarized in the special case of a <m>2\times 2</m> real matrix with eigenvalues <m>\lambda_1,\lambda_2</m>. The major cases to be considered are summarized in <xref ref="fs-tb-stability"/>, leaving out some subtle <q>edge cases</q> when things are zero or exactly equal. The other column of the table is to be explained next.
		</p>
		<table xml:id="fs-tb-stability">
			<title>Eigenvalues and stability</title>
			<tabular>
			<row>
				<cell><term>Eigenvalues</term></cell>
				<cell><term>Stability</term></cell>
				<cell><term>Type</term></cell>
			</row>
			<row>
				<cell><m>\lambda_1 \lt \lambda_2 \lt 0</m></cell>
				<cell>stable</cell>
				<cell>node</cell>
			</row>
			<row>
				<cell><m>\lambda_1 \lt 0 \lt \lambda_2 </m></cell>
				<cell>unstable</cell>
				<cell>saddle</cell>
			</row>
			<row>
				<cell><m>0 \lt \lambda_1 \lt \lambda_2 </m></cell>
				<cell>unstable</cell>
				<cell>node</cell>
			</row>
			<row>
				<cell><m>\lambda = a\pm i b,\: a\lt 0</m></cell>
				<cell>stable</cell>
				<cell>spiral</cell>
			</row>
			<row>
				<cell><m>\lambda = \pm i b</m></cell>
				<cell>stable</cell>
				<cell>center</cell>
			</row>
			<row>
				<cell><m>\lambda = a\pm i b,\: a\gt 0</m></cell>
				<cell>unstable</cell>
				<cell>spiral</cell>
			</row>
			</tabular>
		</table>		
	</introduction>
	<subsection xml:id="fs-sec-phase-plane">
		<title>Phase plane</title>
		<introduction>
			<p>
				In the two-dimensional case, the solutions of <m>\bx'=\bA\bx</m> can be plotted as trajectories parameterized by time in the <m>(x_1,x_2)</m> plane, often called the <idx>phase plane</idx><term>phase plane</term>. Each of the cases in <xref ref="fs-tb-stability"/> has a phase plane portrait that shares certain general characteristics that clarify the major aspects of possible solution behaviors.  
			</p>
		</introduction>
		<subsubsection xml:id="fs-pp-node">
			<title>Node</title>
			<p>
				When the eigenvalues are real and have the same sign, the fixed point is called a <idx>node</idx><term>node</term>. If the sign is negative, all trajectories head into the fixed point and it is stable; conversely, in the positive case it is unstable. 
			</p>
			<example>
				<p>
					Here is a phase plane plot for <m>\bA=\twomat{-4}{-1}{-2}{-5}</m>, which has eigenvalues <m>-3</m> and <m>-6</m>. 
				</p>
				<sidebyside>
					<listing xml:id="fs_pp_nodestable">
						<caption>A stable node.</caption>
						<program language="matlab">
						<input>
							A = [-4 -1; -2 -5];
							[V,D] = eig(A);
							lam = diag(D);
							
							clf
							plot(0,0,'r.','markersize',18)
							hold on
							
							% plot directions
							[x1,x2] = meshgrid(-1:.2:1);
							v = A*[x1(:)';x2(:)'];
							sz = size(x1);
							quiver(x1,x2,reshape(v(1,:),sz),reshape(v(2,:),sz))
							
							% plot eigenvectors
							plot(5*[-V(1,:);V(1,:)],5*[-V(2,:);V(2,:)],'k','linew',2)
							
							% plot trajectories
							colr = get(gca,'colororder');
							t = linspace(0,10,300);
							for theta = 2*pi*(0:23)/24
								for j = 1:length(t)
									X = expm(t(j)*A);
									x(:,j) = X*[ cos(theta); sin(theta) ];
								end
								plot(x(1,:),x(2,:),'linew',1.5,'color',colr(1,:))
							end
							
							axis([-1.1 1.1 -1.1 1.1]), axis square
							title('stable node')
							xlabel('x_1')
							ylabel('x_2')		
						</input>
						</program>
					</listing>
					<image source="figures/fs_pp_nodestable.svg"/>
				</sidebyside>
				<p>
					The black lines show the directions of the eigenvectors. An initial state on one of those lines stays on it forever. Other initial conditions follow a path tending to become parallel to eigenvector <m>\bv_1</m>, since the other component decays out more quickly. The plot for <m>-\bA</m> would just reverse all of the arrows and show an unstable fixed point. 
				</p>
		</example>
		</subsubsection>
		<subsubsection xml:id="fs-pp-saddle">
			<title>Saddle</title>
			<p>
				When the eigenvalues are real and of different signs, the fixed point is called a <idx>saddle</idx><term>saddle</term>. A saddle point is always unstable. The part of the solution along the negative eigenvector decays away, but the contribution from the positive eigenvector grows with time. 
			</p>
			<example>
				<p>
					Here is a phase plane plot for <m>\bA=\twomat{-2}{-3}{-6}{-5}</m>, which has eigenvalues <m>1</m> and <m>-8</m>. 
				</p>
				<sidebyside>
					<listing  xml:id="fs_pp_saddle">
						<caption>A saddle point.</caption>
						<program language="matlab">
						<input>
								A = [-2 -3; -6 -5];
								[V,D] = eig(A);
								lam = diag(D);
								
								clf
								plot(0,0,'r.','markersize',18)
								hold on
								
								% plot directions
								[x1,x2] = meshgrid(-1:.2:1);
								v = A*[x1(:)';x2(:)'];
								sz = size(x1);
								quiver(x1,x2,reshape(v(1,:),sz),reshape(v(2,:),sz))
								
								% plot eigenvectors
								plot(5*[-V(1,:);V(1,:)],5*[-V(2,:);V(2,:)],'k','linew',2)
								
								% plot trajectories
								colr = get(gca,'colororder');
								t = linspace(0,10,300);
								for theta = 2*pi*(0:23)/24
									for j = 1:length(t)
										X = expm(t(j)*A);
										x(:,j) = X*[ cos(theta); sin(theta) ];
									end
									plot(x(1,:),x(2,:),'linew',1.5,'color',colr(1,:))
								end
								
								axis([-1.1 1.1 -1.1 1.1]), axis square
								title('saddle')
								xlabel('x_1')
								ylabel('x_2')					
							</input>
						</program>
					</listing>
					<image source="figures/fs_pp_saddle.svg"/>
				</sidebyside>
				<p>
					An initial condition exactly on the stable black line (eigenvector) will approach the origin, but anything else ends up shooting away more or less in the direction of the unstable eigenvector.  
				</p>
			</example>
		</subsubsection>
		<subsubsection xml:id="fs-pp-spiral">
			<title>Spiral</title>
			<p>
				When the eigenvalues are complex conjugates, the fixed point is called a <idx>spiral</idx><term>spiral</term>. If the eigenvalues are <m>a \pm i b</m>, then all solutions contain <m>e^{at}e^{\pm i b t}</m>, or equivalently, <m>e^{at}\cos{b t}</m> and <m>e^{at}\sin{b t}</m>. The real part causes growth and instability if <m>a\gt 0</m>, or decay and stability if <m>a \lt 0</m>. The imaginary part determines the angular speed of the spiral. 
			</p>
			<example>
				<p>
				Here is a phase plane plot for <m>\bA=\twomat{1}{-2}{4}{-3}</m>, which has eigenvalues <m>-1\pm 2i</m>. 
				</p>
				<sidebyside>
					<listing  xml:id="fs_pp_spiral">
						<caption>A spiral point.</caption>
						<program language="matlab">
						<input>
								A = [1 -2; 4 -3];
								[V,D] = eig(A);
								lam = diag(D);
								
								clf
								plot(0,0,'r.','markersize',18)
								hold on
								
								% plot directions
								[x1,x2] = meshgrid(-1:.2:1);
								v = A*[x1(:)';x2(:)'];
								sz = size(x1);
								quiver(x1,x2,reshape(v(1,:),sz),reshape(v(2,:),sz))
								
								% plot trajectories
								colr = get(gca,'colororder');
								t = linspace(0,7,300);
								for theta = 2*pi*(0:6)/7
									for j = 1:length(t)
										X = expm(t(j)*A);
										c = exp(lam(1)*t(j)-1i*theta);
										x(:,j) = X*[ cos(theta); sin(theta) ];
									end
									plot(x(1,:),x(2,:),'linew',1.5,'color',colr(1,:))
								end
								
								axis([-1.2 1.2 -1.2 1.2]), axis square
								title('stable spiral')
								xlabel('x_1')
								ylabel('x_2')
							</input>					
						</program>
					</listing>
					<image source="figures/fs_pp_spiral.svg"/>
				</sidebyside>
				<p>
					The eigenvectors are complex and don't show up on the plot; there are no purely linear trajectories.   
				</p>
			</example>
		</subsubsection>
		<subsubsection xml:id="fs-sec-center">
			<title>Center</title>
			<p>
				The special case of imaginary eigenvalues, <m>\lambda=\pm i b</m>, is called a <idx>center</idx><term>center</term>. You can think of the trajectories as spirals that is perfectly neutral--that is, circles or ellipses. These equilibria are often associated with conservation principles, such as conservation of energy. 
			</p>
		</subsubsection>
	</subsection>
</section>

<section xml:id="fs-matrix-exp">
	<title>Matrix exponential</title>
	<introduction>
		<p>
			One solution of <m>x'=ax</m> for constant <m>a</m> is <m>x=e^{at}</m>. We know the Taylor series 
			<me>
				e^{at} = 1 + at + \frac{1}{2!}(at)^2 + \frac{1}{3!} (at)^3 + \cdots
			</me>. 
			This series is something we could generalize to a square matrix <m>\bA</m>, for which integer powers are possible:<idx>matrix exponential</idx>
			<men xml:id="fs-eq-matrixexp">
				e^{t\bA} = \bI + t\bA + \frac{1}{2!}t^2 \bA^2 + \frac{1}{3!} t^3 \bA^3 + \cdots
			</men>. 
			Let's not worry too much about whether this converges (it does). What are its properties? 
		</p>
		<theorem xml:id="fs-thm-matrixexp">
			<title>Matrix exponential</title>
			<statement><p>
				Let <m>\bA,\bB</m> be <m>n\times n</m> matrices. Then 
				<ol>
					<li><m>e^{t\bA}=\bI</m> if <m>t=0</m>,</li>
					<li><m>\dd{}{t}e^{t\bA} = \bA e^{t\bA} = e^{t\bA}\bA</m>,</li>
					<li><m>[e^{t\bA}]^{-1} = e^{-t\bA}</m>, and</li>
					<li>If <m>\bA\bB=\bB\bA</m>, then <m>e^{\bA+\bB} = e^{t\bA}e^{t\bB} = e^{t\bB}e^{t\bA}</m>.</li>
				</ol>
			</p></statement>
		</theorem>
		<p>
			These conclusions follow from the series definition <xref ref="fs-eq-matrixexp"/>. They establish some important facts.  
		</p>
		<fact>
			<p>
				<ol>
					<li>For any constant vector <m>\bc</m>, <m>\bx(t)=e^{t\bA}\bc</m> is a solution of <m>\bx'=\bA\bx</m>.</li>
					<li><m>\bX(t)=e^{t\bA}</m> is the unique fundamental matrix for <m>\bx'=\bA\bx</m> with <m>\bX(0)=\bI</m>.</li>
				</ol>
			</p>
		</fact>
		<p>
			Summing an infinite series of matrices may not seem like a great prospect. But we have two ways to keep it fairly tame. Which to use depends on an important property of the matrix. 
		</p>
		<p>
			In section <xref ref="la-ei-multiplicity"/> we saw the definition of a defective eigenvalue (<xref ref="la-df-defective"/>). A matrix with one or more defective eigenvalues is also called <idx>defective</idx>.
		</p>
		<fact xml:id="ls-fact-defective">
			<statement>
				<p>
					A defective <m>n\times n</m> matrix has fewer than <m>n</m> independent eigenvectors--that is, the sum of the eigenspace dimensions is less than <m>n</m>.
				</p>
			</statement>
		</fact>
		<p>
		  This is especially easy to spot when <m>n=2</m>:
		</p>
		<fact>
			<p>A <m>2\times 2</m> matrix with repeated eigenvalue is either a multiple of the identity or defective.</p>
		</fact>
	</introduction>
	<subsection xml:id="fs-me-diagonal">
		<title>Nondefective matrix</title>
		<p>
			For a nondefective <m>\bA</m>, <xref ref="fs-thm-eigensolution"/> guarantees that 
			<me>
				\bX(t) = \begin{bmatrix} e^{t\lambda_1}\bv_1 \amp e^{t\lambda_2}\bv_2 \amp \cdots \amp e^{t\lambda_n}\bv_n \end{bmatrix}  
			</me>
			is a fundamental matrix for the ODE <m>\bx'=\bA\bx</m>, where the <m>(\lambda_j,\bv_j)</m> are eigenpairs. If we now define 
			<me>
				\bY(t) = \bX(t) \bX(0)^{-1}
			</me>, 
			then 
			<me>
				\bY'(t) = \bX'(t) \bX(0)^{-1} = \bA \bX(t) \bX(0)^{-1} = \bA \bY(t)
			</me>,
			so <m>\bY(t)</m> is a fundamental matrix as well. But also <m>\bY(0)=\bX(0)\bX(0)^{-1}=\bI</m>, so in fact <m>\bY(t)</m> is the matrix exponential. 
		</p>
		<p>
			When this argument is fully unraveled, it can be summarized as follows. Given a complete set of eigenvectors, i.e.,
			<me>
				\bA \bv_1 = \lambda_1 \bv_1, \;
				\bA \bv_2 = \lambda_2 \bv_2,\; \dots, \;
				\bA \bv_n = \lambda_n \bv_n
			</me>,
			define 
			<me>
				\bV = \begin{bmatrix} \bv_1 \amp \bv_2 \amp \cdots \amp \bv_n \end{bmatrix}.
			</me>
			(In the notation above, this is <m>\bX(0)</m>.) Then 
			<men xml:id="fs-matrixexp">
				e^{t\bA} = \bV \begin{bmatrix} 
				e^{t\lambda_1} \amp \amp \amp \\ 
				\amp e^{t\lambda_2} \amp \amp \\ 
				\amp \amp \ddots \amp \\ 
				\amp \amp \amp e^{t\lambda_n} 
				\end{bmatrix}  \bV^{-1}
			</men>.
			This is one of those cases in which you're probably better off working from the formula than going each time through the process we used to get it. 
		</p>
		<example>
			<p>
			Let's find the exponential of 
			<me>
				\bA = \twomat{1}{1}{4}{1}
			</me>,
			given <m>\lambda_1=3</m>, <m>\bv_1 = \twovec{1}{2}</m>, and <m>\lambda_2=-1</m>, <m>\bv_2=\twovec{1}{-2}</m>. We identify  
			<me>
				\bV = \twomat{1}{1}{2}{-2}
			</me>
			and compute 
			<md>
				<mrow> e^{t\bA} \amp = \twomat{1}{1}{2}{-2} \twomat{e^{3t}}{0}{0}{e^{-t}} \twomat{1}{1}{2}{-2}^{-1} </mrow>
				<mrow>  \amp = -\frac{1}{4} \twomat{1}{1}{2}{-2} \twomat{e^{3t}}{0}{0}{e^{-t}} \twomat{-2}{-1}{-2}{1} </mrow>
				<mrow>  \amp = -\frac{1}{4} \twomat{e^{3t}}{e^{-t}}{2e^{3t}}{-2e^{-t}} \twomat{-2}{-1}{-2}{1} </mrow>
				<mrow>  \amp = \frac{1}{4} \twomat{2e^{3t}+2e^{-t}}{e^{3t}-e^{-t}}{4e^{3t}-4e^{-t}}{2e^{3t}+2e^{-t}} </mrow>
			</md>. 
			Not exactly a walk in the park, but doable. 
		</p>
		</example>
		<example xml:id="fs-ex-expm1">
			<p>
				Suppose 
				<me>
					\bA = \twomat{1}{-1}{5}{-3}
				</me>, 
				and we are given or work out that 
				<me>
					\lambda_1 = -1+i, \; \lambda_2 = -1-i, \qquad \bV = \twomat{1}{1}{2-i}{2+i}
				</me>.
				Then 
				<me>
					\bV^{-1} = \frac{1}{2+i-2+i} \twomat{2+i}{-1}{-2+i}{1} = \frac{i}{2} \twomat{-2-i}{1}{2-i}{-1}
				</me>,
				and then 
				<md>
					<mrow> e^{t \bA} \amp = e^{-t}\frac{i}{2}\twomat{1}{1}{2-i}{2+i} \twomat{e^{it}}{0}{0}{e^{-it}} \twomat{-2-i}{1}{2-i}{-1} </mrow>
					<mrow>  \amp = e^{-t}\frac{i}{2}\twomat{e^{it}}{e^{-it}}{(2-i)e^{it}}{(2+i)e^{-it}} \twomat{-2-i}{1}{2-i}{-1} </mrow>
					<mrow>  \amp = e^{-t}\frac{i}{2} \twomat{-(2+i)e^{it}+(2-i) e^{-it}}{e^{it}-e^{-it}}{-(2-i)(2+i)e^{it}+(2+i)(2-i) e^{-it}}{(2-i)e^{it}-(2+i) e^{-it}} </mrow>
				</md>. 
				Each entry is written in the form of a complex number minus its conjugate, which gives <m>2i</m> times the imaginary part. This helps a lot.
				<me>
					e^{t \bA} = -e^{-t} \twomat{-\cos(t)-2\sin(t)}{\sin(t)}{-5\sin(t)}{-\cos(t)+2\sin(t)}
				</me>.
			</p>
		</example>
		<p>
			Because the matrix exponential is a fundamental matrix, the general solution to <m>\bx'=\bA\bx</m> can always be written in the form <m>e^{t\bA}\bc</m>. This isn't the easiest way to get a general solution, because the <m>\bX(t)</m> above requires less work. But this form is handy with an IVP, because 
			<me>
				\bx_0 = \bx(0) = e^{0\bA}\bc = \bc
			</me>. 
			That is, <m>e^{t\bA}\bx_0</m> solves the IVP.  
		</p>
		<example>
			<p>
				Using the matrix from the previous example, we show how the numerical solution of <m>\bx'=\bA\bx</m> (which doesn't know what kind of ODE it solves) is the same as the one computed via <m>e^{t\bA}\bx_0</m>.
			</p>
			<sidebyside>
				<listing xml:id="fs_expm">
					<caption>Solution by the matrix exponential</caption>
					<program language="matlab">
						<input>
A = [1 -2; 5 -3];
x0 = [-1;2];
t = linspace(0,8,400);
% Numerical solution: curves
f = @(t,x) A*x;
[t,xnum] = ode45(f,t,x0);
plot(t,xnum)
% Solution by matrix exponential: dots
xexp = zeros(length(t),2);
for j = 1:length(t)
	xexp(j,:) = expm(A*t(j))*x0;
end
hold on, plot(t,xexp,'k.')
xlabel('t'), ylabel('x_1(t), x_2(t)')
						</input>
					</program>
				</listing>
				<image source="figures/fs_expm.svg"/>
			</sidebyside>
		</example>
	</subsection>
	<subsection xml:id="fs-me-defective">
		<title>Defective matrix</title>	
		<p>
			In the 2-by-2 case, a defective matrix has a double eigenvalue but is not diagonal. In this case we have no clear way to construct the fundamental <m>\bX(t)</m> that gave us the matrix exponential above. Fortunately, we have another reasonable path for the <m>2\times 2</m> situation. 
		</p>
		<example>
			<p>
				The matrix 
				<me>
					\bA = \twomat{-1}{1}{0}{-1}
				</me>
				has a double eigenvalue of <m>\lambda=-1</m> and is defective. Now consider 
				<me>
					\bB = \bA - \lambda \bI = \twomat{0}{1}{0}{0}
				</me>. 
				This matrix has the property that <m>\bB^2=\bzero</m>. That means the exponential series for it is just 
				<me>
					e^{t\bB} = \bI + t\bB = \twomat{1}{t}{0}{1}
				</me>. 
				Finally, we can use <xref ref="fs-thm-matrixexp"/> to calculate
				<me> 
					e^{t\bA} = e^{t\lambda\bI}e^{t(\bA-\lambda\bI)} = e^{-t}\twomat{1}{t}{0}{1}
				</me>. 
			</p>
		</example>
		<p>
			Matters are less simple for larger matrices, but we won't be going there. 
		</p>
	</subsection>
</section>

<section xml:id="fs-varparam">
	<title>Variation of parameters</title>
	<p>
		We return to the forced problem <m>\bx'=\bA\bx+\bff(t)</m>. We can use the same technique of variation of parameters as in <xref ref="fl-varparam"/> and get very similar formulas. 
	</p>
	<theorem xml:id="fs-thm-varparam">
		<title>Variation of parameters</title>
		<statement><p>
			Let <m>\bX(t)</m> be a fundamental matrix of the homogeneous problem <m>\bx'=\bA(t)\bx</m>. Then the general solution of <m>\bx'=\bA(t)\bx+\bff(t)</m> is 
			<men xml:id="fs-eq-varparam1">
				\bx(t) = \bx_h(t) + \bx_p(t) = \bX(t)\bc + \bX(t) \int \bX(t)^{-1} \bff(t) \, dt
			</men>. 
			The solution with initial condition <m>\bx(0)=\bx_0</m> is 
			<men xml:id="fs-eq-varparam2">
				\bx(t) = \bX(t)\bX(0)^{-1}\bx_0 + \bX(t) \int_0^t \bX(s)^{-1}\bff(s) \, ds
			</men>.	
			In the case of a constant matrix <m>\bA</m>, 
			<men xml:id="fs-eq-varparam3">
				\bx(t) = e^{t\bA}\bx_0 + \int_0^t e^{(t-s)\bA} \bff(s) \, ds
			</men>.
		</p></statement>
	</theorem>
	<p>
		Given how tedious it is to calculate even a small fundamental matrix by hand, it's no mean feat to apply these formulas to a specific example. 
	</p>
	<example>
		<p>
			In <xref ref="fs-ex-expm1"/> we worked out a matrix exponential:
			<me>
				\bA = \twomat{1}{-1}{5}{-3}
				\Rightarrow 
				e^{t \bA} = -e^{-t} \twomat{-\cos(t)-2\sin(t)}{\sin(t)}{-5\sin(t)}{-\cos(t)+2\sin(t)}
			</me>
			If we want to solve 
			<me>
				\bx' = \bA\bx + \twovec{1}{t}, \quad \bx(0)=\bzero,
			</me>
			then <xref ref="fs-eq-varparam3"/> gives 
			<md>
			  <mrow> \bx(t) \amp = \int_0^t -e^{s-t} \twomat{-\cos(t-s)-2\sin(t-s)}{\sin(t-s)}{-5\sin(t-s)}{-\cos(t-s)+2\sin(t-s)} \twovec{1}{s} \, ds</mrow>
			  <mrow>  \amp = \int_0^t -e^{s-t} \twovec{-\cos(t-s)+(s-2)\sin(t-s)}{-s\cos(t-s)+(2s-5)\sin(t-s)} \, ds.   </mrow>
			</md>
			Some computer algebra would be advisable at this point.	
		</p>	
	</example>
	<p>
		The formulas of the theorem are useful mainly as a theoretical tool, a starting point for further derivations, and in specific cases where they can be greatly simplified. But <xref ref="fs-eq-varparam3"/> does have a reasonable interpretation. The term <m>e^{t\bA}\bx_0</m> is the homogeneous (unforced) response to the initial condition. The integrand <m>e^{(t-s)\bA} \bff(s)</m> represents the ODE evolution of an initial <m>\bff(s)</m> through time <m>t-s</m>; that is, the forcing at time <m>s</m> contributes an impulse of amplitude <m>\bff(s)</m>. Finally, the integral just adds up all of those impulse contributions from prior times. 
	</p>
</section>

</chapter>