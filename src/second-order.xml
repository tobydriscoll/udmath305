<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="second-order-ode" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Second-order ODEs</title>
 
	<section xml:id="so-simple-harmonic">
	<title>Simple harmonic motion</title>

	<subsection xml:id="so-sh-newton">
	  <title>Newton's law</title>
	<p>The subtext of this chapter is, "F=ma is not just a good idea; it's the law." Newton's second law, to be exact. Since acceleration is the second derivative of position, this leads us to second-order ODEs of the form <m>y''=F(t,y,y')</m>. You can always think about this as a problem specifying a position-dependent force, though it's much more general than that.</p>

	<p>Suppose you ask a question such as, "Where will my car be after 4 seconds of maximum acceleration?" You can't fully answer it unless you know two more pieces of data: the position and velocity of the car at the start. In general, we need to know <m>y(0)</m> and <m>y'(0)</m> (or values at some other time) in order to solve a second-order problem uniquely.</p>

	<p>The grandpappy of all second-order ODEs is <m>y''=-ay</m>, for a positive constant <m>a</m>. In terms of Newton's second law, this arises with a force term defined as <m>F=-ky</m>, because then <m>my''=-ky</m>. Most springs behave like this to a good approximation, provided you don't stretch or compress them too far (an idea called Hooke's Law). Playground swings, molecular bonds, and electrical inductors are often well-approximated this way as well. </p>

	<p>The negative sign in the force is crucial. It makes the force a "restoring" one: move to the right, and you are pushed to the left, and vice versa. Soon we will let additional external forces act, in which case the ODE becomes <m>my''+ky=f(t)</m>.</p>
	</subsection>

	<subsection xml:id="so-sh-null">
	  <title>Unforced solution</title>
	<p>Solutions of <m>my''+ky=0</m> are quite easy to get. Define <m>\omega_n=\sqrt{k/m}</m>. Then you can easily check that <m>\cos(\omega_n t)</m> is a solution. So are <m>-\cos(\omega_n t)</m> and <m>7164\cos(\omega_n t)</m>, for that matter; any constant multiplier is allowed. You can also check that <m>A\sin(\omega_n t)</m> is a solution for any constant multiplier. Finally, we can combine them as well, into
	<me>y = c_1 \cos(\omega_n t) + c_2 \sin(\omega_n t).</me>
	It turns out that this expression captures all possible solutions. We have <term>simple harmonic motion</term>.	</p>

	<p>We started with a second derivative in the ODE and now we have two arbitrary "integration constants." That's no coincidence. It's also not a coincidence that we need two initial values to find a unique solution. For the record, we can easily find that
	<me>y(t) = y(0) \cos(\omega_n t) + \frac{y'(0)}{\omega_n} \sin(\omega_n t).</me></p>

	<p>Recall from <xref ref="frc-shifted-cos"/> that a combination of sin and cos at the same frequency is equivalent to a shifted cosine, so we can also write <m>y=R\cos(\omega_n t - \alpha)</m> for an amplitude <m>R</m> and a phase <m>\alpha</m>, and we get those values by expressing the cartesian point <m>(y(0),y'(0)/\omega_n)</m> in polar form. </p>

	<exercise>
	  <statement><p>Find the solution in shifted cosine form of <m>2y''+32y=0</m> with <m>y(0)=1</m>, <m>y'(0)=-8</m>.</p>
	  </statement>
	  <solution>
	    <p>We have <m>\omega_n=\sqrt{16}=4</m>, so <m>y = c_1 \cos(4 t) + c_2 \sin(4 t).</m> We then derive <m>1=y(0)=c_1</m> and <m>-8=y'(0)=4c_2</m>, so <m>y = \cos(4 t) -2 \sin(4 t).</m> Converting <m>(1,-2)</m> to polar form gives <m>R=\sqrt{5}</m> and <m>\alpha = \tan^{-1}(-2)</m>. </p>
	  </solution>
	</exercise>
	    

	<p>A quick word about words. In math we typically use the term frequency to mean an <term>angular frequency</term>, as with the value <m>\omega_n</m> that multiplies <m>t</m> inside the trig functions. You can think of this value as meaning "radians per second." In engineering and physics one might be more comfortable using "cycles per second" (or Hertz), which leads to a different value of frequency, <m>\omega_n/2\pi</m>. We won't be using that, just to avoid confusion, but now you're aware that corresponding formulas in other sources might have extra factors of <m>2\pi</m> sprinkled about.</p>
	
	</subsection>

	<subsection>
	  <title>Forced response</title>
	  <p>Let's consider the forced problem now, <m>my''+ky=f(t)</m>. The most natural type of forcing is also harmonic, but at a different frequency <m>\omega</m>. This is a linear problem, and just as in the first-order case, we get a solution consisting of the unforced part from above plus a particular solution <m>y_p</m> which we often call the <term>response</term> to the forcing term.</p>

	  <p>The math remains incredibly simple for now. Suppose <m>f(t)=\cos(\omega t)</m>, and let <m>y_p=Y\cos(\omega t)</m>. (Why? Because it works.) Plug <m>y_p</m> into the equation and you find that it's valid if <m>Y=1/(k-m\omega^2)</m>. Let's write the complete solution using <m>k=m\omega_n^2</m>:
	  <men xml:id="eq-so-sh-response">y(t) = R\cos(\omega_n t-\alpha) + \frac{1}{m(\omega_n^2-\omega^2)}\cos(\omega t).</men>
	  It's clear that this formula breaks when <m>\omega=\omega_n</m>, which is the case of <term>resonance</term>. We'll worry about that later on. Notice that otherwise the solution consists of one part at the natural frequency and another at the forcing (or driving) frequency. This is the "null plus particular" structure we have seen all along for linear equations. </p>

	  <exercise>
	    <statement>
	      <p>Find a particular solution of <m>y''+9y = \sin(t)-\cos(t).</m></p>
	    </statement>
	    <solution>
	      <p>It's not hard to see that the sin forcing term can be solved by <m>y_p=Y\sin(t)</m> for a constant <m>Y</m>. We can put both forcing terms together with <m>y_p=A\cos(t)+B\sin(t)</m> and insert it into the equation to find
	      <me>(9-A)\cos(t)+(9-B)\sin(t) = \sin(t)-\cos(t),</me>
	      which is identically true if and only if <m>9-A=-1</m> and <m>9-B=1</m>. So
	      <me>y_p=10\cos(t)+8\sin(t)</me>.</p>
	    </solution>
	  </exercise>
	  
	  <p>The analogy goes even deeper. Recall in <xref ref="flcs-impulse"/> that the null solution was relatable to the impulse response, i.e., the particular solution when <m>f(t)=\delta(t)</m>. That happens here, too. In fact, the solution of <m>my''+ky=\delta(t)</m> with zero initial conditions is
	  <men xml:id="eq-so-sh-fundsol">g(t) = \frac{1}{m\omega_n} \sin(\omega_n t).</men>
	  This solution has <m>g(0)=0</m>, <m>g'(0)=1/m</m>, and solves <m>my''+ky=0</m> for <m>t\gt 0</m>.</p>

	  <p>(Sort-of justification: Since <m>g(0)=0</m>, we have <m>mg''=\delta(t)</m> there. Integrating once tells us that <m>mg'</m> is a step function, so <m>g'=1/m</m> immediately at time zero. Kinda.)</p>

	  <p>The kicker is that the impulse response can be used to solve <em>any</em> forcing problem, at least in principle. Given the forcing term <m>f(t)</m>, a particular solution is
	  <me>y_p(t) = \int_0^t g(t-s)f(s)\,ds.</me>
	  Again, in a linear problem we just need to add up the contributions of impulses at every moment. Because of this formula, the impulse response <m>g</m> is also called a <term>fundamental solution</term>. We'll be using and coming at this formula a few different ways throughout the chapter. </p>
	</subsection>

	</section>

  <section xml:id="so-complex-exponentials">
	<title>Complex exponentials</title>
	<p>For second-order equations the complex plane becomes indispensible. Recall again the polar form of a complex number,
	<me>re^{i\theta} = (r\cos \theta)+i(r\sin \theta)</me>.
	This gives us a way of looking at the exponential of any complex number as
	<me>e^{x+iy} = e^x e^{iy} = (e^x\cos y)+i(e^x\sin y)</me>.
	More importantly, we can look at exponential functions of time more generally as
	<men xml:id="eq-so-exp-exp">e^{(a+i \omega)t} =  e^{at} (\cos \omega t + i \sin \omega t)</men>.
	Reading from right to left, we have the product of <m>e^{i\omega t}</m>, which goes around and around the unit circle at constant speed, and <m>e^{at}</m>, which is an amplitude that exponentially grows or decays (or stays fixed if <m>a=0</m>).  </p>

	<p>The upshot is that in the function <m>e^{st}</m>, the real part of <m>s</m> determines exponential growth or decay, and the imaginary part of <m>s</m> determines frequency of oscillation. All real exponentials, and all harmonic sines and cosines, are just special cases.</p>

	<p>Let's look back a moment at <m>my''+ky=0</m>. If we now try an exponential solution of the form <m>y_n=e^{st}</m>, then we arrive at <m>(ms^2+k)e^{st}=0</m>, which we can make true if <m>s^2=-k/m</m>. this has two roots, <m>s=\pm i \omega_n</m>, and we are led to
	<me> y_n = c_1 e^{i\omega t} + c_2 e^{-i\omega t}</me>.
	A little rearrangement should convince you that this is equivalent to the form we got in the last section, just using complex numbers.</p>

	<p>Similarly, for the forced problem <m>my''+ky=e^{i\omega t}</m>, we can get a particular solution <m>y_p=Y e^{i\omega t}</m> if we let <m>Y=1/(k-m\omega^2)</m>. This works for both cosine and sine forcing upon taking real and imaginary parts, and we go right back to the formulas from before.</p>

  </section>

	<section xml:id="so-constant-coefficients">
	<title>Constant coefficients</title>
		<introduction>
	  <p>We're now set to tackle the linear, 2nd order ODE <m>a \frac{d^2y}{dt^2}+b\frac{dy}{dt}+cy=0</m> for constants <m>a,b,c</m>. (This is the only second-order problem we will solve. In fact, there aren't many more second-order problems that even can be solved with relatively little effort.) In order to cut down on the options in our explanations, we'll assume these constants are all real, and that <m>a\gt 0</m>. </p>

	  <p>It's going to be easy to lose track of the most basic fact of this equation: <em>the solutions are exponentials</em>. (Well, with an asterisk.) In fact plugging in <m>y=e^{st}</m> takes us to <m>(as^2+bs+c)e^{st}=0</m>, which is obviously true if we can solve the <term>characteristic equation</term>
	<men xml:id="eq-so-char">as^2+bs+c = 0</men>.
	As you know, there are two (counting multiplicity) and only two solutions for <m>s</m> in this equation. Let's call them <m>s_1</m> and <m>s_2</m>. I won't insult you by writing out the quadratic formula here.</p>

	<p>With two roots for the characteristic equation, the general null solution is
	<me> y = c_1 e^{s_1t} + c_2 e^{s_2t}</me>.
	There are two integration constants here, just what we need to satisfy two initial conditions. Except for that pesky asterisk!</p>
	</introduction>

	<subsection xml:id="so-cc-double">
	  <title>Double root</title>
	  <p>Suppose <m>s_1=s_2</m> is a double root of <xref ref="eq-so-char"/>. This happens when <m>b^2=4ac</m>, and <m>s_1=-b/(2a)</m>. In that case our candidate for the general solution is inadequate, because we can add the two terms together. Having <m>c_1+c_2</m> is no more general than having a single integration constant. Somehow we have to come up with a genuinely different second solution. It turns out that the job is done by <m>te^{s_1t}</m>. The proof is just a bunch of algebra: plug it in and show that it works.</p>

	  <p>So in the double-root case our null solution is 
	  <me> y = c_1 e^{s_1t} + c_2 t e^{s_1t}</me>.
	  Honestly, this is the only odd thing that happens in this entire section. Everything else is just exploring the consequences of the solutions, and naming things that we find out.</p>
	</subsection>

	<subsection xml:id="so-cc-real">
	  <title>Distinct real roots</title>
	  <p>Our two roots of the characterstic equation <xref ref="eq-so-char"/> might be distinct real numbers. This happens when <m>b^2\gt 4ac</m>. There isn't a whole lot else to say. The null solution is, as already stated,
	  <me> y = c_1 e^{s_1t} + c_2 e^{s_2t}</me>.
	  </p>

	  <example>
	    <p>Consider <m>y''-y'-2y=0</m>. The characteristic equation <m>s^2-s-2=0</m> has roots <m>s_1=-1</m>, <m>s_2=2</m>. This gives the solution <m>y=c_1 e^{-t} + c_2 e^{2t}</m>.</p>
	  </example>

	  <!--p>Let's look at that quadratic formula after all:
	  <me> s_{1,2} = \frac{ -b \pm \sqrt{b^2-4ac} }{ 2a }</me>.
	  Suppose <m>b\lt 0</m>. Then at least the "+" root is positive, because <m>-b</m> is positive and then something is added to it. Now suppose <m>b\gt 0</m>. The "-" root is trivially negative. But the other "+" root is negative too, because <m>\sqrt{b^2-4ac}-->
	</subsection>

	<subsection xml:id="so-cc-complex">
	  <title>Complex roots</title>
	  <p>There is exactly one more possibility for our characteristic roots: they are complex numbers. This happens if <m>b^2\lt 4ac</m>. In this case the roots must be conjugates of one another: <m>s_1=\overline{s}_2</m>. We'll find it convenient to write them in cartesian form, as <m>r\pm i\omega</m>. Then our null solution is
	  <me> y = c_1 e^{s_1t} + c_2 e^{s_2t} = e^{rt}\left( c_1 e^{i\omega t} + c_2 e^{-i \omega t} \right)</me>.
	  This is a perfectly fine way to write the solution, but not the only way. If we use Euler's formula and rearrange, we get
	  <me> y = e^{rt}\left( C_1 \cos(\omega t) + C_2 \sin(\omega t) \right)</me>.
	  These are still exponential solutions, but in disguise. </p>
	</subsection>

	<subsection xml:id="so-cc-oscillators">
	  <title>Unforced oscillators</title>
	  <p>We're going to narrow things a bit here and require both <m>a\gt 0</m> and <m>c\gt 0</m>. Remember, this arises from Newton's law to describe a restorative force. </p>

	  <p>What shall we make of the <m>by'</m> term here? We didn't have it with simple harmonic motion. Let's look at that quadratic formula after all:
	  <me> s_{1,2} = \frac{ -b \pm \sqrt{b^2-4ac} }{ 2a }</me>.
	  Recall our three cases, depending on the sign of <m>b^2-4ac</m> (aka the discriminant). If this is negative, then the roots are complex with real part equal to <m>r=-b/2a</m>. The amplitude of the complex exponential solutions is <m>e^{rt}</m>. So these decay to zero if <m>b \gt 0</m> and grow unboundedly if <m>b\lt 0</m>.</p>

	  <p>In the case <m>b^2-4ac= 0</m>, we have the double root <m>s_1=-b/2a</m>, and we reach the same conclusions about the sign of <m>b</m> (exponential decay beats linear growth). Finally, if <m>b^2-4ac \gt 0</m>, it's not hard to conclude that the same things happen. Physically, the ODE term <m>by'</m> represents <term>damping</term>, but only if <m>b\gt 0</m>. Typically this represents loss of energy due to damping or thermodynamic effects. It's not impossible to have <m>b\lt 0</m>, meaning energy is being pumped into the system, but it is far more rare.</p>

	  <p>It's instructive to do a thought experiment in which we increase <m>b</m> from zero while everything else is fixed. First, at <m>b=0</m>, we have the idealized situation of an <term>undamped oscillator</term>. This is back to simple harmonic motion, with perfectly sinusoidal results. The characteristic roots in this case are imaginary, <m>s_{1,2}=\pm i \omega_n=\pm i \sqrt{c/a}</m>, corresponding to purely oscillating complex exponentials.</p>

	  <p>If we increase <m>b</m> slightly, the roots will behave continuously. They go into the complex plane as conjugates with negative real part <m>-b/2a</m>. At this point it's convenient to shift notation by renaming our constants:
	  <me>\frac{-b}{ 2a } \pm i \frac{ \sqrt{4ac-b^2} }{ 2a } = -p \pm i\omega_d</me>,
	  where <m>p=b/2a</m>, and <m>\omega_d^2 = \omega_n^2-p^2</m> defines a <term>damped frequency</term> that's a bit lower than the natural undamped frequency. The solutions oscillate at <m>\omega_d</m> and decay with exponential rate <m>-p</m>. This is called <term>underdamped oscillation</term>. </p>

	  <p>As <m>b</m> continues to increase, eventually our complex roots coalesce into the double real root <m>-b/2a</m>. Even though the mathematical expression for the solution changes here, the behavior is continuous; we continue to see exponential decay, but the oscillatory component has gone from very slow to nonexistent. This is called <term>critical damping</term>.</p>

	  <p>Finally, as <m>b</m> increases more our roots again separate but stay on the negative real axis. The solutions are all decaying real exponentials, and we call the situation <term>overdamped</term>. (This is what you want for your airplane's wings!)</p>

	  <p>The four types of oscillation can be predicted from the sign of <m>b^2-4ac</m>, or equivalently from the sign of <m>\omega_n-p</m>. Or you can use the dimensionless value <m>Z=p/\omega_n</m>, but enough already.</p>
	</subsection>

	<subsection xml:id="so-cc-impulse">
	  <title>Impulse response</title>

	  <p>Recall our claim that the fundamental solution <m>g(t)</m> of the impulse forcing problem <m>ay''+by'+c=\delta(t)</m> leads to the particular response to any input <m>f(t)</m> via <m>y_p(t)=\int_0^t g(t-s)f(s)\,ds</m>. Also recall that we can fudge finding <m>g</m> by having no forcing, <m>g(0)=0</m>, and <m>g'(0)</m> becoming instantly nonzero. </p>
		
		<p>
			Consider an infinitesimally small time <m>\epsilon</m>. If we integrate <m>ay''+by'+c=\delta(t)</m> up to this time, then we can estimate
	  <me>a(g'(\epsilon)-g'(0)) + b(g(\epsilon)-g(0)) + c\int_0^\epsilon g(t)\,dt = 1 </me>.
	  Assuming that <m>g</m> itself is continuous, as <m>\epsilon\to 0</m> we conclude that <m>g'(\epsilon)-g'(0) = 1/a</m>, which is the jump we seek. That is, we use <m>g'(0)=1/a</m> as the "cheater's way" to get the fundamental solution.  </p>

	  <example>
	    <p>The solution of <m>g''=\delta(t)</m> is equivalent to <m>g''=0</m> with <m>g(0)=0</m>, <m>g'(0)=1</m>. We have <m>g(t)=c_1+c_2t</m> and then conclude that <m>c_1=0</m>, <m>c_2=1</m>. That is, <m>g(t)=t</m>.
			</p>
			<p>
				Now suppose we want to solve <m>g''=\sin(t)</m>. We find
				<me>y_p(t)=\int_0^t (t-s) \sin(s) \,ds = \bigl[ (s-t)\cos(s) - \sin(s) \bigr]_0^t = -\sin(t)-t</me>. 
				This is not the same result we get by following the other methods of this section, which would suggest only <m>-\sin(t)</m>. The other <m>-t</m> coming from the integral formula is a part of the null solution, so both expressions are valid particular solutions. 
			</p>
	  </example>
	</subsection>
	  
  </section>

  <section xml:id="so-forced-oscillators">
	<title>Exponentially forced oscillators</title>
	<introduction>
	  <p>We now consider problems in the form <m>ay''+by'+cy=f(t)</m>, where the coefficients are constant, <m>a</m> and <m>c</m> are positive (oscillation), and the driving force is exponential, <m>f(t)=e^{rt}</m> for constant <m>r</m>.</p>

	  <p> There are plently of details to sift through, but the essence is quite simple. The general solution of this problem has two contributions, <m>y=y_n+y_p</m>. The first, <m>y_n</m>, is the null solution to the associated <term>homogeneous</term> equation <m>ay''+by'+cy=0</m>, and the second, <m>y_p</m>, is any particular solution of the original problem. To see how this works, plug in:
	  <md><mrow>ay''+by'+cy \amp = a(y_n+y_p)''+b(y_n+y_p)'+c(y_n+y_p) </mrow>
	  <mrow> \amp = (ay_n''+by_n'+cy_n) + (ay_p''+by_p'+cy_p) </mrow>
	  <mrow> \amp = 0 + f = f</mrow>
	  </md>.
	  The null solution contains two arbitary constants that are used to soak up the initial conditions. We add that the particular solution is not unique, so the complete representation is not either. However, all such expressions are equivalent in the end, and the solution is always expressible in this form.</p>

	  <p>In the context of underdamped oscillators, <m>y_n</m> is also called the <term>transient solution</term>, because it decays to zero as time increases. What remains might be called the <term>steady response</term>. It depends only on the oscillator parameters and the forcing function; all dependence on initial conditions vanishes with the transient solution. </p>

	  <p>We covered finding the null solution in detail in <xref ref="so-constant-coefficients"/>. So the particular solution is our first item of business.</p>
	</introduction>

	<subsection xml:id="so-fo-expresponse">
	  <title>Exponential response</title>
	  <p>For <m>f(t)=e^{rt}</m> we let <m>y_p=Y e^{rt}</m> and plug into the ODE:
	  <me>(ar^2 + br + c)Ye^{rt} = e^{rt}</me>.
	  This is identically satisfied if we choose
	  <me>Y = \frac{1}{ar^2 + br + c}</me>.
	  In various contexts we call this the <term>exponential response function</term> or the <term>transfer function</term>. It's constant in time, but a function of the forcing exponent <m>r</m> (which is usually called <m>s</m> in transfer functions).</p>

	  <example>
	    <p>Let's solve <m>y''+5y'+6y=4 e^{-t}</m>, with <m>y(0)=0</m>, <m>y'(0)=1</m>. In stages:
	    <ol>
	      <li><p><em>Null solution</em>.
	      The characteristic equation of <m>y''+5y'+6y=0</m> is <m>s^2+5s+6=0</m>, which has roots <m>s_1=-2</m>, <m>s_2=-3</m>. Thus <m>y_n=c_1 e^{-2t}+c_2e^{-3t}</m>. </p>
	      </li>
	      <li><p><em>Particular solution</em>.
	      The transfer function is <m>Y(s)=1/(s^2+5s+6)</m>, and <m>Y(-1)=1/2</m>. Hence <m>y_p=\frac{1}{2}\cdot 4 e^{-t}=2e^{-t}</m>. </p></li>
	      <li><p><em>Complete solution</em>.
	      <m>y=y_n+y_p=c_1 e^{-2t}+c_2e^{-3t} + 2 e^{-t}</m></p>
	      </li>
	      <li><p><em>Initial values</em>.
	      We get <m>0=y(0)=c_1+c_2+2</m> and <m>1=y'(0)=-2c_1-3c_2-2</m>. This implies <m>c_1=-3</m>, <m>c_2=1</m>. Finally, <m>y=-3 e^{-2t}+e^{-3t} + 2 e^{-t}</m>.</p>
	      </li>
	    </ol>
	    </p>
	  </example>

	  <p>You may have noticed that the denominator of the transfer function is simply the characteristic polynomial of the homogeneous problem. That is always the case. The textbook uses <m>Y(s)=1/P(s)</m> in many spots to emphasize the connection.</p>

	  <p>You should be getting comfortable with the idea that exponential forcing includes sin and cos! We are simply interested in the transfer function for imaginary exponents, <m>Y(i\omega)</m> followed by the imaginary or real part of <m>Ye^{i\omega t}</m>. There is some complex-number algebra required here. </p>

	  <example>
	    <p>Consider <m>y''+y'=\sin(2t)</m>. Since the forcing term is the imaginary part of <m>e^{2it}</m>, we use
	    <me>Y(2i) = \frac{1}{(2i)^2+2i} = \frac{1}{2i-4}</me>.
	    We need to take the imaginary part of <m>Y(2i)e^{2it}</m> to get the particular solution we seek. The course of least resistance is to make the denominator of <m>Y</m> real by using a complex conjugate:
	    <me> Y(2i) = \frac{1}{2i-4}\cdot \frac{2i+4}{2i+4} = \frac{2i+4}{4+16} = \frac{i+2}{10} = \frac{1}{5} + i\frac{1}{10}</me>.
	    There are <em>two</em> contributions to each of the real and imaginary parts of the product:
	    <md>
	      <mrow>Y(2i)e^{2it} \amp = \left(\frac{1}{5} + i\frac{1}{10}\right) \bigl( \cos(2t) + i \sin(2t) \bigr)</mrow>
	      <mrow> \amp = \Bigl[ \cdots \Bigr] + i \Bigl[ \frac{1}{5}\sin(2t) + \frac{1}{10}\cos(2t) \Bigr]</mrow>
	      </md>.
	    (We don't need the real part, inside the first set of brackets.) Thus <m>y_p = [2\sin(2t)+\cos(2t)]/10</m>. </p>

	    <p>In retrospect we might have come at this problem from an entirely real standpoint. We could posit <m>y_p=A\cos(2t) + B\sin(2t)</m> and then plug in to get some algebraic conditions to determine <m>A</m> and <m>B</m>. There's still a fair amount of algebra and plenty of chances to make silly errors (see the derivation of formula (23) in section 2.4 of the textbook). I recommend that you give the complex method a fair try before retreating to the familiarity of the real-only method. </p>
	  </example>
	</subsection>

	<subsection xml:id="so-fo-resonance">
	  <title>Resonance</title>
	  <p>Time for some deja vu. When we solved <m>y'-at=e^{ct}</m> in <xref ref="flcs-exp"/>, we found that we got different formulas in the case <m>c=a</m>. We also needed a special formula for <m>y_n</m> when we got a double root in <xref ref="so-cc-double"/> The same sort of thing happens to <m>y_p</m> if we force an oscillator at one of the characteristic exponents. The reason is simple: if we try <m>y_p=Ye^{rt}</m> and <m>r</m> is a characteristic root, then we get <m>0=f(t)</m>, which can't be solved. Equivalently, the transfer function <m>Y</m> requires taking <m>1/0</m>. </p>

	  <p>Suppose we have a root <m>s</m> and that <m>r</m> is close to but not equal to <m>s</m>. Then <m>y_p=e^{rt}/P(r)</m> is a particular solution to the exponential forcing. We can add any null solution to this and it will remain a solution, thus
	  <me>y=\frac{e^{rt}-e^{st}}{P(r)}</me>
	  is a valid particular solution as well. Since <m>P(s)=0</m> by assumption, we can also write
	  <me>y=\frac{e^{rt}-e^{st}}{P(r)-P(s)}</me>.
	  Suddenly this becomes very interesting as <m>r\to s</m>; we get an indeterminate <m>0/0</m>. Using L'Hopital's rule (but with <m>r</m> as the variable, not <m>t</m>), we get the limit
	  <me>y_p = \frac{te^{st}}{P'(s)}</me>,
	  which is a valid particular solution even when forcing exactly at exponent <m>s</m>. </p>

	  <p>Realistically we only care about this effect for harmonic forcing, <m>f(t)=e^{i\omega t}</m>, where we call it <term>resonance</term>. Since the characteristic polynomial <m>P(s)=as^2+bs+c</m> has imaginary roots only if <m>b=0</m>, we only see resonance in an undamped oscillator. The resonant particular solution has an amplitude that is linearly growing in time. The derivation above shows that though the solution formula (and unboundedness) is singular at the resonant frequency, the actual behavior is a continuous limit of near-resonant behavior. </p>
	</subsection>

	<subsection xml:id="so-fo-gain-lag">
	  <title>Gain and lag</title>
	  <p>By now we have seen multiple times that forcing with <m>\cos(\omega t)</m> or <m>\sin(\omega t)</m> leads to solutions with a combination of both terms. We also know that such a result is equivalent to a shifted cosine in amplitude/phase form, and we turn our attention to this interpretation next.</p>

	  <p>With forcing <m>f(t)=e^{i\omega t}</m> we get a solution <m>y_p=Y(i\omega) e^{i\omega t}</m>. If we write the transfer function in polar form, we can express this as <m>y_p = (G e^{-i\alpha }) e^{i\omega t} = Ge^{i(\omega t - \alpha)}</m> for positive amplitude <m>G</m>, which we call the <term>gain</term>. It depends on the forcing frequency; in fact, <m>G(\omega) = |Y(i\omega)|</m>. The <m>\alpha</m> term is a <term>phase lag</term> that shows how the response may lag behind the driving force. It's usually not as interesting as the gain.  </p>

	  <example>
	    <p>The oscillator <m>4y'' + y' + 36y</m> has transfer function <m>Y(s)=1/(4s^2+s+36)</m>. At <m>s=i\omega</m> we get
	    <me>Y(i\omega) = \frac{1}{ -4\omega^2 +i\omega + 36}</me>.
	    We can compute the gain using a complex conjugate.
	    <me> G^2 = |Y|^2 = \frac{1}{ 36-4\omega^2 +i\omega} \frac{1}{ 36-4\omega^2 - i\omega} = \frac{1}{(36-4\omega^2)^2+\omega^2}</me>.
	    The gain is maximized when the denominator is minimized. This happens when <m>2 (36-4\omega^2)(-8\omega)+2\omega = 0</m>, or <m>\omega^2=9-(1/32)</m>. This is just slightly smaller than the natural undamped frequency <m>\omega_n^2=36/4=9</m>.</p>
	  </example>

	  <p>In the undamped limit, resonance produces an infinite gain. Damping reduces the peak and the maximal frequency, at first gradually and then dramatically.</p>

	</subsection>
  
  </section>

  <section xml:id="so-higher-order">
	<title>Higher-order ODEs</title>
	<introduction>
	<p>First-order and second-order ODEs dominate the landscape when it comes to scientific applications. But once in a while we go to higher order. For linear problems with constant coefficients, this looks a lot like the second-order case.</p>
	</introduction>

	<subsection>
		<title>Solving linear high-order ODEs</title>
		<p>
		Any solution of the linear ODE
		<me>a_N \frac{d^Ny}{dt^N} + \cdots + a_2 \frac{d^2y}{dt^2} + a_1 \frac{dy}{dt}  + a_0y = f(t)</me>,
		where the <m>a_N,\ldots,a_0</m> are all constants, can be expressed as <m>y=y_n+y_p</m>, the sum of a null part <m>y_n</m> that solves the problem when <m>f(t)\equiv 0</m> and a particular solution <m>y_p</m>. 
	</p>
	<p>An exponential <m>e^{st}</m> is a null solution if <m>s</m> is a root of the characteristic polynomial
		<me>P(s) = a_N s^N + \cdots + a_1 s  + a_0</me>. 
		If the roots <m>s_1,\ldots,s_N</m> are all distinct, then every null solution has the form 
		<me>y_n = c_1 e^{s_1t} + c_2 e^{s_2t} + \cdots + c_N e^{s_Nt}</me>. 
		(Repeated roots are handled by using powers of <m>t</m> as multipliers of repeated terms; we do not give the details.) 
	</p>
	<p>
		If <m>f(t)=e^{rt}</m>, then a particular solution is <m>y_p=Ye^{rt}</m> for the transfer function <m>Y(r)=1/P(r)</m>. 
	</p>

	<example>
		<p>The ODE <m>y'''+2y''-y'-2y=4 e^{2t}</m> has characteristic polynomial <m>P(s)=s^2+2s^2-s-2</m>. The roots of <m>P</m> are <m>-2,-1,1</m>. So the general null solution is 
		<me>y_n = c_1 e^{-2t} + c_2 e^{-t} + c_3 e^{2t}</me>. 
		The transfer function is evaluated at <m>r=2</m> to get <m>Y(2)=1/P(2)=1/12</m>. So the complete solution is 
		<me>y = c_1 e^{-2t} + c_2 e^{-t} + c_3 e^{2t} + \frac{1}{3}e^{2t}</me>. 
		We would need three initial conditions in order to specify the integration constants uniquely.		
		</p>
	</example>

	<p>Factoring a polynomial of degree three or higher requires luck or a computer. Only a relative few special problems yield simple closed answers.</p>

	<example>
		<p>The characteristic polynomial of <m>y''''-y=0</m> is <m>s^4-1</m>. The roots are the "fourth roots of unity," given by <m>s_j=\exp(2\pi i j/4)</m> for <m>j=0,1,2,3</m>. These simplify to <m>\pm 1, \pm i</m>. In real form the null solution is 
		<me> y_n = c_1 e^{-t} + c_2 e^{t} + c_3 \cos(t) + c_4\sin(t)</me>. 
		</p>
	</example>
	</subsection>

	<subsection>
		<title>Linear operator notation</title>
		<p>
			You're familiar with a function as a rule mapping a value (number) to another value. A rule for mapping functions to other functions is called an <term>operator</term>. The first nontrivial operator you ever encountered was differentiation, the map from <m>u(x)</m> to <m>u'(x)</m>. We sometimes write this operator as a capital D: <m>u'=D[u]</m>. (Many times we leave out the square brackets, too.)
		 </p>
		 <p>
			 One of the most salient and familiar facts about the differentiation operator is that it is <term>linear</term>. By this we mean that it obeys two rules:
			 <ul>
				 <li><p><m>D[ku] = kD[u]</m> if <m>k</m> is any constant, and</p> </li>
				 <li><p><m>D[u+v] = D[u]+D[v]</m> if <m>u,v</m> are any functions.</p></li>
			 </ul>
			 We use <m>D^k</m> to refer to the operation of taking the <m>k</m>th derivative.
			</p>
			<p>
			 Given a linear ODE of order <m>N</m> as above, we can define 
			 <me>L[y] = a_N D^N y + \cdots  + a_1 Dy + a_0 y</me>.   
			This makes it much easier to write the ODE, as <m>L[y]=f</m>. Furthermore, it's easy to check that <m>L</m> satisfies the two rules above and thus is a linear operator as well. The linearity also makes certain kinds of reasoning very simple, such as <m>L[y_n+y_p] = L[y_n]+L[y_p] = 0+f=f</m>. 
		 </p>

		 <p>
			 An interesting and seomtimes useful consequence of the defitions is that if <m>L</m> has constant coefficients and the characteristic polynomial factors as <m>P(s)=(s-s_1)\cdots(s-s_N)</m>, then <m>L=(D-s_1)\cdots (D-s_N)</m>. Here the juxtaposition means operator compositions.
		 </p>

		 <example>
			 <p>
				 The operator <m>L[y]=y'''+2y''+y'+2y</m> has characteristic roots <m>-2,\pm i</m>. Now note that for any function <m>y</m>,
				 <md>
					 <mrow> (D+2)(D-i)(D+i)y \amp = (D+2)(D-i)(y'+iy) </mrow>
					 <mrow> \amp = (D+2)\Bigl((y''+iy')-i(y'+iy)\Bigr) </mrow>
					 <mrow> \amp = (D+2)(y''+y) = y''' +y' +2y'' + 2y = L[y]</mrow>
				 </md>. We can write the factors in any order, and even things like <m>L[y]=(D^2+1)(D-2)</m>.
			 </p>
		 </example>
	</subsection>

  </section>

  <section xml:id="so-applications">
	<title>Applications of oscillators</title>
	<introduction>
		<p>Oscillations appear all over science and engineering, and 2nd order linear equations are often used to model them. The best-known examples are mechanical and electrical, but there are chemical and biochemical ones as well.</p>
	</introduction>
	

		<subsection>
			<title>Mechanical oscillators</title>
			<p>
				As we have already seen, a mass hanging from a spring satisfies the ODE
				<me>my'' + b y' + ky = f(t)</me>.
				(As is typical, all the named constants are nonnegative.) Here <m>m</m>> is the mass, <m>b</m>>is a coefficient of friction or other damping forces, and <m>k</m>> is a characteristic constant of the spring. The term <m>f(t)</m> represents an external driving force. If we have initial conditions, they give values for <m>y</m> and <m>y'</m>.
			</p>

			<p>
				The gravitational force on a pendulum bob (think child on a swing) has a related equation. The proper ODE is 
				<me>\theta'' + \gamma \theta + \frac{g}{L}\sin \theta = f(t)</me>,
				where <m>\theta(t)</m> is the angle made with vertical-down and <m>L</m> is the arm length. This equation is nonlinear. But if the angle remains small, then a reasonable approximation is 
				<me>\theta'' + \gamma \theta + \frac{g}{L}\theta = f(t)</me>, 
				which is a linear oscillator. 
			</p>
		</subsection>

		<subsection>
			<title>Electrical oscillators</title>
			<p>
				An AC circuit often has elements of resistance, capacitance, and inductance. These analogize perfectly to friction/damping, spring constant, and mass. If these elements are wired in series with a generator, the governing ODE is
				<me>LI'' + RI' + CI = \mathcal{E}(t)</me>,
				where <m>L</m> is inductance, <m>R</m> is resistance, <m>C</m> is capacitance, and <m>I(t)</m> is the current flowing through the circuit. The forcing term represents the generator (EMF). For alternating current this is modeled as a cosine (or complex exponential) forcing.  The transfer function is expressed a little differently, because often the target quantity is 
			</p>
		</subsection>
      
		<subsection xml:id="so-ap-notation">
	  <title>Unifying notation</title>

	  <p>When you have lots of different versions of the same root problem, with different symbols and units, you have three options. You can solve each new problem from scratch, you can derive custom formulas for each application, or you can try to find a minimal set of parameters and express each problem using those. Let's consider the last pathway here.</p>

	  <p>
			The mechanical oscillator <m>ay'' + by' + cy = f(t)</m> is more conveniently expressed as
	  	<me> y'' + \frac{b}{a} y' + \frac{c}{a} y + \frac{c}{a} F(t)</me>.
			One reason to prefer this form is that <m>y</m> must have the same units as the forcing <m>F</m>. Furthermore, we see the undamped natural frequency <m>\omega_n=\sqrt{c/a}</m>, and this has units of inverse time only. The same units apply to <m>b/a</m>, so we introduce the new dimensionless parameter
			<me>Z = \frac{1}{2} \frac{b/a}{\omega_n} = \frac{b}{\sqrt{4ac}}</me>. 
			Now the ODE becomes 
			<me> y'' + 2 Z \omega_n y' + \omega_n^2 y + \omega_n^2 F(t)</me>.
		</p>
		<p>
			The characteristic roots are now conveniently <m>-Z \omega_n \pm \omega_n \sqrt{Z^2-1}</m>. We see that the condition for being underdamped is simply <m>Z \lt 1</m>. In this case we express the roots as <m>-Z \omega_n \pm \omega_d</m>, using the damped frequency <m>\omega_d=\omega_n\sqrt{1-Z^2}</m>. 
		</p>
		<p>
			Continuing the underdamped case, the gain works out to be
	  	<me>G^2 = \frac{1}{(\omega_n^2-\omega^2)^2 + 4\omega_n^2Z^2\omega^2}  = \frac{\omega_n^4}{(1-\rho^2)^2 + 4Z^2\rho^2}</me>,
			where <m>\rho=\omega/\omega_n</m> is a dimensionless frequency ratio. If also <m>Z^2\lt 1/2</m>, the gain is maximized when <m>\rho=\sqrt{1-2Z^2} \lt 1</m>. The gain at that maximum is proportional to <m>1/Z</m>, so much growth in the response is possible near the natural frequency when damping is small in the sense of <m>Z\approx 0</m>. This can be considered a practical resonance for lightly damped systems. 
		</p>

	  <example>
	    <p>The oscillator <m>4y'' + y' + 36y</m> from an earlier example has <m>\omega_n=3</m> and <m>Z=1/\sqrt{16\cdot 36}=1/24</m>. Accordingly, gain is maximized when <m>\omega/\omega_n=1-\frac{1}{288}\approx 0.9965</m>, so for practical purposes a resonance occurs at just slightly less than the natural, undamped frequency.
			</p>
		</example>

	</subsection>
	
	</section>

  <section xml:id="so-general-forcing">
	<title>Other forcing functions</title>
	<introduction>
		<p>
			Exponential forcing functions are very important, especially when allowing complex exponents. But they aren't the whole world. In this section we go over two other major techniques for trying to get particular solutions to linear, 2nd-order ODEs. Both assume that you have access to the complete null solution already. 
		</p>
	</introduction>
	
		<subsection xml:id="so-gf-muc">
			<title>Undetermined coefficients</title>
			<p>In the method of undetermined coefficients we assume that we already know everything about the particular solution except for some constants to be found using algebra. Best to start with examples.</p>

			<example>
				<p>Let's find a particular solution of <m>y'' +4y'+4y=8t^2</m>. We reason as follows. If we plug in some <m>Ct^2</m> for <m>y</m> with a constant <m>C</m>, then all the derivatives generate multiples of <m>t</m> and <m>1</m>. That is, both sides of the ODE will be quadratic polynomials. So we generalize just a bit to allow 
				<me>y_p = A + Bt + Ct^2</me>. 
				Plugging that into the ODE yields
				<md>
					<mrow>2C + 4(B+2Ct) + 4(A+Bt+Ct^2) \amp = 8t^2</mrow>
					<mrow>4C t^2 + (8C+4B) + (2C+4B+4A) \apm = 8t^2</mrow>
				</md>. This has to be an identity for all <m>t</m>>. Matching like powers, we can read off <m>C=2</m>, then <m>B=-4</m>, and finally <m>A=3</m>>. This provides us with <m>y_p=2t^2+4t+3</m>. (The complete solution would require us to use the characteristic equation to find the null solution as well.)
				</p>
			</example>

			<p>
				In general, if <m>f(t)</m> is a polynomial, then we let <m>y_p</m> be a polynomial of the same degree with unknown coefficients. Substitution into the ODE and matching powers gives equations to determine those. As in the example above, you must put <em>all</em> the polynomial terms into <m>y_p</m>, even if they are missing from <m>f(t)</m>.  
			</p>

			<p>The next example has a polynomial times an exponential.</p>

			<example>
				<p>Let's try for <m>y'' -2y'-3y=10te^{4t}</m>. For much the same reasons as in the last case, we try 
				<me>y_p = (A + Bt)e^{4t}</me>. There's nothing to do now except grind out some derivatives. 
				<md>
					<mrow>8(2A+B+2Bt)e^{4t} - 2(4A+B+4Bt)e^{4t} -3 (A + Bt)e^{4t} \amp =10t e^{4t}</mrow>
					<mrow> 8(2A+B) - 2(4A+B) -3A + t(16B-8B-3B) =  10t</mrow>
				</md>. From this, by matching powers, we deduce <m>B=2</m> and then <m>5A+12=0</m>. So <m>y_p = (2 - (12/5)t)e^{4t}</m>.
				</p>
			</example>

			<p>
				So if <m>f(t)</m> is a polynomial times <m>e^{st}</m>, let <m>y_p</m> be the same exponential times a polynomial of the same degree with unknown coefficients. This technically applies too if the polynomial has degree zero; this was our mainstay in <xref ref="so-forced-oscillators"/>. It also applies for imaginary or even complex exponents.
			</p>

			<p>
				There's a method devoted to forcing by sine or cosine, multiplied by a polynomial and/or exponential. As we have seen  in previous examples, any sin or cos in the forcing requires both to be allowed in <m>y_p</m>. I don't personally see much value in exploring these rules, as they are no easier to use, and may be even more ornate, than the exponentials we have been doing. 
			</p>

			<p>
				There is one more situation to be aware of, another variation on a familiar theme. If your <m>y_p</m> is part of the null solution of the ODE, you have to "promote" it by powers of <m>t</m> until it no longer overlaps. 
			</p>

			<example>
				<p>
					Given <m>y'' -2y'-3y=4e^{-t}</m>, we are led to try <m>y_p=Ae^{-t}</m>: 
					<md> 
						<mrow>(Ae^{-t}) - 2(-Ae^{-t}) - 3Ae^{-t} \amp = 4e^{-t} </mrow>
						<mrow> 0 \amp 4e^{-t} </mrow>
					</md>. This is impossible to satisfy. What's happened is that the characteristic polynomial is <m>(s+1)(s-3)</m>, so <m>e^{-t}</m> is part of the null solution (as the above calculation already showed). Instead we have to use <m>y_p=Ate^{-t}</m>, which gives 
					<md> 
						<mrow> Ae^{-t}(t-2) - 2Ae^{-t}(1-t) - 3Ate^{-t} \amp = 4e^{-t} </mrow>
						<mrow> -4Ae^{-t} \amp 4e^{-t} </mrow>
					</md>, so <m>A=1</m> and <m>y_p</m> is valid.
				</p>
			</example>
		
		</subsection>

		<subsection xml:id="so-gf-vp">
		<title>Variation of parameters</title>
		<p>
			While UC relies on "educated guesswork" (actually rigid rules that are a bit complex to apply), <term>variation of parameters</term> follows the same recipe every time. Given <m>L[y]=f</m>, suppose we have a general null solution in the form 
			<me>y_n(t) = c_1 y_1(t) + c_2 y_2(t)</me>.
			For problems with constant coefficients, <m>y_1</m> and <m>y_2</m> are exponentials (or sin/cos equivalents), possibly multiplied by <m>t</m>. In any case, the VP trick is to replace the integration constants with functions of time:
			<me>y_p(t) = c_1(t) y_1(t) + c_2(t) y_2(t)</me>. 
			To keep notation simpler, we assume below that the coefficient of <m>y''</m> in the ODE is 1. 
		</p>

		<p>
			Taking this odd assumption on faith, we compute 
			<me> 
				y_p' = c_1'y_1+c_1y_1'+ c_2'y_2+c_2y_2'  
			</me>. 
			Before we go on, we're going to put a restriction on our mystery functions, namely <m>c_1'y_1+c_2'y_2=0</m>. That makes life a little easier. Continuing, we get
			<me>
				y_p'' = c_1'y_1'+ c_1y_1'' + c_2'y_2'+ c_2y_2''  
			</me>.
			When we put all this mess into <m>y''+By'+Cy=f</m>, something cool happens: 
			<me> 
				c_1(y_1''+By_1'+Cy_1) + c_2(y_2''+By_2'+Cy_2) + c_1'y_1' + c_2'y_2' = f 
			</me>. Thanks to the null property of <m>y_1</m> and <m>y_2</m>, this boils down to <m>c_1'y_1' + c_2'y_2' = f</m>. 
		</p>

		<p>
			To summarize so far, we get a valid particular solution using two conditions, 
			<md>
				<mrow>c_1'y_1+c_2'y_2 \amp = 0 </mrow>
				<mrow> c_1'y_1' + c_2'y_2' \amp = f </mrow>
			</md>. Stare at this long enough and you'll realize that it's just two linear equations for <m>c_1'</m> and <m>c_2'</m>. You can do some elimination to solve them, and you wind up with 
			<me>c_1' = -\frac{y_2f}{W}, \qquad c_2' = \frac{y_1f}{W},</me>
			with 
			<me>W(t) = y_1y_2'-y_1'y_2</me>,
			known as the <term>Wronskian</term> of <m>y_1</m> and <m>y_2</m>. Finally, "all" you have to do is integrate these expressions to get <m>c_1</m> and <m>c_2</m>, and from there you get <m>y_p</m>. 
		</p>

		<p>If you immediately wondered "what if the Wronskian is zero?" when you saw the above formulas, you have earned a special place in my heart. This possibility can be ruled out for some rather general theortical reasons, but we can also just calculate away the issue for equations with constant coefficients. If we have distinct characteristic roots <m>s_1</m> and <m>s_2</m>, then 
		<me>
			W = (e^{s_1t})(s_2e^{s_2t}) - (s_1e^{s_1t})(e^{s_2t}) = (s_2-s_1)e^{(s_1+s_2)t}
		</me>. 
		Not zero, as promised. In the case of repeated root <m>s_1</m>, then
		<me>
			W = e^{s_1t}(s_1t +1)e^{s_1t} - (s_1 e^{s_1t})(t e^{s_1t}) = e^{2s_1t} 
		</me>. 
		And here you were all worried. 
		</p>
		</subsection>

		<subsection>
			<title>Fundamental solutions, again</title>
			<p>
				If we continue to push ahead with the VP formulas, interesting things start to happen. For distinct roots,
				<me>
					c_1' = -\frac{\exp(s_2t)}{(s_2-s_1)\exp((s_1+s_2)t)} f(t)  =  \frac{f(t)e^{-s_1t}}{s_1-s_2}
				</me>,
				and similarly <m>c_2'= f(t)e^{-s_2t}/(s_2-s_1) </m>. Then 
				<md>
					<mrow>
					y_p \amp = e^{s_1 t} \int_0^t \frac{f(\tau)e^{-s_1\tau}}{s_1-s_2}\, d\tau + 
					e^{s_2 t} \int_0^t \frac{f(\tau)e^{-s_2\tau}}{s_2-s_1}\, d\tau </mrow>
					<mrow> \amp =  \int_0^t  f(\tau) \frac{  e^{s_1(t-\tau)} -  e^{s_2(t-\tau)}}{s_1-s_2}\, d\tau  </mrow>
					<mrow> \amp =  \int_0^t  f(\tau) g(t-\tau) \, d\tau  </mrow>
				</md>, with
				<me>g(t) = \frac{ e^{s_1t} -  e^{s_2t} }{s_1-s_2} </me>. 
				By definition the integral formula makes <m>g</m> an fundamental solution. Remember, <m>g</m> is also the impulse response, and the null solution with <m>g(0)=0</m>, <m>g'(0)=1/A=1</m>. But this derivation doesn't rely on any sketchy deltas.
			</p>
			<p>
				For the case of a repeated root we can go though the VP formulas again, or just let <m>s_2\to s_1</m> and get
				<me>g(t) = t e^{s_1t} </me>. 
			</p>

		</subsection>
  </section>

  <section xml:id="laplace-solutions">
	<title>Laplace transform methods</title>
	<introduction>
	<p>
		The correspondence between <m>L[y]=Ay''+By'+Cy</m> and <m>P(s)=As^2+Bs+C</m> is by now obvious. It came by way of assuming <m>y(t)=Y(s)e^{st}</m>, and we get <m>Y(s)=1/P(s)</m>. Factoring <m>P</m> tells us what we need to know. For example, if its roots are <m>-2</m> and <m>-4</m>, then we know that <m>y(t)</m> is a combination of <m>e^{-2t}</m> and <m>e^{-4t}</m>. If an exponential, polynomial, step, or impulse forcing term is present, it contributes to the solution in similarly predictable ways. The only mystery to work out is the coefficients. 
	</p>

	<p>
		There is a method of expressing all this knowledge called the <term>Laplace transform</term>. Given (almost) any function <m>f(t)</m>, there is a doppelganger function <m>F(s)</m> defined by an integral,
		<men xml:id="so-lap-lxform">
			F(s) = \int_0^\infty f(t) e^{-st}\, dt
		</men>. 
		In this business, time starts at <m>t=0</m>, and nothing before then matters. One thing to note right away is that the Laplace transform is a linear operator. It's often written using a script L, as in <m>F(s)=\mathcal{L}[f(t)]</m>. The usual convention is that a lowercase function of <m>t</m> transforms to its uppercase namesake as a function of <m>s</m>. 
	</p>
	</introduction>

	<subsection>
		<title>Derivatives</title>
		<p>
			Applying integration by parts to the definition reveals the main reason Laplace transforms are useful in ODEs.  
			<md>
				<mrow>\mathcal{L}[y'(t)] \amp = \int_0^\infty y'(t) e^{-st}\, dt </mrow>
				<mrow>\amp = \left[ y(t)e^{-st}\right]_0^\infty - \int_0^\infty (-s) y(t) e^{-st}\, dt </mrow>
				<mrow>\amp = -y(0) + s Y(s)</mrow>
			</md>. 
			Calculus (differentiation) is turned into algebra (multiplication). We can apply this rule repeatedly to transform higher derivatives:
			<me>\mathcal{L}[y''(t)] = s \mathcal{L}[y'(t)] - y'(0) = s^2Y(s) - sy(0) - y'(0)</me>.
		</p>

		<p>
			So here is the plan of attack. Given an ODE <m>Ay''+By'+Cy=f</m>, we transform everything in sight (using linearity!) to get 
			<me>
				A[s^2Y(s) - sy(0)-y'(0)] + B[sY(s)-y(0)] + CY(s) = F(s)
			</me>. 
			Now solve for the transform of the solution:
			<me>
				Y(s) = \frac{ F(s) + (As+B)y(0) - Ay'(0)}{As^2 + Bs + C}
			</me>. Note that if we want just a particular solution, we can use <m>y(0)=y'(0)=0</m>. In any case, "all" we have to do know is to compute the inverse transform, that is, find <m>y(t)</m> given <m>Y(s)</m>. This is less straightforward than computing the forward transform, unfortunately. But we can make it managable in much the same way you learned indefinite integration, by learning some canonical cases and then matching patterns. 
		</p>
	</subsection>

	<subsection> 
		<title>Canonical transforms</title>
		<p>
			<ol>
				<li><em>Exponential functions.</em> This is straightforward to calculate from definition. For any complex number <m>z</m>,
				<me>
					{\cal L}[e^{z t}] = \int_0^\infty e^{(z-s)t}\, dt = \left[ -\frac{1}{s-z} e^{(z-s)t} \right]_0^\infty = \frac{1}{s-z}
				</me>. 
				(To make this valid requires that <m>{\text Re}(z)\lt s</m>, but we are going to ignore such technicalities.) An important special case is when <m>z=0</m>: <m>{\cal L}[1]=1/s</m>.
				</li>
				<li>
					<em>Cos and sin.</em> We can use an imaginary number in the exponential and take the real or imaginary part of the result.  Thus,
					<me>{\cal L}[\cos(\omega t)] = \text{Re}\left( \frac{1}{s-i\omega} \frac{s+i\omega}{s+i\omega} \right) = \frac{s}{s^2+\omega^2}</me>,
					and 
					<me>{\cal L}[\sin(\omega t)] = \text{Im}\left( \frac{1}{s-i\omega} \frac{s+i\omega}{s+i\omega} \right) = \frac{\omega}{s^2+\omega^2}</me>.
				</li>
				<li><em>Resonant term.</em> We know that terms such as <m>te^{zt}</m> appear due to repeated roots and resonances. The clever way to get its transform is to think about differentiation <em>with respect to <m>z</m></em>:
				<me>\mathcal{L} \left[ \frac{d}{dz} e^{zt} \right] = \frac{d}{dz}  \mathcal{L} \left[ e^{zt} \right] = \frac{d}{dz}\left[ \frac{1}{s-z} \right] </me>,
				from which we conclude 
				<me>\mathcal{L} \left[ t e^{zt} \right] = \frac{1}{(s-z)^2} </me>.
				If we differentiate again we find 
				<me>\mathcal{L} \left[ t^2 e^{zt} \right] = \frac{2}{(s-z)^3}</me>, 
				and in general
				<me>\mathcal{L} \left[ t^n e^{zt} \right] = \frac{n!}{(s-z)^{n+1}} </me>.
				As a special case, taking <m>z=0</m> allows us to transform any power of <m>t</m>, and therefore any polynomial. 
				</li>
			</ol>
		</p>
	</subsection>

	<subsection>
		<title>The first-order case</title>
		<p>
			Let's go back and revisit an old first-order linear friend, <m>y'-ay=e^{ct}</m>. Transforming both sides, we get 
			<me> [sY(s)-y(0)] - aY(s) =\frac{1}{s-c}</me>,
			and thus 
			<me> Y(s) = \frac{y(0)}{s-a} + \frac{1}{(s-c)(s-a)}</me>. 
			The first part corresponds to <m>y_n(t)=y(0)e^{at}</m>, which is the null solution. To invert the transform of the second term, we need to express it using <term>partial fractions</term>. The text has a clever way mnemonic for this:
			<me> \frac{1}{(s-c)(s-a)} = \frac{1}{(a-c)(s-a)} + \frac{1}{(s-c)(c-a)}</me>. Thus we get a sum of two exponentials,
			<me>y_v(t) =  \frac{1}{c-a}\left( e^{ct}-e^{at} \right) </me>. 
			This is the "very particular" solution that has <m>y_v(0)=0</m>. 
		</p>
		<p>
			There's no new result here, though the process is quite streamlined. The values <m>s=a</m> and <m>s=c</m> that are roots of the denominator of <m>Y(s)</m> are called <term>poles</term> of the transform. They signal clearly and quickly that every solution is a combination of the exponentials <m>e^{at}</m> and <m>e^{ct}</m>.
		</p>
	</subsection>

	<subsection>
		<title>Second-order equations</title>
		<p>
			Let's break down what happens in a fundamental case, <m>y''-y=e^{rt}</m>.  Transforming both sides gives <m> [s^2Y(s) - sy(0)-y'(0)] - Y(s) = 1/(s-r)</m>. For reasons to be clarified later, let's skip to the "very particular" part with <m>y_v(0)=y_v'(0)=0</m>, for which
			<me>Y(s) = \frac{1}{(s^2-1)(s-r)} = \frac{1}{(s+1)(s-1)(s-r)}</me>. 
			This time there are three poles. Again the book's trick works well here:
			<me> \frac{1}{(s+1)(s-1)(s-r)} = \frac{1}{(s+1)(-1-1)(-1-r)} + \frac{1}{(1+1)(s-1)(1-r)}  + \frac{1}{(r+1)(r-1)(s-r)} </me>. 
			It should be apparent that this works only if <m>r\neq -1</m> and <m>r \neq 1</m> (i.e., no resonance). From here we get three exponentials:
			<me>y_v(t) = \frac{1}{2(1+r)}e^{-t} + \frac{1}{2(1-r)}e^{t} + \frac{1}{r^2-1}e^{rt}</me>. 
		</p>
		<p>
			From here we can write out a general solution. The characteristic roots are <m>\pm 1</m>, so
			<me>y(t) = c_1 e^{-t} + c_2 e^{t} + y_v(t)</me>. 
			If desired, it's easy to solve for the constants in this, because <m>y_v</m> contributes nothing:
		<md>
			<mrow>y(0) \amp = c_1 + c_2  </mrow>
			<mrow>y'(0) \amp = -c_1 + c_2 </mrow>
		</md>. 
		That's a lot less work than doing a separate PFD for the initial conditions as represented in the transform. 
		</p>
		<p>
			The main value in the Laplace transform method is in finding a particular solution. The poles <m>Y(s)</m> due to the homogeneous or null part are just the characteristic roots, and we know how to work with those anyway. 
		</p>
	</subsection>

	
  </section>

  <section xml:id="laplace-properties">
	<title>Laplace transform properties</title>
	<p>From section 8.5</p>
  </section>

  <section xml:id="convolutions">
	<title>Convolutions</title>
	<p>From section 8.6</p>
  </section>

</chapter>
