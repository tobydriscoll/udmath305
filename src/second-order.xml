<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="second-order-ode" xmlns:xi="http://www.w3.org/2001/XInclude">
	<title>Second-order ODEs</title>
 
	<section xml:id="so-simple-harmonic">
	<title>Simple harmonic motion</title>

	<subsection xml:id="so-sh-newton">
	  <title>Newton's law</title>
		<p>The subtext of this chapter is, "<m>F=ma</m> is not just a good idea; it's the law." Newton's second law, to be exact. Since acceleration is the second derivative of position, this leads us to second-order ODEs of the form <m>y''=F(t,y,y')</m>. You can always think about this as a problem specifying a position-dependent force, though it's much more general than that.</p>

		<p>Suppose you ask a question such as, "Where will my car be after 4 seconds of maximum acceleration?" You can't fully answer it unless you know two more pieces of data: the position and velocity of the car at the start. In general, we need to know <m>y(0)</m> and <m>y'(0)</m> (or values at some other time) in order to solve a second-order problem uniquely.</p>

		<p>The grandpappy of all second-order ODEs is <m>y''=-ay</m>, for a positive constant <m>a</m>. In terms of Newton's second law, this arises with a force term defined as <m>F=-ky</m>, because then <m>my''=-ky</m>. Most springs behave like this to a good approximation, provided you don't stretch or compress them too far (an idea called Hooke's Law). Playground swings, molecular bonds, and electrical inductors are often well represented this way as well. </p>

		<p>The negative sign in the force is crucial. It makes the force a "restoring" one: move to the right, and you are pushed to the left, and vice versa. Soon we will let additional external forces act, in which case the ODE becomes <m>my''+ky=f(t)</m>.</p>
	</subsection>

	<subsection xml:id="so-sh-null">
		<title>Unforced solution</title>
		<p>Solutions of <m>my''+ky=0</m> are quite easy to get. Define <m>\omega_n=\sqrt{k/m}</m>. Then you can easily check that <m>\cos(\omega_n t)</m> is a solution. So are <m>-\cos(\omega_n t)</m> and <m>7164\cos(\omega_n t)</m>, for that matter; any constant multiplier is allowed. You can also check that <m>A\sin(\omega_n t)</m> is a solution for any constant multiplier. Finally, we can combine them as well, into
		<me>y = c_1 \cos(\omega_n t) + c_2 \sin(\omega_n t).</me>
		It turns out that this expression captures all possible solutions. We have <term>simple harmonic motion</term>.	</p>

		<p>We started with a second derivative in the ODE and now we have two arbitrary "integration constants." That's no coincidence. It's also not a coincidence that we need two initial values to find a unique solution. For the record, we can easily find that
		<me>y(t) = y(0) \cos(\omega_n t) + \frac{y'(0)}{\omega_n} \sin(\omega_n t).</me></p>

		<p>Recall from <xref ref="frc-shifted-cos"/> that a combination of sin and cos at the same frequency is equivalent to a shifted cosine, so we can also write <m>y=R\cos(\omega_n t - \alpha)</m> for an amplitude <m>R</m> and a phase <m>\alpha</m>, and we get those values by expressing the cartesian point <m>(y(0),y'(0)/\omega_n)</m> in polar form. </p>

		<exercise>
		  <statement><p>Find the solution in shifted cosine form of <m>2y''+32y=0</m> with <m>y(0)=1</m>, <m>y'(0)=-8</m>.</p>
		  </statement>
		  <solution>
		    <p>We have <m>\omega_n=\sqrt{16}=4</m>, so <m>y = c_1 \cos(4 t) + c_2 \sin(4 t).</m> We then derive <m>1=y(0)=c_1</m> and <m>-8=y'(0)=4c_2</m>, so <m>y = \cos(4 t) -2 \sin(4 t).</m> Converting <m>(1,-2)</m> to polar form gives <m>R=\sqrt{5}</m> and <m>\alpha = \tan^{-1}(-2)</m>. </p>
		  </solution>
		</exercise>
	    

		<p>A quick word about words. In math we typically use the term frequency to mean an <term>angular frequency</term>, as with the value <m>\omega_n</m> that multiplies <m>t</m> inside the trig functions. You can think of this value as meaning "radians per second." In engineering and physics one might be more comfortable using "cycles per second" (or Hertz), which leads to a different value of frequency, <m>\omega_n/2\pi</m>. We won't be using that, just to avoid confusion, but now you're aware that corresponding formulas in other sources might have extra factors of <m>2\pi</m> sprinkled about.</p>
	
	</subsection>

	<subsection>
	  <title>Forced response</title>
	  <p>Let's consider the forced problem now, <m>my''+ky=f(t)</m>. The most natural type of forcing is also harmonic, but at a different frequency <m>\omega</m>. This is a linear problem, and just as in the first-order case, we get a solution consisting of the unforced part from above plus a particular solution <m>y_p</m> which we often call the <term>response</term> to the forcing term.</p>

	  <p>The math remains incredibly simple for now. Suppose <m>f(t)=\cos(\omega t)</m>, and let <m>y_p=Y\cos(\omega t)</m>. (Why? Because it works.) Plug <m>y_p</m> into the equation and you find that it's valid if <m>Y=1/(k-m\omega^2)</m>. Let's write the complete solution using <m>k=m\omega_n^2</m>:
	  <men xml:id="eq-so-sh-response">y(t) = R\cos(\omega_n t-\alpha) + \frac{1}{m(\omega_n^2-\omega^2)}\cos(\omega t).</men>
	  It's clear that this formula breaks when <m>\omega=\omega_n</m>, which is the case of <term>resonance</term>. We'll worry about that later on. Notice that otherwise the solution consists of one part at the natural frequency and another at the forcing (or driving) frequency. This is the "null plus particular" structure we have seen all along for linear equations. </p>

	  <exercise>
	    <statement>
	      <p>Find a particular solution of <m>y''+9y = \sin(t)-\cos(t).</m></p>
	    </statement>
	    <solution>
	      <p>It's not hard to see that the sin forcing term can be solved by <m>y_p=Y\sin(t)</m> for a constant <m>Y</m>. We can put both forcing terms together with <m>y_p=A\cos(t)+B\sin(t)</m> and insert it into the equation to find
	      <me>(9-A)\cos(t)+(9-B)\sin(t) = \sin(t)-\cos(t),</me>
	      which is identically true if and only if <m>9-A=-1</m> and <m>9-B=1</m>. So
	      <me>y_p=10\cos(t)+8\sin(t)</me>.</p>
	    </solution>
	  </exercise>
	</subsection>

	<!--subsection>
		<title>Impulse response</title>
		  
	  	<p>
			Recall in <xref ref="flcs-impulse"/> that in first-order problems, the particular solution when <m>f(t)=\delta(t)</m> is called the impulse response. The solution of <m>my''+ky=\delta(t)</m> with zero initial conditions is
	  		<men xml:id="eq-so-sh-fundsol">
				  g(t) = \frac{1}{m\omega_n} \sin(\omega_n t)
			</men>.
	  		Also like the first-order case, the impulse response is also the solution to a certain homogeneous problem: <m>g(0)=0</m>, <m>g'(0)=1/m</m>, and solves <m>mg''+kg=0</m> for <m>t\gt 0</m>.
		</p>

	 	 <p>
		  	(Sort-of justification: Since <m>g(0)=0</m>, we have <m>mg''=\delta(t)</m> there. Integrating once tells us that <m>mg'</m> is a step function, so <m>g'=1/m</m> immediately at time zero. Kinda. We will have a better derivation much later on.) 
		</p>

	  	<p>
			The kicker is that the impulse response can be used to solve <em>any</em> forcing problem, at least in principle. Given the forcing term <m>f(t)</m>, a particular solution is
	  		<me>
				  y_p(t) = \int_0^t g(t-s)f(s)\,ds
			</me>.
	  		Again, in a linear problem we just need to add up the contributions of impulses at every moment. Because of this formula, the impulse response <m>g</m> is also called a <term>fundamental solution</term>. We'll be using and coming at this formula a few different ways throughout the chapter.
		</p>
	</subsection-->

	</section>

  	<section xml:id="so-complex-exponentials">
		<title>Complex exponentials</title>
		<p><em>From 2.2 in the text, but some of this was covered by me in the last chapter. You can skip "<m>s^n=1</m>".</em></p>
		<p>For second-order equations the complex plane becomes indispensible. Recall again the polar form of a complex number,
		<me>re^{i\theta} = (r\cos \theta)+i(r\sin \theta)</me>.
		This gives us a way of looking at the exponential of any complex number as
		<me>e^{x+iy} = e^x e^{iy} = (e^x\cos y)+i(e^x\sin y)</me>.
		More importantly, we can look at exponential functions of time more generally as
		<men xml:id="eq-so-exp-exp">e^{(a+i \omega)t} =  e^{at} (\cos \omega t + i \sin \omega t)</men>.
		Reading from right to left, we have the product of <m>e^{i\omega t}</m>, which goes around and around the unit circle at constant speed, and <m>e^{at}</m>, which is an amplitude that exponentially grows or decays (or stays fixed if <m>a=0</m>).  </p>

		<example>
			<p> 
				Here are some looks at the function <m>e^{st}</m> for different complex values of <m>s</m>. First, when <m>s</m> is purely imaginary, the function values stay on the unit circle in the complex plane. Taking the real and imaginary parts of the function give cosine and sine, respectively. 
			</p>
			<sidebyside>
	 			<program language="matlab" permid="so_cexp_neutral">
	    			<input>
a = 0;  om = 1;
t = linspace(0, 25, 500);
f = exp((a+1i*om)*t);

plot3(t, real(f), imag(f), 'LineWidth',2)
hold on
plot3(t,real(f), 0*t-1.5)
plot3(t, 0*t+2, imag(f))
plot3(0*t+30,real(f),imag(f),'k')
axis([0 30 -2  2 -1.5 1.5])
title(sprintf('a = %.2f, omega = %.1f',a,om))
grid on, xlabel('Time')
ylabel('Real Axis')
zlabel('Imag Axis')	
set(gca,'dataaspect',[6,1,1])    			
					</input>
				</program>
				<image source="matlab/so_cexp_neutral.svg"/>	  
			</sidebyside>
			<p>
				If <m>\text{Re} s \gt 0</m>, the magnitude of the function grows exponentially. The result is an outward spiral. The imaginary part of <m>s</m> controls the frequency or "tightness" of the spiral. 
			</p>
			<sidebyside>
	 			<program language="matlab" permid="so_cexp_grow">
	    			<input>
a = 0.01;  om = 2;
t = linspace(0, 25, 500);
f = exp((a+1i*om)*t);

plot3(t, real(f), imag(f), 'LineWidth',2)
hold on
plot3(t,real(f), 0*t-1.5)
plot3(t, 0*t+2, imag(f))
plot3(0*t+30,real(f),imag(f),'k')
axis([0 30 -2  2 -1.5 1.5])
title(sprintf('a = %.2f, omega = %.1f',a,om))
grid on, xlabel('Time')
ylabel('Real Axis')
zlabel('Imag Axis')	  
set(gca,'dataaspect',[6,1,1])  			
					</input>
				</program>
				<image source="matlab/so_cexp_grow.svg"/>	  
			</sidebyside>
			<p>
				Finally, if <m>\text{Re} s \lt 0</m>, the spiral is a decaying one. The real and imaginary parts are attenuated oscillations. 
			</p>
			<sidebyside>
	 			<program language="matlab" permid="so_cexp_decay">
	    			<input>
a = -0.1;  om = 0.8;
t = linspace(0, 25, 500);
f = exp((a+1i*om)*t);

plot3(t, real(f), imag(f), 'LineWidth',2)
hold on
plot3(t,real(f), 0*t-1.5)
plot3(t, 0*t+2, imag(f))
plot3(0*t+30,real(f),imag(f),'k')
axis([0 30 -2  2 -1.5 1.5])
title(sprintf('a = %.2f, omega = %.1f',a,om))
grid on, xlabel('Time')
ylabel('Real Axis')
zlabel('Imag Axis')	 
set(gca,'dataaspect',[6,1,1])   			
					</input>
				</program>
				<image source="matlab/so_cexp_decay.svg"/>	  
			</sidebyside>

      </example>

	<p>
		The upshot is that in the function <m>e^{st}</m>, the real part of <m>s</m> determines exponential growth or decay, and the imaginary part of <m>s</m> determines frequency of oscillation. All real exponentials, and all harmonic sines and cosines, are just special cases.
	</p>

	<p>
		Let's look back a moment at <m>my''+ky=0</m>. If we now try an exponential solution of the form <m>y_n=e^{st}</m>, then we arrive at <m>(ms^2+k)e^{st}=0</m>, which we can make true if <m>s^2=-k/m</m>. This has two roots, <m>s=\pm i \omega_n</m>, and we are led to
		<me> y_n = c_1 e^{i\omega_n t} + c_2 e^{-i\omega_n t}</me>.
		A little rearrangement should convince you that this is equivalent to the form we got in the last section, just using complex numbers.
	</p>

	<p>
		Similarly, for the forced problem <m>my''+ky=e^{i\omega t}</m>, we can get a particular solution <m>y_p=Y e^{i\omega t}</m> if we let <m>Y=1/(k-m\omega^2)</m>. This works for both cosine and sine forcing upon taking real and imaginary parts, and we go right back to the formulas from before. Rather than exploring the details here, we will next do it all for a more general problem that includes a <m>y'</m> term in the ODE.
	</p>
  	</section>

	<section xml:id="so-constant-coefficients">
		<title>Constant coefficients</title>
		<introduction>
		<p><em>From 2.3 in the text, skipping "higher order equations", "shift invariance", and "damping ratio" for now.</em></p>
	  		<p>
		 		 We're now set to tackle the linear, 2nd order ODE <m>A \frac{d^2y}{dt^2}+B\frac{dy}{dt}+Cy=0</m> for constants <m>A,B,C</m>. (This is the only second-order problem we will solve. In fact,there aren't many more second-order problems that even can be solved with relatively little effort.) In order to cut down on the options in our explanations, we'll assume these constants are all real, and that <m>A\gt 0</m>. 
			</p>

			<p>
				It's going to be easy to lose track of the most basic fact of this equation: <em>the solutions are exponentials</em>. (Well, with an asterisk.) In fact plugging in <m>y=e^{st}</m> takes us to <m>(As^2+Bs+C)e^{st}=0</m>, which is obviously true if we can solve the <term>characteristic equation</term>
				<men xml:id="eq-so-char">
					As^2+Bs+C = 0
				</men>.
				As you know, there are two (counting multiplicity) and only two solutions for <m>s</m> in this equation. Let's call them <m>s_1</m> and <m>s_2</m>. I won't insult you by writing out the quadratic formula here.
			</p>

			<p>
				With two roots for the characteristic equation, the general null solution is
					<me> y = c_1 e^{s_1t} + c_2 e^{s_2t}</me>.
				There are two integration constants here, just what we need to satisfy two initial conditions. That is, unless <m>s_1=s_2</m>... 
			</p>
		</introduction>

	<subsection xml:id="so-cc-double">
		<title>Double root</title>
	  	<p>
			  Suppose <m>s_1=s_2</m> is a double root of <xref ref="eq-so-char"/>. This happens when <m>B^2=4AC</m>, and <m>s_1=-B/(2A)</m>. In this case our candidate for the general solution is inadequate, because we can add the two terms together and having <m>c_1+c_2</m> is no more general than having a single integration constant. Somehow we have to come up with a genuinely different second solution. It turns out that the job is done by <m>te^{s_1t}</m>. The proof is just a bunch of algebra: plug it in and show that it works. So in the double-root case our null solution is 
	 	 	<me> y = c_1 e^{s_1t} + c_2 t e^{s_1t}</me>.
	  		Honestly, this is the only unusual thing that happens in this entire section. Everything else is just exploring the consequences of the solutions, and naming the things that we find out.
		</p>
	</subsection>

	<subsection xml:id="so-cc-real">
	  <title>Distinct real roots</title>
	  <p>Our two roots of the characterstic equation <xref ref="eq-so-char"/> might be distinct real numbers. This happens when <m>B^2\gt 4AC</m>. There isn't a whole lot else to say. The null solution is, as already stated,
	  <me> y = c_1 e^{s_1t} + c_2 e^{s_2t}</me>.
	  </p>

	  <example>
	    <p>Consider <m>y''-y'-2y=0</m>. The characteristic equation <m>s^2-s-2=0</m> has roots <m>s_1=-1</m>, <m>s_2=2</m>. This gives the solution <m>y=c_1 e^{-t} + c_2 e^{2t}</m>.</p>
	  </example>

	  <!--p>Let's look at that quadratic formula after all:
	  <me> s_{1,2} = \frac{ -b \pm \sqrt{b^2-4ac} }{ 2a }</me>.
	  Suppose <m>b\lt 0</m>. Then at least the "+" root is positive, because <m>-b</m> is positive and then something is added to it. Now suppose <m>b\gt 0</m>. The "-" root is trivially negative. But the other "+" root is negative too, because <m>\sqrt{b^2-4ac}-->
	</subsection>

	<subsection xml:id="so-cc-complex">
	  <title>Complex roots</title>
	  <p>There is exactly one more possibility for our characteristic roots: they are complex numbers. This happens if <m>B^2\lt 4AC</m>. In this case the roots must be conjugates of one another: <m>s_1=\overline{s}_2</m>. We'll find it convenient to write them in cartesian form, as <m>r\pm i\omega</m>. Then our null solution is
	  <me> y = c_1 e^{s_1t} + c_2 e^{s_2t} = e^{rt}\left( c_1 e^{i\omega t} + c_2 e^{-i \omega t} \right)</me>.
	  This is a perfectly fine way to write the solution, but not the only way. If we use Euler's formula and rearrange, we get
	  <me> y = e^{rt}\left( C_1 \cos(\omega t) + C_2 \sin(\omega t) \right)</me>.
	  These are still exponential solutions, but in disguise. </p>
	</subsection>

	<subsection xml:id="so-cc-oscillators">
	  <title>Unforced oscillators</title>
	  <p>We're going to narrow things a bit here and require both <m>A\gt 0</m> and <m>C\gt 0</m>. Remember, this arises from Newton's Law to describe a restorative force. </p>

	  <p>What shall we make of the <m>By'</m> term here? We didn't have it with simple harmonic motion. Now is the time to look at that quadratic formula:
	  <me> s_{1,2} = \frac{ -B \pm \sqrt{B^2-4AC} }{ 2A }</me>.
	  Recall our three cases, depending on the sign of <m>B^2-4AC</m> (aka the discriminant). If this is negative, then the roots are complex with real part equal to <m>r=-B/2A</m>. The amplitude of the complex exponential solutions is <m>e^{rt}</m>. So these decay to zero if <m>B \gt 0</m> and grow unboundedly if <m>B\lt 0</m>.</p>

	  <p>In the case <m>B^2-4AC= 0</m>, we have the double root <m>s_1=-B/2A</m>, and we reach the same conclusions about the sign of <m>b</m> (exponential decay beats linear growth). Finally, if <m>B^2-4AC \gt 0</m>, it's not hard to conclude that the same things happen. Physically, the ODE term <m>By'</m> represents <term>damping</term> or dissipative loss of energy, but only if <m>B\gt 0</m>. It's not impossible to have <m>B\lt 0</m>, meaning energy is being pumped into the system, but it is much more rare.</p>

	  <p>It's instructive to do a thought experiment in which we increase <m>B</m> from zero while everything else is fixed. First, at <m>B=0</m>, we have the idealized situation of an <term>undamped oscillator</term>. This is back to simple harmonic motion, with perfectly sinusoidal results. The characteristic roots in this case are imaginary, <m>s_{1,2}=\pm i \omega_n=\pm i \sqrt{C/A}</m>, corresponding to purely oscillating complex exponentials.</p>

	  <p>If we increase <m>B</m> slightly, the roots will behave continuously. They go into the complex plane as conjugates with negative real part <m>-B/2A</m>. At this point it's handy to shift notation by renaming our constants:
	  <me>\frac{-B}{ 2A } \pm i \frac{ \sqrt{4AC-B^2} }{ 2A } = -p \pm i\omega_d</me>,
	  where <m>p=B/2A</m>, and <m>\omega_d^2 = \omega_n^2-p^2</m> defines a <term>damped frequency</term> that's a bit lower than the natural undamped frequency. The solutions oscillate at <m>\omega_d</m> and decay with exponential rate <m>-p</m>. This is called <term>underdamped oscillation</term>. </p>

	  <p>As <m>B</m> continues to increase, eventually our complex roots coalesce into the double real root <m>-B/2A</m>. Even though the mathematical expression for the solution changes here, the behavior is continuous; we continue to see exponential decay, but the oscillatory component has gone from very slow to nonexistent. This is called <term>critical damping</term>.</p>

	  <p>Finally, as <m>B</m> increases more our roots again separate but stay on the negative real axis. The solutions are all decaying real exponentials, and we call the situation <term>overdamped</term>. (This is what you want for your airplane's wings!)</p>

	  <p>The four types of oscillation can be predicted from the sign of <m>B^2-4AC</m>, or equivalently from the sign of <m>\omega_n-p</m>.</p>
	</subsection>

	<!--subsection xml:id="so-cc-impulse">
	  	<title>Impulse response</title>

	 	<p>Recall our claim that the fundamental solution <m>g(t)</m> of the impulse forcing problem <m>Ay''+By'+C=\delta(t)</m> leads to the particular response to any input <m>f(t)</m> via <m>y_p(t)=\int_0^t g(t-s)f(s)\,ds</m>. Also recall that we can fudge finding <m>g</m> by having no forcing, <m>g(0)=0</m>, and <m>g'(0)</m> becoming instantly nonzero. </p>
		
		<p>
			Consider an infinitesimally small time <m>\epsilon</m>. If we integrate <m>Ay''+By'+Cy=\delta(t)</m> up to this time, then we can estimate
	  		<me>A(g'(\epsilon)-g'(0)) + B(g(\epsilon)-g(0)) + C\int_0^\epsilon g(t)\,dt = 1 </me>.
	  		Assuming that <m>g</m> itself is continuous, as <m>\epsilon\to 0</m> we conclude that <m>g'(\epsilon)-g'(0) = 1/A</m>, which is the jump we seek. That is, we use <m>g'(0)=1/A</m> as the "cheater's way" to get the fundamental solution.  
		</p>

	  	<example>
			<p>
				The solution of <m>g''=\delta(t)</m> is equivalent to <m>g''=0</m> with <m>g(0)=0</m>, <m>g'(0)=1</m>. We have <m>g(t)=c_1+c_2t</m> and then conclude that <m>c_1=0</m>, <m>c_2=1</m>. That is, <m>g(t)=t</m>.
			</p>
			<p>
				Now suppose we want to solve <m>g''=\sin(t)</m>. We find
				<me>y_p(t)=\int_0^t (t-s) \sin(s) \,ds = \bigl[ (s-t)\cos(s) - \sin(s) \bigr]_0^t = -\sin(t)-t</me>. 
				This is not the same result we get by following the other methods of this section, which would suggest only <m>-\sin(t)</m>. The other <m>-t</m> coming from the integral formula is a part of the null solution, so both expressions are valid particular solutions. 
			</p>
	 	</example>
	</subsection-->
  </section>

  <section xml:id="so-forced-oscillators">
	<title>Exponentially forced oscillators</title>
	<introduction>
	<p><em>This and the next section cover 2.4 in the text.</em></p>
	  <p>We now consider problems in the form <m>Ay''+By'+Cy=f(t)</m>, where the coefficients are constant, <m>A</m> and <m>C</m> are positive (oscillation), and the driving force is exponential, <m>f(t)=e^{rt}</m> for constant <m>r</m>.</p>

	  <p> There are plenty of details to sift through, but the essence is quite simple. The general solution of this problem has two contributions, <m>y=y_n+y_p</m>. The first, <m>y_n</m>, is the null solution to the associated <term>homogeneous</term> equation <m>Ay''+By'+Cy=0</m>, and the second, <m>y_p</m>, is any particular solution of the original problem. To see how this works, plug in:
	  <md><mrow>Ay'' + By' + Cy \amp = A(y_n+y_p)'' + B(y_n+y_p)' + C(y_n+y_p) </mrow>
	  <mrow> \amp = (Ay_n''+By_n'+Cy_n) + (Ay_p''+By_p'+Cy_p) </mrow>
	  <mrow> \amp = 0 + f = f</mrow>
	  </md>.
	  The null solution contains two arbitrary constants that are used to soak up the initial conditions. We add that the particular solution is not unique, so the complete representation is not either. However, all such expressions are equivalent in the end, and the solution is always expressible in this form.</p>

	  <!--p>In the context of underdamped oscillators, <m>y_n</m> is also called the <term>transient solution</term>, because it decays to zero as time increases. What remains might be called the <term>steady response</term>. It depends only on the oscillator parameters and the forcing function; all dependence on initial conditions vanishes with the transient solution. </p-->

	  <p>We covered finding the null solution in detail in <xref ref="so-constant-coefficients"/>. So the particular solution is our first item of business.</p>
	</introduction>

	<subsection xml:id="so-fo-expresponse">
	  <title>Exponential response</title>
	  <p>For <m>f(t)=e^{rt}</m> we let <m>y_p=Y e^{rt}</m> and plug into the ODE:
	  <me>(Ar^2 + Br + C)Ye^{rt} = e^{rt}</me>.
	  This is identically satisfied if we choose
	  <me>Y = \frac{1}{Ar^2 + Br + C}</me>.
	  This is called the <term>transfer function</term>. It's constant in time, but a function of the forcing exponent <m>r</m>. In fact we'll switch to using  <m>s</m> in the transfer function (as we will see, it's all relatable to Laplace transforms). </p>

	  <example>
	    <p>Let's solve <m>y''+5y'+6y=4 e^{-t}</m>, with <m>y(0)=0</m>, <m>y'(0)=1</m>. In stages:
	    <ol>
	      <li><p><em>Null solution</em>.
	      The characteristic equation of <m>y''+5y'+6y=0</m> is <m>s^2+5s+6=0</m>, which has roots <m>s_1=-2</m>, <m>s_2=-3</m>. Thus <m>y_n=c_1 e^{-2t}+c_2e^{-3t}</m>. </p>
	      </li>
	      <li><p><em>Particular solution</em>.
	      The transfer function is <m>Y(s)=1/(s^2+5s+6)</m>, and <m>Y(-1)=1/2</m>. Hence <m>y_p=\frac{1}{2}\cdot 4 e^{-t}=2e^{-t}</m>. </p></li>
	      <li><p><em>Complete solution</em>.
	      <m>y=y_n+y_p=c_1 e^{-2t}+c_2e^{-3t} + 2 e^{-t}</m></p>
	      </li>
	      <li><p><em>Initial values</em>.
	      We get <m>0=y(0)=c_1+c_2+2</m> and <m>1=y'(0)=-2c_1-3c_2-2</m>. This implies <m>c_1=-3</m>, <m>c_2=1</m>. Finally, <m>y=-3 e^{-2t}+e^{-3t} + 2 e^{-t}</m>.</p>
	      </li>
	    </ol>
	    </p>
	  </example>

	  <p>You may have noticed that the denominator of the transfer function is simply the characteristic polynomial of the homogeneous problem. That is always the case. The textbook uses <m>Y(s)=1/P(s)</m> in many spots to emphasize the connection.</p>

	  <p>You should be getting comfortable with the idea that exponential forcing includes sin and cos! We are simply interested in the transfer function for imaginary exponents, <m>Y(i\omega)</m> followed by the imaginary or real part of <m>Ye^{i\omega t}</m>. There is some complex-number-fu required here. </p>

	  <example xml:id="ex-second-sin-forcing">
	    <p>Consider <m>y''+y'=\sin(2t)</m>. Since the forcing term is the imaginary part of <m>e^{2it}</m>, we use
	    <me>Y(2i) = \frac{1}{(2i)^2+2i} = \frac{1}{2i-4}</me>.
	    We need to take the imaginary part of <m>Y(2i)e^{2it}</m> to get the particular solution we seek. The course of least resistance is to make the denominator of <m>Y</m> real by using a complex conjugate:
	    <me> Y(2i) = \frac{1}{2i-4}\cdot \frac{2i+4}{2i+4} = \frac{2i+4}{4+16} = \frac{i+2}{10} = \frac{1}{5} + i\frac{1}{10}</me>.
	    There are <em>two</em> contributions to each of the real and imaginary parts of the product:
	    <md>
	      <mrow>Y(2i)e^{2it} \amp = \left(\frac{1}{5} + i\frac{1}{10}\right) \bigl( \cos(2t) + i \sin(2t) \bigr)</mrow>
	      <mrow> \amp = \Bigl[ \cdots \Bigr] + i \Bigl[ \frac{1}{5}\sin(2t) + \frac{1}{10}\cos(2t) \Bigr]</mrow>
	      </md>.
	    (We don't need the real part, inside the first set of brackets.) Thus <m>y_p = [2\sin(2t)+\cos(2t)]/10</m>. </p>

	    <p>In retrospect we might have come at this problem from an entirely real standpoint. We could posit <m>y_p=A\cos(2t) + B\sin(2t)</m> and then plug in to get some algebraic conditions to determine <m>A</m> and <m>B</m>. There's still a fair amount of algebra and plenty of chances to make silly errors (see the derivation of formula (23) in section 2.4 of the textbook). I recommend that you give the complex method a fair try before retreating to the familiarity of the real-only method. </p>
	  </example>
	</subsection>

	<subsection xml:id="so-fo-resonance">
	  <title>Resonance</title>
	  <p>Time for some deja vu. When we solved <m>y'-at=e^{ct}</m> in <xref ref="flcs-exp"/>, we found that we got different formulas in the case <m>c=a</m>. We also needed a special formula for <m>y_n</m> when we got a double root in <xref ref="so-cc-double"/> The same sort of thing happens to <m>y_p</m> if we force an oscillator at one of the characteristic exponents. The reason is simple: if we try <m>y_p=Ye^{rt}</m> and <m>r</m> is a characteristic root, then we get <m>0=f(t)</m>, which can't be solved. Equivalently, the transfer function <m>Y</m> requires taking <m>1/0</m>. </p>

	  <p>Suppose we have a root <m>s</m> and that <m>r</m> is close to but not equal to <m>s</m>. Then <m>y_p=e^{rt}/P(r)</m> is a particular solution to the exponential forcing. We can add any null solution to this and it will remain a solution, thus
	  <me>y=\frac{e^{rt}-e^{st}}{P(r)}</me>
	  is a valid particular solution as well. Since <m>P(s)=0</m> by assumption, we can also write
	  <me>y=\frac{e^{rt}-e^{st}}{P(r)-P(s)}</me>.
	  Suddenly this becomes very interesting as <m>r\to s</m>; we get an indeterminate <m>0/0</m>. Using L'Hopital's rule (but with <m>r</m> as the variable, not <m>t</m>), we get the limit
	  <me>y_p = \frac{te^{st}}{P'(s)}</me>,
	  which is a valid particular solution even when forcing exactly at exponent <m>s</m>. </p>

	  <p>Realistically we only care about this effect for harmonic forcing, <m>f(t)=e^{i\omega t}</m>, where we call it <term>resonance</term>. Since the characteristic polynomial <m>P(s)=as^2+bs+c</m> has imaginary roots only if <m>b=0</m>, we only see resonance in an undamped oscillator. The resonant particular solution has an amplitude that is linearly growing in time. The derivation above shows that though the solution formula (and unboundedness) is singular at the resonant frequency, the actual behavior is a continuous limit of near-resonant behavior. </p>
	</subsection>

	<subsection xml:id="so-fo-gain-lag">
	  <title>Gain and lag</title>
	  <p>By now we have seen multiple times that forcing with <m>\cos(\omega t)</m> or <m>\sin(\omega t)</m> leads to solutions with a combination of both terms. We also know that such a result is equivalent to a shifted cosine in amplitude/phase form, and we turn our attention to this interpretation next.</p>

	  <p>With forcing <m>f(t)=e^{i\omega t}</m> we get a solution <m>y_p=Y(i\omega) e^{i\omega t}</m>. If we write the transfer function in polar form, we can express this as <m>y_p = (G e^{-i\alpha }) e^{i\omega t} = Ge^{i(\omega t - \alpha)}</m> for positive amplitude <m>G</m>, which we call the <term>gain</term>. It depends on the forcing frequency; in fact, <m>G(\omega) = |Y(i\omega)|</m>. The <m>\alpha</m> term is a <term>phase lag</term> that shows how the response may lag behind the driving force. It's usually not as interesting as the gain.  </p>

	  <example>
	    <p>The oscillator <m>4y'' + y' + 36y</m> has transfer function <m>Y(s)=1/(4s^2+s+36)</m>. At <m>s=i\omega</m> we get
	    <me>Y(i\omega) = \frac{1}{ -4\omega^2 +i\omega + 36}</me>.
	    We can compute the gain using a complex conjugate.
	    <me> G^2 = |Y|^2 = \frac{1}{ 36-4\omega^2 +i\omega} \frac{1}{ 36-4\omega^2 - i\omega} = \frac{1}{(36-4\omega^2)^2+\omega^2}</me>.
	    The gain is maximized when the denominator is minimized. This happens when <m>2 (36-4\omega^2)(-8\omega)+2\omega = 0</m>, or <m>\omega^2=9-(1/32)</m>. This is just slightly smaller than the natural undamped frequency <m>\omega_n^2=36/4=9</m>.</p>
	  </example>

	  <p>In the undamped limit, resonance produces an infinite gain. Damping reduces the peak and the maximal frequency, at first gradually and then dramatically.</p>

	</subsection>
  
  </section>

  <section xml:id="so-higher-order">
	<title>Higher-order ODEs</title>
	<introduction>
	<p><em>From 2.4 in the text.</em></p>

	<p>First-order and second-order ODEs dominate the landscape when it comes to scientific applications. But once in a while we go to higher order. For linear problems with constant coefficients, this looks a lot like the second-order case.</p>
	</introduction>

	<subsection>
		<title>Solving linear high-order ODEs</title>
	<p>
		Any solution of the linear ODE
		<me>a_N \frac{d^Ny}{dt^N} + \cdots + a_2 \frac{d^2y}{dt^2} + a_1 \frac{dy}{dt}  + a_0y = f(t)</me>,
		where the <m>a_N,\ldots,a_0</m> are all constants, can be expressed as <m>y=y_n+y_p</m>, the sum of a null part <m>y_n</m> that solves the problem when <m>f(t)\equiv 0</m> and a particular solution <m>y_p</m>. 
	</p>
	<p>An exponential <m>e^{st}</m> is a null solution if <m>s</m> is a root of the characteristic polynomial
		<me>P(s) = a_N s^N + \cdots + a_1 s  + a_0</me>. 
		If the roots <m>s_1,\ldots,s_N</m> are all distinct, then every null solution has the form 
		<me>y_n = c_1 e^{s_1t} + c_2 e^{s_2t} + \cdots + c_N e^{s_Nt}</me>. 
		(Repeated roots are handled by using powers of <m>t</m> as multipliers of repeated terms; we do not give the details.) 
	</p>
	<p>
		If <m>f(t)=e^{rt}</m>, then a particular solution is <m>y_p=Ye^{rt}</m> for the transfer function <m>Y(r)=1/P(r)</m>. 
	</p>

	<example>
		<p>The ODE <m>y'''+2y''-y'-2y=4 e^{2t}</m> has characteristic polynomial <m>P(s)=s^2+2s^2-s-2</m>. The roots of <m>P</m> are <m>-2,-1,1</m>. So the general null solution is 
		<me>y_n = c_1 e^{-2t} + c_2 e^{-t} + c_3 e^{2t}</me>. 
		The transfer function is evaluated at <m>r=2</m> to get <m>Y(2)=1/P(2)=1/12</m>. So the complete solution is 
		<me>y = c_1 e^{-2t} + c_2 e^{-t} + c_3 e^{2t} + \frac{1}{3}e^{2t}</me>. 
		We would need three initial conditions in order to specify the integration constants uniquely.		
		</p>
	</example>

	<p>Factoring a polynomial of degree three or higher requires luck or a computer. Only a relative few special problems yield simple closed answers.</p>

	<example>
		<p>The characteristic polynomial of <m>y''''-y=0</m> is <m>s^4-1</m>. The roots are the "fourth roots of unity," given by <m>s_j=\exp(2\pi i j/4)</m> for <m>j=0,1,2,3</m>. These simplify to <m>\pm 1, \pm i</m>. In real form the null solution is 
		<me> y_n = c_1 e^{-t} + c_2 e^{t} + c_3 \cos(t) + c_4\sin(t)</me>. 
		</p>
	</example>
	</subsection>

	<subsection>
		<title>Linear operator notation</title>
		<p>
			You're familiar with a function as a rule mapping a value (number) to another value. A rule for mapping functions to other functions is called an <term>operator</term>. The first nontrivial operator you ever encountered was differentiation, the map from <m>u(x)</m> to <m>u'(x)</m>. We sometimes write this operator as a capital D: <m>u'=D[u]</m>. (Many times we leave out the square brackets, too.)
		 </p>
		 <p>
			 One of the most salient and familiar facts about the differentiation operator is that it is <term>linear</term>. By this we mean that it obeys two rules:
			 <ul>
				 <li><p><m>D[ku] = kD[u]</m> if <m>k</m> is any constant, and</p> </li>
				 <li><p><m>D[u+v] = D[u]+D[v]</m> if <m>u,v</m> are any functions.</p></li>
			 </ul>
			 We use <m>D^k</m> to refer to the operation of taking the <m>k</m>th derivative.
			</p>
			<p>
			 Given a linear ODE of order <m>N</m> as above, we can define 
			 <me>L[y] = a_N D^N y + \cdots  + a_1 Dy + a_0 y</me>.   
			This makes it much easier to write the ODE, as <m>L[y]=f</m>. Furthermore, it's easy to check that <m>L</m> satisfies the two rules above and thus is a linear operator as well. The linearity also makes certain kinds of reasoning very simple, such as our most basic pattern for general solutions:  
			<me>
				L[y_n+y_p] = L[y_n]+L[y_p] = 0+f=f
			</me>. 
		 </p>

		 <p>
			 An interesting and sometimes useful consequence of the defitions is that if <m>L</m> has constant coefficients and the characteristic polynomial factors as <m>P(s)=(s-s_1)\cdots(s-s_N)</m>, then <m>L=(D-s_1)\cdots (D-s_N)</m>. Here the juxtaposition means operator compositions.
		 </p>

		 <example>
			 <p>
				 The operator <m>L[y]=y'''+2y''+y'+2y</m> has characteristic roots <m>-2,\pm i</m>. Now note that for any function <m>y</m>,
				 <md>
					 <mrow> (D+2)(D-i)(D+i)y \amp = (D+2)(D-i)(y'+iy) </mrow>
					 <mrow> \amp = (D+2)\Bigl((y''+iy')-i(y'+iy)\Bigr) </mrow>
					 <mrow> \amp = (D+2)(y''+y) = y''' +y' +2y'' + 2y = L[y]</mrow>
				 </md>. We can write the factors in any order, and even things like <m>L[y]=(D^2+1)(D-2)</m>.
			 </p>
		 </example>
	</subsection>

  </section>

  <section xml:id="so-applications">
	<title>Applications of oscillators</title>
	<introduction>
		<p><em>From 2.5 in the text, but very little from the electrical side. Also go back to "better notation" from 2.4.</em></p>

		<p>Oscillations appear all over science and engineering, and 2nd order linear equations are often used to model them. The best-known examples are mechanical and electrical, but there are chemical and biochemical ones as well.</p>
	</introduction>
	

		<subsection>
			<title>Mechanical oscillators</title>
			<p>
				As we have already seen, a mass hanging from a spring satisfies the ODE
				<me>my'' + b y' + ky = f(t)</me>.
				(As is typical, all the named constants are nonnegative.) Here <m>m</m>> is the mass, <m>b</m>>is a coefficient of friction or other damping forces, and <m>k</m>> is a characteristic constant of the spring. The term <m>f(t)</m> represents an external driving force. If we have initial conditions, they give values for <m>y</m> and <m>y'</m>.
			</p>

			<p>
				The gravitational force on a pendulum bob (think child on a swing) has a related equation. The proper ODE is 
				<me>\theta'' + \gamma \theta + \frac{g}{L}\sin \theta = f(t)</me>,
				where <m>\theta(t)</m> is the angle made with vertical-down and <m>L</m> is the arm length. This equation is nonlinear. But if the angle remains small, then a reasonable approximation is 
				<me>\theta'' + \gamma \theta + \frac{g}{L}\theta = f(t)</me>, 
				which is a linear oscillator. 
			</p>
		</subsection>

		<subsection>
			<title>Electrical oscillators</title>
			<p>
				An AC circuit often has elements of resistance, capacitance, and inductance. These analogize perfectly to friction/damping, spring constant, and mass. If these elements are wired in series with a generator, the governing ODE is
				<me>LI'' + RI' + CI = \mathcal{E}'(t)</me>,
				where <m>L</m> is inductance, <m>R</m> is resistance, <m>C</m> is capacitance, and <m>I(t)</m> is the current flowing through the circuit. The forcing term represents the generator whose voltage (EMF) is <m>\mathcal{E}(t)</m>. For alternating current this is modeled as a cosine (or complex exponential) function.  
			</p>
			<p>
				In DC circuits we have Ohm's Law, <m>I=V/R</m>, for a resistance <m>R</m>. In the AC case, if <m>\mathcal{E}(t)=Ve^{i\omega t}</m>, then <m>e^{i\omega t}</m> appears in the current as well, and it still makes sense to talk about <m>V/I</m> as a kind of resistance, now called <term>impedance</term>, written as <m>Z</m>, and dependent on the frequency <m>\omega</m>. The impedance is very closely related to the transfer function for <m>s=i\omega</m>. 
			</p>
		</subsection>
      
		<subsection xml:id="so-ap-notation">
	  <title>Unifying notation</title>

	  <p>When you have lots of different versions of the same root problem, with different symbols and units, you have three options. You can solve each new problem from scratch, you can derive custom formulas for each application, or you can try to find a minimal set of parameters and express each problem using those. Let's consider the last pathway here.</p>

	  <p>
			The mechanical oscillator <m>Ay'' + By' + Cy = f(t)</m> is more conveniently expressed as
	  	<me> y'' + \frac{B}{A} y' + \frac{C}{A} y + \frac{C}{A} F(t)</me>.
			One reason to prefer this form is that <m>y</m> must have the same units as the forcing <m>F</m>. Furthermore, we see the undamped natural frequency <m>\omega_n=\sqrt{C/A}</m>, and this has units of inverse time only. The same units apply to <m>B/A</m>, so we introduce the new dimensionless parameter
			<me>Z = \frac{1}{2} \frac{B/A}{\omega_n} = \frac{B}{\sqrt{4AC}}</me>. 
			Now the ODE becomes 
			<me> y'' + 2 Z \omega_n y' + \omega_n^2 y = \omega_n^2 F(t)</me>.
		</p>
		<p>
			The characteristic roots are now a bit more transparent: <m>-Z \omega_n \pm \omega_n \sqrt{Z^2-1}</m>. We see that the condition for being underdamped is simply <m>Z \lt 1</m>. In this case we express the roots as <m>-Z \omega_n \pm i \omega_d</m>, using the damped frequency <m>\omega_d=\omega_n\sqrt{1-Z^2}</m>. 
		</p>
		<p>
			Continuing the underdamped case, the gain works out to be
	  	<me>G^2 = \frac{1}{(\omega_n^2-\omega^2)^2 + 4\omega_n^2Z^2\omega^2}  = \frac{\omega_n^{-4}}{(1-\rho^2)^2 + 4Z^2\rho^2}</me>,
			where <m>\rho=\omega/\omega_n</m> is a dimensionless frequency ratio. If also <m>Z^2\lt 1/2</m>, the gain is maximized when <m>\rho=\sqrt{1-2Z^2} \lt 1</m>. The gain at that maximum is proportional to <m>1/Z</m>, so much growth in the response is possible near the natural frequency when damping is small in the sense of <m>Z\approx 0</m>. This can be considered a practical resonance for lightly damped systems. 
		</p>

	  <example>
	    <p>The oscillator <m>4y'' + y' + 36y</m> from an earlier example has <m>\omega_n=3</m> and <m>Z=1/\sqrt{16\cdot 36}=1/24</m>. Accordingly, gain is maximized when <m>\omega/\omega_n=1-\frac{1}{288}\approx 0.9965</m>, so for practical purposes a resonance occurs at just slightly less than the natural, undamped frequency.
			</p>
		</example>

	</subsection>
	
	</section>

  <section xml:id="so-general-forcing">
	<title>Other forcing functions</title>
	<introduction>
		<p><em>From 2.6 in the text.</em></p>

		<p>
			Exponential forcing functions are very important, especially when allowing complex exponents. But they aren't the whole world. In this section we go over two other major techniques for trying to get particular solutions to linear, 2nd-order ODEs. Both assume that you have access to the complete null solution already. 
		</p>
	</introduction>
	
		<subsection xml:id="so-gf-muc">
			<title>Undetermined coefficients</title>
			<p>In the method of undetermined coefficients we assume that we already know everything about the particular solution except for some constants to be found using algebra. Best to start with examples.</p>

			<example>
				<p>Let's find a particular solution of <m>y'' +4y'+4y=8t^2</m>. We reason as follows. If we plug in some <m>Ct^2</m> for <m>y</m> with a constant <m>C</m>, then all the derivatives generate multiples of <m>t</m> and <m>1</m>. That is, both sides of the ODE will be quadratic polynomials. So we generalize just a bit to allow 
				<me>y_p = A + Bt + Ct^2</me>. 
				Plugging that into the ODE yields
				<md>
					<mrow>2C + 4(B+2Ct) + 4(A+Bt+Ct^2) \amp = 8t^2</mrow>
					<mrow>4C t^2 + (8C+4B) + (2C+4B+4A) \amp = 8t^2</mrow>
				</md>. This has to be an identity for all <m>t</m>. Matching like powers, we can read off <m>C=2</m>, then <m>B=-4</m>, and finally <m>A=3</m>. This provides us with <m>y_p=2t^2+4t+3</m>. (The complete solution would require us to use the characteristic equation to find the null solution as well.)
				</p>
			</example>

			<p>
				In general, if <m>f(t)</m> is a polynomial, then we let <m>y_p</m> be a polynomial of the same degree with unknown coefficients. Substitution into the ODE and matching powers gives equations to determine those. As in the example above, you must put <em>all</em> the polynomial terms into <m>y_p</m>, even if they are missing from <m>f(t)</m>.  
			</p>

			<p>The next example has a polynomial times an exponential.</p>

			<example>
				<p>Let's try for <m>y'' -2y'-3y=10te^{4t}</m>. For much the same reasons as in the last case, we try 
				<me>y_p = (A + Bt)e^{4t}</me>. There's nothing to do now except grind out some derivatives. 
				<md>
					<mrow>8(2A+B+2Bt)e^{4t} - 2(4A+B+4Bt)e^{4t} -3 (A + Bt)e^{4t} \amp =10t e^{4t}</mrow>
					<mrow> 8(2A+B) - 2(4A+B) -3A + t(16B-8B-3B) =  10t</mrow>
				</md>. From this, by matching powers, we deduce <m>B=2</m> and then <m>5A+12=0</m>. So <m>y_p = [2t - (12/5)]e^{4t}</m>.
				</p>
			</example>

			<p>
				So if <m>f(t)</m> is a polynomial times <m>e^{st}</m>, let <m>y_p</m> be the same exponential times a polynomial of the same degree with unknown coefficients. This technically applies too if the polynomial has degree zero, which was our mainstay in <xref ref="so-forced-oscillators"/>. It also applies for imaginary or complex exponents.
			</p>

			<p>
				The third major type of forcing amenable to UC is sin or cos. We did this type of forcing in <xref ref="ex-second-sin-forcing"/> by converting the forcing to a complex exponential. Here is how it plays out with UC instead.
			</p>

			<example>
				<p>
					Consider <m>y''+y'=\sin(2t)</m>. Derivatives of <m>sin(2t)</m> procuce only itself and <m>cos(2t)</m>, and we are justified in choosing
					<me>
						y_p = a\cos(2t) + b\sin(2t)
					</me>. 
					Inserting this into the ODE leads to 
					<me>
						[-4a\cos(2t) - 4b\sin(2t)] + [-2a\sin(2t) + 2b\cos(2t)] = \sin(2t)
					</me>. 
					This is true for all time if and only if we match the coefficients of the trig functions,
					<me>
						-4a + 2b = 0, \qquad -4b-2a = 1 
					</me>. 
					The solution of these equations is <m>a=-1/10</m>, <m>b=-1/5</m>. So <m>-[\cos(2t)+2\sin(2t)]/10</m> is a particular solution.
				</p>
			</example>
			<p>
				The method can be described when combining all of the above: forcing by sine or cosine, multiplied by a polynomial and/or exponential. I believe a term like <m>e^{\alpha t}\cos(\beta t)</m> is more easily treated as a complex exponential, and the whole process is usually best left to a computer if more than two unknown coefficients are needed.  
			</p>
			<p>
				There is one more important situation to be aware of, another variation on a familiar theme. If your <m>y_p</m> is part of the null solution of the ODE, you have to "promote" it by powers of <m>t</m> until it no longer overlaps. 
			</p>

			<example>
				<p>
					Given <m>y'' -2y'-3y=4e^{-t}</m>, we are led to try <m>y_p=Ae^{-t}</m>: 
					<md> 
						<mrow>(Ae^{-t}) - 2(-Ae^{-t}) - 3Ae^{-t} \amp = 4e^{-t} </mrow>
						<mrow> 0 \amp 4e^{-t} </mrow>
					</md>. This is impossible to satisfy. What's happened is that the characteristic polynomial is <m>(s+1)(s-3)</m>, so <m>e^{-t}</m> is part of the null solution (as the above calculation verified). Instead we have to use <m>y_p=Ate^{-t}</m>, which gives 
					<md> 
						<mrow> Ae^{-t}(t-2) - 2Ae^{-t}(1-t) - 3Ate^{-t} \amp = 4e^{-t} </mrow>
						<mrow> -4Ae^{-t} \amp 4e^{-t} </mrow>
					</md>, so <m>A=1</m> and <m>y_p</m> is valid.
				</p>
			</example>
		
		</subsection>

		<subsection xml:id="so-gf-vp">
		<title>Variation of parameters</title>
		<p>
			While UC relies on "educated guesswork" followed by derivatives and algebra, <term>variation of parameters</term> follows a simpler recipe but requires integration. Given <m>L[y]=f</m>, suppose we have a general null solution in the form 
			<me>y_n(t) = c_1 y_1(t) + c_2 y_2(t)</me>.
			For problems with constant coefficients, <m>y_1</m> and <m>y_2</m> are exponentials (or sin/cos equivalents), possibly multiplied by <m>t</m>. In any case, the VP trick is to replace the integration constants with functions of time:
			<me>y_p(t) = c_1(t) y_1(t) + c_2(t) y_2(t)</me>. 
			To keep notation simpler, we assume below that the coefficient of <m>y''</m> in the ODE is 1. 
		</p>

		<p>
			Taking this odd assumption on faith for now, we compute 
			<me> 
				y_p' = c_1'y_1+c_1y_1'+ c_2'y_2+c_2y_2'  
			</me>. 
			Before we go on, we're going to put a restriction on our mystery functions, namely <m>c_1'y_1+c_2'y_2=0</m>. That makes life a little easier. Continuing, we get
			<me>
				y_p'' = c_1'y_1'+ c_1y_1'' + c_2'y_2'+ c_2y_2''  
			</me>.
			When we put all this mess into <m>y''+By'+Cy=f</m>, something cool happens: 
			<me> 
				c_1(y_1''+By_1'+Cy_1) + c_2(y_2''+By_2'+Cy_2) + c_1'y_1' + c_2'y_2' = f 
			</me>. Thanks to the null property of <m>y_1</m> and <m>y_2</m>, this boils down to <m>c_1'y_1' + c_2'y_2' = f</m>. 
		</p>

		<p>
			To summarize so far, we get a valid particular solution using two conditions, 
			<md>
				<mrow>c_1'y_1+c_2'y_2 \amp = 0 </mrow>
				<mrow> c_1'y_1' + c_2'y_2' \amp = f </mrow>
			</md>. 
			Stare at this long enough and you'll realize that it's just two linear equations for <m>c_1'</m> and <m>c_2'</m>. You can do some elimination to solve them (more on this in future chapters), and you wind up with 
			<me>c_1' = -\frac{y_2f}{W}, \qquad c_2' = \frac{y_1f}{W},</me>
			with 
			<me>W(t) = y_1y_2'-y_1'y_2</me>,
			known as the <term>Wronskian</term> of <m>y_1</m> and <m>y_2</m>. It's easiest to remember as a determinant:
			<me>
				W = \begin{vmatrix} 
				y_1 \amp y_2 \\ y_1' \amp y_2' 
				\end{vmatrix}
			</me>.
			Finally, "all" you have to do is integrate these expressions to get <m>c_1</m> and <m>c_2</m>, and from there you get <m>y_p</m>. 
		</p>

		<example>
			<p>
				Let's apply VP to <m>y''-y=1</m>. The null solutions are <m>y_1=e^{-t}</m> and <m>y_2=e^t</m>, which implies <m>W=1+1=2</m>. Then 
				<me> 
					c_1' = -\frac{y_2f}{W} = -\frac{1}{2}e^t, \qquad 
					c_2' = \frac{y_1f}{W} = \frac{1}{2}e^{-t}
				</me>. 
				Now we integrate.
				<me> 
					c_1(t) = -\frac{1}{2} \int_0^t e^s \, ds = \frac{1}{2}(1-e^{t}), \qquad 
					c_2(t) = \frac{1}{2} \int_0^t e^{-s} \, ds = \frac{1}{2}(1-e^{-t})
				</me>. 
				In fact these are determined only up to integration constants (because particular solutions aren't unique), so we can drop the extra constants and get 
				<me> 
					y_p = -\frac{1}{2}e^t \cdot e^{-t} - \frac{1}{2} e^{-t} \cdot e^t = -1
				</me>. 
				This was a lot more difficult than UC on the same problem. However, now consider solving <m>y''-y=H(t-T)</m> for a step time <m>T \gt 0</m>. The VP equations are now <m>c_1' = -\frac{1}{2}e^tH(t-T)</m>, <m>c_2'=\frac{1}{2}e^{-t}H(t-T)</m>. From here we get 
				<me> 
					c_1(t) = \int_0^T -\frac{1}{2}e^s H(s-T)\, ds = -\frac{1}{2} H(t-T) \int_T^t e^s\, ds = \frac{1}{2} H(t-T) [e^T - e^t]
				</me>
				and, similarly, <m>c_2(t) = \frac{1}{2} H(t-T) [e^{-T}-e^{-t}]</m>.  This leads to 
				<me> 
					y_p = \frac{1}{2} H(t-T) \left[ e^{T-t} - 1 + e^{t-T} - 1 \right]
				</me>, 
				which is a lot less obvious in the UC method. 
			</p>
		</example>
		<p>
			If you immediately wondered "what if the Wronskian is zero?" when you saw the VP formulas above, you have earned a special place in my heart. This possibility can be ruled out for some rather general theortical reasons, but we can also just calculate away the issue for equations with constant coefficients. If we have distinct characteristic roots <m>s_1</m> and <m>s_2</m>, then 
			<me>
				W = (e^{s_1t})(s_2e^{s_2t}) - (s_1e^{s_1t})(e^{s_2t}) = (s_2-s_1)e^{(s_1+s_2)t}
			</me>. 
			Never zero, as promised. In the case of repeated root <m>s_1</m>, then
			<me>
				W = e^{s_1t}(s_1t +1)e^{s_1t} - (s_1 e^{s_1t})(t e^{s_1t}) = e^{2s_1t} 
			</me>. 
			And here you were all worried. 
		</p>
		</subsection>

		<subsection>
			<title>Fundamental solutions, again</title>
			<p>
				If we continue to push ahead with the VP formulas, interesting things start to happen. For distinct roots,
				<me>
					c_1' = -\frac{\exp(s_2t)}{(s_2-s_1)\exp((s_1+s_2)t)} f(t)  =  \frac{f(t)e^{-s_1t}}{s_1-s_2}
				</me>,
				and similarly <m>c_2'= f(t)e^{-s_2t}/(s_2-s_1) </m>. Then 
				<md>
					<mrow>
					y_p(t) \amp = e^{s_1 t} \int_0^t \frac{f(\tau)e^{-s_1\tau}}{s_1-s_2}\, d\tau + 
					e^{s_2 t} \int_0^t \frac{f(\tau)e^{-s_2\tau}}{s_2-s_1}\, d\tau </mrow>
					<mrow> \amp =  \int_0^t  f(\tau) \frac{  e^{s_1(t-\tau)} -  e^{s_2(t-\tau)}}{s_1-s_2}\, d\tau  </mrow>
					<mrow> \amp =  \int_0^t  f(\tau) g(t-\tau) \, d\tau  </mrow>
				</md>, with
				<me>g(t) = \frac{ e^{s_1t} -  e^{s_2t} }{s_1-s_2} </me>. 
				For the case of a repeated root we can go though the VP formulas again, or just let <m>s_2\to s_1</m> and get
				<me>g(t) = t e^{s_1t} </me>. 
			</p>
			<p> 
				The formula <m>\int_0^t  f(\tau) g(t-\tau) \, d\tau</m> is reminiscent of the growth factor formula we got in first-order equations, which had <m>\int_0^t G(s,t)f(s)\, ds</m>, with <m>G(s,t)=\exp[a(t-s)]</m> for constant coefficients. Since we are no longer concerned only with growth processes, we now refer to such a <m>G</m> or <m>g</m> as a <term>fundamental solution</term> or <term>Green function</term>. 			
			</p>
			<p>
				If we use an impulse forcing function <m>f(t)=\delta(t)</m> in the integral formula for <m>y_p</m>, then we get simply <m>y_p(t)=g(t)</m>. So the fundamental solution is also the impulse response. It's easy to see from the formula for <m>g</m> that <m>g(0)=0</m> and <m>g'(0)=1</m>; that is, the impulse forcing is equivalent to an instantaneous jump in <m>y'</m>. 
			</p>
			<p>
				
			</p>

		</subsection>
  </section>

  <section xml:id="so-laplace">
	<title>Laplace transform methods</title>
	<introduction>
	<p><em>This and the next section are from 2.7 in the text, but much of that is stuff we did in the previous chapter.</em></p>

	<p>
		The correspondence between <m>L[y]=Ay''+By'+Cy</m> and <m>P(s)=As^2+Bs+C</m> is by now familiar. It came by way of assuming <m>y(t)=Y(s)e^{st}</m>, from which we get <m>Y(s)=1/P(s)</m>. Factoring <m>P</m> tells us what we need to know. For example, if its roots are <m>-2</m> and <m>-4</m>, then we know that <m>y(t)</m> is a combination of <m>e^{-2t}</m> and <m>e^{-4t}</m>. The only mystery to work out is the coefficients. 
	</p>

	<p>
		We can use the method of Laplace transforms to systematize the manipulations. As in first-order equations, these are useful mostly for linear, constant-coefficient problems. 
	</p>
	</introduction>

	<subsection>
		<title>Second derivatives</title>
		<p>
			Recall that the transform of <m>y'(t)</m> is <m>sY(s)-y(0)</m>. Applying this formula a second time shows that 
			<me>
				\mathcal{L}[y''(t)] = s \mathcal{L}[y'(t)] - y'(0) = s^2Y(s) - sy(0) - y'(0)
			</me>.
		</p>
		<p>
			Let's break down what happens in a fundamental case, <m>y''-y=e^{rt}</m>.  Transforming both sides gives 
			<me> 
				[s^2Y(s) - sy(0)-y'(0)] - Y(s) = 1/(s-r)
			</me>. 
			For reasons to be clarified later, let's skip to the "very particular" part with zero initial conditions, for which
			<me>
				Y(s) = \frac{1}{(s^2-1)(s-r)} = \frac{1}{(s+1)(s-1)(s-r)}
			</me>. 
			This time there are three poles. Our mnemonic still works here,
			<me> 
				\frac{1}{(s+1)(s-1)(s-r)} = \frac{1}{(s+1)(-1-1)(-1-r)} + \frac{1}{(1+1)(s-1)(1-r)}  + \frac{1}{(r+1)(r-1)(s-r)} 
			</me>. 
			Note that this requires <m>r\neq -1</m> and <m>r \neq 1</m> (i.e., no resonance). From here we get three exponentials:
			<me>
				y_v(t) = \frac{1}{2(1+r)}e^{-t} + \frac{1}{2(1-r)}e^{t} + \frac{1}{r^2-1}e^{rt}
			</me>. 
		</p>
		<p>
			Now we can write out a general solution. The characteristic roots are <m>\pm 1</m>, so
			<me>y(t) = c_1 e^{-t} + c_2 e^{t} + y_v(t)</me>. 
			If desired, it's easy to solve for the constants in this, because <m>y_v</m> contributes nothing:
			<md>
				<mrow>y(0) \amp = c_1 + c_2  </mrow>
				<mrow>y'(0) \amp = -c_1 + c_2 </mrow>
			</md>. 
			That's a lot less work than doing a separate PFD needed when the initial conditions are kept in the transform. That's why it often pays to just ignore those parts when using the transform method, putting them back in at the very end. 
		</p>

		
	</subsection>

	<subsection> 
		<title>Imaginary poles</title>
		<p>
			If we transform <m>e^{i\omega t}</m>, we can take the real and imaginary parts to get some new formulas:
			<me>
				{\cal L}[\cos(\omega t)] = \text{Re}\left( \frac{1}{s-i\omega} \frac{s+i\omega}{s+i\omega} \right) = \frac{s}{s^2+\omega^2}
			</me>,
			and 
			<me>
				{\cal L}[\sin(\omega t)] = \text{Im}\left( \frac{1}{s-i\omega} \frac{s+i\omega}{s+i\omega} \right) = \frac{\omega}{s^2+\omega^2}
			</me>.
			Conversely, a transform <m>Y(s)</m> that has <m>s^2+\omega^2</m> as a factor in the denominator has poles at <m>\pm i\omega</m>. The simplest case is 
			<me>
				Y(s) = \frac{A+Bs}{s^2+\omega^2}  \quad \Rightarrow \quad y(t)=(A/\omega)\sin(\omega t) + B\cos(\omega t)
			</me>. 
		</p>
		<example>
			<p>
				Let's solve <m>y''+9y=\delta(t-2)</m>. A particular solution has transform
				<me>
					Y(s) = \frac{e^{-2s}}{s^2+9} = e^{-2s} F(s)
				</me>,
				where <m>F(s)</m> is the transform of <m>\sin(3t)/3</m>. So the shift theorem implies
				<me>
					y(t) = \frac{1}{3} H(t-2) \sin(3t-6)
				</me>. 
			</p>
		</example>


		<p>
			Things are not too bad if there is an additional real pole: 
			<me>
				Y(s) = \frac{A+Bs}{s^2+\omega^2} + \frac{C}{s-c} \quad y(t)=(A/\omega)\sin(\omega t) + B\cos(\omega t) + C e^{ct}
			</me>. 
			Usually there is some grunt work involved in finding the values of <m>A</m>, <m>B</m>, and <m>C</m>. 	
		</p>
		<example>
			<p>
				The undamped oscillator <m>y''+16y=40e^{-2t}</m> has a solution whose transform is 
				<me>
					Y(s) = \frac{40}{(s+2)(s^2+16)} = \frac{A+Bs}{s^2+16^2} + \frac{C}{s+2}
				</me>. 
				Clearing the denominators gives the identity
				<me>
					(A+Bs)(s+2) + C(s^2+16) = 40
				</me>. 
				We need three equations to determine the unknown coefficients. Setting <m>s=-2</m> quickly gives <m>20C=40</m>, so <m>C=2</m>. Then setting <m>s=\pm 4</m> gives the system
				<md>
					<mrow>(A+4B)(6) \amp = 40 - 64 = -24</mrow>
					<mrow>(A-4B)(-2) \amp = -24</mrow>
				</md>,
				or <m>A+4B=-4</m> and <m>A-4B=12</m>. We can add and subtract these equations to get <m>2A=8</m>, <m>8B=-16</m>, so finally 
				<me>
					Y(s) = \frac{4-2s}{s^2+16^2} + \frac{2}{s+2} 
				</me>. 
				Hence
				<me>
					y(t) = \sin(4t) - 2\cos(4 t) + 2e^{-2t}
				</me>.
			</p>
		</example>

		<p>
			Finally, consider the driven, undamped oscillator <m>y''+\omega_n^2 y= \cos(\omega t)</m>. It leads to 
			<me> 
				Y(s) = \frac{s}{(s^2+\omega_n^2)(s^2+1)} = \frac{1}{\omega_n^2-\omega^2} \left( \frac{s}{s^2+\omega^2} - \frac{s}{s^2+\omega_n^2} \right)
			</me>, 
			which is readily inverted into a combination of <m>\cos(\omega t)</m> and <m>\cos(\omega_n t)</m>. However, at this point we are probably better off using the method of undetermined coefficients instead. 
		</p>


	</subsection>
	
  </section>

  <section xml:id="so-laplace-repeated">
	<title>Repeated and complex poles</title>

	<subsection>
		<title>Repeated poles and resonance</title>
		<p>
			We know by now that repeated poles in <m>Y(s)</m> arise in two ways: from repeated characteristic roots (i.e. poles of the transfer function), or perfect resonance in the forcing function. We can anticipate the effect on the transform, because we know the answer in the time domain is something like <m>te^{rt}</m>. 
			<!-- The fiendlishly clever approach is to notice that this function is a derivative of <m>e^{rt}</m> <em>with respect to <m>r</m></em>:
			<me>
				\mathcal{L} \left[ \frac{d}{dr} e^{rt} \right] = \frac{d}{dr}  \mathcal{L} \left[ e^{rt} \right] = \frac{d}{dr}\left[ \frac{1}{s-r} \right] 
			</me>,
			from which we conclude 
			<me>
				\mathcal{L} \left[ t e^{rt} \right] = \frac{1}{(s-r)^2} 
			</me>.-->
		</p>

		<p>
			In fact if we have any <m>f(t)</m> with transform <m>F(s)</m>, then from the transform definition we compute
			<md>
				<mrow>F'(s) \amp = \frac{d}{ds} \int_0^\infty f(t) e^{-st}\, dt</mrow>
				<mrow>\amp = -\int_0^\infty t f(t) e^{-st}\, dt</mrow>
				<mrow>\amp = - \mathcal{L}[ tf(t) ]</mrow>
			</md>. 
			Take a moment to appreciate this. Earlier we found that a derivative in <m>t</m> corresponds to multiplying by <m>s</m>. Up to a change in sign, a derivative in <m>s</m> corresponds to multiplying by <m>t</m>!
		</p>
		<example>
			<p>
				The undamped oscillator <m>y''+9y=\sin(3t)</m> has a perfect resonance. The transform suggests the particular solution 
				<me>
					Y(s) = \frac{3}{(s^2+9)^2}
				</me>. 
				This doesn't look like anything easy. However, note that
				<me>
					sY(s) = \frac{3s}{(s^2+9)^2} = -\frac{3}{2} \frac{d}{ds} \left[ \frac{1}{s^2+9} \right] 
				</me>. 
				Therefore, <m>sY(s)</m> is the transform of <m>\frac{1}{2} t \sin(3t)</m>. So this must be <m>y'(t)</m>. An integration by parts then recovers
				<me>
					y(t) = \frac{1}{18} [ \sin(3t) - 3t\cos(3t) ]
				</me>.
			</p>
		</example>
		<p>
			If we keep differentiating in <m>s</m> we find 
			<me> 
				\frac{d^n}{ds^n} \mathcal{L}[ f(t) ] = (-1)^n \mathcal{L}[ t^n f(t) ]
			</me>.
			Of interest is the special case of an exponential function <m>e^{ct}</m>, whose transform is <m>1/(s-c)</m>: 
			<me>
				\mathcal{L} \left[ t^n e^{ct} \right] = \frac{n!}{(s-c)^{n+1}} 
			</me>.
			The even specialer case <m>c=0</m> allows us to transform any power of <m>t</m>, and therefore any polynomial. 
		</p>
	</subsection>

	<subsection>
		<title>Complex poles</title>
		<p> 
			Complex poles in the transform correspond to complex exponentials in the time domain. When they occur in conjugate pairs, the entire expression can be cast in real terms. We have 
			<me>
				\frac{1}{s-(a+ib)} + \frac{1}{s-(a-ib)} = \frac{2(s-a)}{(s-a)^2+b^2} 
			</me>. 
			Since we also know that 
			<me>
				e^{(a+ib)t} + e^{(a-ib)t} = 2 e^{at} \cos( b t)
			</me>,
			the conclusion is 
			<me>
				\mathcal{L}[ e^{at} \cos( b t) ] = \frac{s-a}{(s-a)^2+b^2}
			</me>. 
			In similar fashion we can show that <m>e^{at} \sin( b t)</m> transforms to <m>b/[(s-a)^2+b^2]</m>. In principle we can always find quadratic denominators like these for a partial fraction conversion. However, the details can get pretty intense for hand calculation.
		</p>
		<example>
			<p>
				Suppose <m>y''+2y'+10y=6</m>. The very particular solution has <m>Y(s)=6/(s^2+2s+10)</m>. We can use the trick of completing the square to write this as <m>Y(s)=6/[(s-1)^2+9]</m>, and thus <m>y(t) = 2e^{-t}\sin(3t)</m>. 
			</p>
			<p>
				Things are much more complicated for a harmonically forced case such as <m>y''+2y'+10y=\cos(2t)</m>. Now 
				<me>
					Y(s) = \frac{s}{(s^2+4)(s^2+2s+10)}
				</me>. 
				It is true that we have a decomposition in the form 
				<me> 
					Y(s) = \frac{A+Bs}{s^2+4} + \frac{C+Es}{(s-1)^2+9}
				</me>, 
				which is eminently invertible. But finding the four coefficients is a pretty heavy lift to do by hand. 
			</p>
		</example>

		<p> 
			The problem with Laplace transforms as a one-stop solver is that the particular solution we find is always the "very particular" one that includes some terms from the null solution. Equivalently, the poles of <m>Y(s)</m> include those of the transfer function <em>and</em> the forcing function. It's extra algebra to find the coefficients of the null/transient parts, but that step can't be skipped. Undetermined coefficients is a less sexy but simpler way to solve concrete problems in most cases. 
		</p>
	</subsection>
  </section>

  <section xml:id="so-convolutions">
	<title>Convolutions</title>
	<introduction>
		<p><em>From section 8.6, pp. 479–483 (skip Fourier and matrix material)</em></p>
	</introduction>
	
	<subsection>
		<title>Moving averages</title>
		<p>
			The following grabs and plots the closing price of Bitcoin for the last 31 days.
		</p>
			<sidebyside>
	 			<program language="matlab" permid="so_conv_bitcoin1">
	    			<input>
%bc = webread('https://api.coindesk.com/v1/bpi/historical/close.json');
%data = jsondecode(bc);
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-o')
					</input>
				</program>
				<image source="matlab/so_conv_bitcoin1.svg"/>	  
			</sidebyside>

			<p>
				It's a noisy curve. We can smooth that out by taking 4-day moving averages, for example. I don't claim that this is the best way, but we can do this by:
			</p>
			<sidebyside>
	 			<program language="matlab" permid="so_conv_bitcoin2">
	    			<input>
%bc = webread('https://api.coindesk.com/v1/bpi/historical/close.json');
%data = jsondecode(bc);
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-ko')
for i = 4:31
    z(i) = (v(i) + v(i-1) + v(i-2) + v(i-3)) / 4;
end
z(1:3) = NaN;  % not a number
hold on, plot(z,'-o')
					</input>
				</program>
				<image source="matlab/so_conv_bitcoin2.svg"/>	  
			</sidebyside>

			<p>
				We might decide, however, to weight the most recent values more heavily. Here's how this might look.
			</p>
			<sidebyside>
	 			<program language="matlab" permid="so_conv_bitcoin3">
	    			<input>
%bc = webread('https://api.coindesk.com/v1/bpi/historical/close.json');
%data = jsondecode(bc);
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-ko')
w = [4 3 2 1];
for i = 4:31
    z(i) = (w(1)*v(i) + w(2)*v(i-1) + w(3)*v(i-2) + w(4)*v(i-3)) / sum(w);
end
z(1:3) = NaN;  % not a number
hold on, plot(z,'-o')
					</input>
				</program>
				<image source="matlab/so_conv_bitcoin3.svg"/>	  
			</sidebyside>
			<p>
				Note that each new value <m>z_i</m> is a linear combination of the elements of <m>v</m>, weighted by the values in <m>w</m>. And we always go forward 1,2,3,4 in <m>w</m>, while on <m>v</m> we have a sliding group going backward: <m>i,i-1,i-2,i-3</m>. Put concisely, the indices add up to the constant value <m>i+1</m>. 
			</p>
	</subsection>

	<subsection>
		<title>Convolution integral</title>
		<p>
			The moving average above gave us a certain way to multiply together vectors <m>v</m> and <m>w</m> to get a new vector <m>z</m>: 
			<me>
				z_i =  \sum_{j} w_j v_{i-j} 
			</me>
			(although I reindexed the output vector from <m>i+1</m> to <m>i</m> compared to the computer code). This is analogous to a different way to multiply two <em>functions</em>, by the <term>convolution integral</term>
			<me> 
				[f*g](t) = \int_0^t f(t-\tau)g(\tau)\, d\tau
			</me>. 
			This too is like a weighted average, with the values of <m>g(\tau)</m> being multiplied against different windows of the function <m>f</m> such that the arguments sum to <m>t</m>. 
		</p>
		<p> 
			An interesting fact is that while I interpreted <m>f*g</m> as <m>g</m> acting on <m>f</m>, in fact the operation is symmetric:
			<md>
				<mrow>[g*f](t) \amp = \int_0^t f(t-\tau)g(\tau)\, d\tau</mrow>
				<mrow>\amp = \int_t^0 f(u)g(t-u)\, (-du)  </mrow>
				<mrow>\amp = \int_0^t g(t-u)f(u)\, du = [f*g](t)</mrow>
			</md>. 
			We can also easily prove some more properties we really like to have:
			<md>
				<mrow>f * ( g * h ) = ( f * g) * h </mrow>
				<mrow>f * (g+h)=(f * g)+(f*h)</mrow> 
				<mrow>f * 0=0</mrow>
			</md>. 
			It is <em>not</em> true, however, that <m>f*1=f</m>, unless <m>f</m> is the zero function. Instead, the right formula is <m> f * \delta=f</m>: 
			<me> 
				[f*\delta](t) = \int_0^t f(t-\tau) \delta(\tau)\, d\tau = f(t)
			</me>. 
		</p>
	</subsection>

	<subsection>
		<title>Convolution theorem</title>
		<p>
			Here is what makes convolution a pretty big deal. 
		</p>
		<theorem>
			<p>
				Suppose <m>{\cal L}[f] = F(s)</m>, <m>{\cal L}[g] = G(s)</m>, <m>h=f*g</m>, and <m>{\cal L}[h] = H(s)</m>. Then <m>H(s)=F(s)G(s)</m>.
			</p>
		</theorem> 
		<p> 
			Why does this matter? Consider finding a particular solution of <m>Ay''+By'+Cy=f(t)</m>. After taking transforms we get  
			<me> 
				Y(s) = \frac{1}{As^2+Bs+C} F(s) = G(s) F(s)
			</me>, 
			where <m>G(s)</m> is our old friend the transfer function. Right away then, we know that <m>y(t)=f(t)*g(t)</m>. 
		</p>
		<p>
			Wait, there's more! Consider the problem of the impulse response, <m>Ay''+By'+Cy=\delta(t)</m>, with zero initial conditions. The solution of this problem has <m>Y(s)=G(s)</m>. That is, 
		</p>
		<fact>
			<p>The impulse response of a linear constant-coefficient equation is the inverse Laplace transform of the transfer function.</p>
		</fact>
		<p> 
			Put it all together and you see that the solution to any forced problem is a convolution of the forcing with the impulse response/fundamental solution, whose transform is the transfer function. Which is the reciprocal of the characteristic polynomial! Evaluating convolution integrals for specific functions is rarely easy, so the most important implications here are theoretical. 
		</p>
	</subsection>
  </section>

</chapter>
