<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch4-second-order-linear" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>Second-order ODEs</title>

<section xml:id="so-convert">
	<title>Order equals dimension</title>
	<p>
		We now turn our attention to second-order ODEs of the form 
		<me>
			x''=f(t,x,x')
		</me>. 
		Most of what we develop extends easily to vector-valued <m>u(t)</m>  and equations of order higher than two, but we focus on this problem to keep notation simple. 
	</p>
	<p>
		Our first observation is that this problem can always be converted to a first-order problem in two dimensions. Define a vector <m>\bu</m> by <m>u_1=x</m>, <m>u_2=x'</m>. By definition, <m>u_1'=u_2</m>. Since <m>u_2'=x''=f(t,x,x')</m>, we get the system 
		<md>
			<mrow> u_1' \amp =  u_2</mrow>
			<mrow> u_2' \amp =  f(t,u_1,u_2).</mrow>
		</md>
		This is a first-order system <m>\bu'=\mathbf{g}(t,\bu)</m> of two scalar equations for the two components. 
	</p>
	<example>
		<p>
			A pendulum is governed by the nonlinear equation <m>\theta''+ b\theta' + \frac{g}{L}\sin(\theta)=0</m>, where <m>\theta(t)</m> is the angle made by the pendulum from the straight-down position. Define <m>u_1=\theta</m>, <m>u_2=\theta'</m>. Then 
			<me>
				\twovec{u_1'}{u_2'} = \twovec{u_2}{-bu_2 - \frac{g}{L}\sin(u_1)}
			</me>
			is an equivalent system.
		</p>
	</example>
	<p>
		The technique above generalizes to any system of equations of any order. 
	</p>
	<example>
		<p>
			Two pendulums hanging from a bar and swinging in parallel planes are coupled through torsion on the bar, according to 
			<md>
			  <mrow> \theta_1''+ b\theta_1' + \frac{g}{L}\sin(\theta_1) + \kappa(\theta_1-\theta_2) \amp = 0 </mrow>
			  <mrow> \theta_2''+ b\theta_2' + \frac{g}{L}\sin(\theta_2) + \kappa(\theta_2-\theta_1) \amp = 0 </mrow>
			</md>
			We define 
			<me>
				u_1 = \theta_1, \quad u_2 = \theta_1', \quad u_3 = \theta_2, \quad u_4 = \theta_2'.
			</me>
			Two differential equations, <m>u_1'=u_2</m> and <m>u_3'=u_4</m>, follow automatically from the definitions. The remaining two come from substituting into the original ODEs: 
			<md>
			  <mrow>u_2' = \theta_1'' \amp = - b u_2 - \frac{g}{L}\sin(u_1) - \kappa(u_1-u_3)</mrow>
			  <mrow>u_4' = \theta_2'' \amp = - b u_4 - \frac{g}{L}\sin(u_3) - \kappa(u_3-u_1)</mrow>
			</md>
			The result is a four-dimensional, first-order system <m>\bu'=\mathbf{g}(t,\bu)</m>.
		</p>
	</example>
	<p>
		Our main item of business in this chapter is the linear, constant-coefficient model problem 
		<men xml:id="so-eq-linearmodel">
			x'' + bx' + cx = f(t)
		</men>,
		for which the conversion process with <m>u_1=x</m>, <m>u_2=x'</m> yields 
		<men xml:id="so-eq-matrixmodel">
			\twovec{u_1'}{u_2'} = \twovec{u_2}{f(t)-cu_1-bu_2} = \twomat{0}{1}{-c}{-b} \bu + \twovec{0}{f(t)}
		</men>,
		which has the form <m>\bu'=\bA\bu+\bff(t)</m> for the <idx>companion matrix</idx><term>companion matrix</term> 
		<men xml:id="so-eq-companion">
			\bA = \twomat{0}{1}{-c}{-b}
		</men>.
		Thus everything we learned from the last chapter is again in play. However, the applications of <xref ref="so-eq-linearmodel"/> are important and numerous enough to consider the solutions in detail, and the structure of the companion matrix is such that the conclusions can often be rewritten more simply than in the general case. 
	</p>
	<p>
		As a result of the equivalence to a first-order system, we know that the general solution of <xref ref="so-eq-linearmodel"/> consists of the general solution of the homogeneous (unforced) problem, and a particular solution of the forced problem: <m>x(t)=x_h(t)+x_p(t)</m>. Our next steps will be to translate the system results for each of these parts to the new situation. 
	</p>
</section>

<section xml:id="so-homogeneous">
	<title>Homogeneous problem</title>
	<introduction>
		<p>
			Referring to <xref ref="so-eq-companion"/>, the homogeneous equation <m>x''+bx'+cx=0</m> can be recast as a first-order homogeneous system <m>\bu'=\bA\bu</m> through the definitions 
			<me>
				\bu = \twovec{x}{x'},\qquad  \bA = \twomat{0}{1}{-c}{-b}
			</me>.
			Note that an initial-value problem for the system specifies <m>\bu(0)</m>, or equivalently, both <m>x(0)</m> and <m>x'(0)</m>. Hence <alert>a second-order scalar problem requires two initial conditions to have a unique solution.</alert>
		</p>
		<p>
			The matrix in this system has the characteristic polynomial 
			<me>
				|\bA-\lambda\bI| = \twodet{-\lambda}{1}{-c}{-b-\lambda} = \lambda^2 + b\lambda + c
			</me>.
			This is easily remembered in terms of the original ODE <m>x'' + bx' + cx=0</m>. The eigenvalues can therefore be found from the quadratic formula. 
		</p>
	</introduction>
	<subsection>
		<title>Distinct eigenvalues</title>
		<p>
			In the nondefective case, we find the eigenvectors from the nullspaces of <m>\bA-\lambda_j\bI</m> for <m>j=1,2</m>. These have the basis vectors <m>[1,\,\lambda_j]</m>. Thus the general homogeneous solution is 
			<me>
				\bu_h(t) = c_1 e^{\lambda_1 t} \twovec{1}{\lambda_1} + c_2 e^{\lambda_2 t} \twovec{1}{\lambda_2}
			</me>. 
			In terms of the original variable <m>x=u_1</m>, this is more simply 
			<men xml:id="so-eq-homogdistinct">
				x_h(t) = c_1 e^{\lambda_1 t} + c_2 e^{\lambda_2 t}, \qquad \text{if } \lambda_1\neq\lambda_2
			</men>.
			For real problems with complex eigenvalues, the second term in the sum is the conjugate of the first, and we could write instead 
			<me>
				x_h(t) = \Re \left[ c_1 e^{\lambda_1 t} \right] 
			</me>
			We might want an alternative real-only form. Suppose the eigenvalues are written as <m>\alpha \pm i \beta</m>. Starting with 
			<me>
				e^{(\alpha + i\beta)t} = e^{\alpha t} [\cos(\beta t) + i \sin(\beta t)],
			</me>
			we can take real and imaginary parts to get independent real solutions. Thus 
			<men xml:id="so-eq-homogcomplex">
				x_h(t) = e^{\alpha t} \bigl[ a_1 \cos(\beta t) + a_2 \sin(\beta t) \bigr], \qquad \text{if } \lambda=\alpha \pm i\beta
			</men>.
			There is a third equivalent form in the complex case: the <idx>phase-amplitude form</idx><term>phase-amplitude form</term>, 
			<men xml:id="so-eq-homogphaseamp">
				x_h(t) = R e^{\alpha t} \cos(\beta t - \theta)
			</men>. 
			To see the equivalence to <xref ref="so-eq-homogcomplex"/>, we can use a trig identity on the phase-shifted cosine: 
			<me>
				R e^{\alpha t} \cos(\beta t - \theta) = R e^{\alpha t} \cos(\theta) \cos(\beta t) + R e^{\alpha t} \sin(\theta) \sin(\beta t).
			</me>
			We get back to <xref ref="so-eq-homogcomplex"/> by choosing 
			<me>
				R\cos(\theta) = a_1, \qquad R\sin(\theta) = a_2.
			</me>
			More simply, we can think of <m>(a_1,a_2)</m> as Cartesian coordinates and express the same point in polar form to get <m>(R,\theta)</m>. Usually the amplitude, <m>R=\sqrt{a_1^2+a_2^2}</m>, is of the most interest. Whichever form is chosen for the general solution, it has two real constants that can be determined by the initial conditions, if supplied. 
		</p>
		<example>
			<p>
				Let's find the general solution of <m>x''+5x'+6x=0</m>. The characteristic polynomial is <m>\lambda^2+5\lambda+6</m>, which has roots <m>\lambda_1=-2</m>, <m>\lambda_2=-3</m>. Thus <m>x(t)=c_1 e^{-2t}+c_2e^{-3t}</m>. 
			</p>
		</example>
		<example>
			<p>
				The characteristic polynomial of <m>x''-2x'=0</m> is <m>\lambda^2 - 2\lambda</m>, which has roots <m>\lambda_1=0</m>, <m>\lambda_2=2</m>. Thus <m>x(t)=c_1 + c_2e^{2t}</m>.
			</p>
		</example>
		<example>
			<p>
				To solve <m>3x''+6x'+30x=0</m>, we first should divide through by 3 to get the standard form. Then the characteristic polynomial is <m>\lambda^2 + 2\lambda + 10</m>, which has roots <m>-1\pm 3i</m>. So <m>x(t) = e^{-t}[c_1 e^{3it} + \overline{c_1}e^{-3it}]</m>. Equivalently, <m>x(t) = e^{-t}[a_1 \cos(3t) + a_2\sin(3t)]</m>, or <m>x(t) = R e^{-t}\cos(3t-\theta)</m>.
			</p>
		</example>
	</subsection>
	<subsection>
		<title>Repeated eigenvalue</title>
		<p>
			If the companion matrix has a double eigenvalue, then it is defective (unless <m>b=c=0</m>, which leaves the truly trivial problem <m>x''=0</m>). In terms of the original ODE, this occurs when <m>b^2=4c</m>, and then <m>\lambda_1=\lambda_2=-b/2</m>. Recalling the trick used in <xref ref="fs-me-defective"/>, we define 
		<me>
			\bB = \bA + \frac{b}{2}\bI = \twomat{b/2}{1}{-c}{-b/2} = \twomat{b/2}{1}{-b^2/4}{-b/2}
		</me>. 
		You can easily check that <m>\bB^2=\bzero</m>. Hence the power series for the matrix exponential has just two terms:
		<me>
			e^{t\bA} = e^{-(bt/2)\bI} e^{t\bB} = e^{-bt/2}(\bI + t\bB) 
		</me>. 
		The general solution is <m>e^{t\bA}\bc</m>, but again we need only the first row to get the solution for the original <m>x=u_1</m>: 
		<me>
			x_h(t) = e^{-bt/2} \left[ c_1 + t\left(c_2 + c_1\frac{b}{2}\right) \right] = e^{-bt/2} \left( \tilde{c}_1 + c_2 t\right)
		</me>, 
		where <m>\tilde{c}_1</m> is just a different arbitrary constant. We can rename the constants and express the result as 
		<men xml:id="so-eq-homogdefect">
			x_h(t) = e^{-bt/2} \left( c_1 + c_2 t\right) , \qquad \text{if } \lambda_1 = \lambda_2 \neq 0.
		</men>
	</p>
	<example>
		<p>
			The characteristic polynomial of <m>x''-4x'+4x=0</m> is <m>\lambda^2 - 4\lambda + 4</m>, which has a double root at 2. Hence <m>x(t)=e^{2t}(c_1+c_2 t)</m> is the general solution. 
		</p>
	</example>
</subsection>
</section>

<section xml:id="so-undetermined">
	<title>Undetermined coefficients</title>
	<p>
		We saw two ways to get particular solutions for first-order equations: variation of parameters and undetermined coefficients. Since a second-order problem has a first-order system equivalent, we can use variation of parameters for a system, as in <xref ref="fs-varparam"/>, to find a particular solution. However, for many common second-order problems with constant coefficients, it is much easier to adapt the method of undetermined coefficients. 
	</p>
	<p>
		Specifically, the ODE <m>x''+bx'+cx=f(t)</m> can be solved with this method when <m>f</m> is a polynomial, exponential, sin, or cos function. The appropriate form for the particular <m>x_p</m>  for each case is given in <xref ref="fl-tb-undeter"/>. This form is plugged into the ODE, and the resulting identity is used to deduce the undetermined coefficients. 
	</p>
	<example>
		<p>Let's find a particular solution of <m>x'' +4x'+4x=8t^2</m>. The correct form of <m>x_p</m> is a quadratic polynomial, so 
		<me>
			x_p = A + Bt + Ct^2
		</me>. 
		Plugging that into the ODE yields
		<md>
			<mrow>2C + 4(B+2Ct) + 4(A+Bt+Ct^2) \amp = 8t^2</mrow>
			<mrow>4C t^2 + (8C+4B) + (2C+4B+4A) \amp = 8t^2</mrow>
		</md>. This has to be an identity for all <m>t</m>. Matching like powers, we can read off <m>C=2</m>, then <m>B=-4</m>, and finally <m>A=3</m>. This provides us with <m>x_p=2t^2+4t+3</m>. 
		</p>
	</example>
	<!--example>
		<p>Let's try for <m>y'' -2y'-3y=10te^{4t}</m>. For much the same reasons as in the last case, we try 
		<me>y_p = (A + Bt)e^{4t}</me>. There's nothing to do now except grind out some derivatives. 
		<md>
			<mrow>8(2A+B+2Bt)e^{4t} - 2(4A+B+4Bt)e^{4t} -3 (A + Bt)e^{4t} \amp =10t e^{4t}</mrow>
			<mrow> 8(2A+B) - 2(4A+B) -3A + t(16B-8B-3B) =  10t</mrow>
		</md>. From this, by matching powers, we deduce <m>B=2</m> and then <m>5A+12=0</m>. So <m>y_p = [2t - (12/5)]e^{4t}</m>.
		</p>
	</example-->
	<example>
		<p>
			For <m>x'' - 2x'-3x=10e^{4t}</m>, the proper choice is 
			<me>
				x_p = Ae^{4t}
			</me>. Everything else is just basic manipulations.  
			<md>
				<mrow>16A e^{4t} - 2 (4A e^{4t}) - 3 (Ae^{4t}) \amp =10 e^{4t}</mrow>
				<mrow> 16A -8A -3A =  10</mrow>
			</md>. From this it's clear that <m>A=2</m>.
			</p>
	</example>
	<example>
		<p>
			For <m>x''+x'=\sin(2t)</m>, we use 
			<me>
				x_p = A\cos(2t) + B\sin(2t)
			</me>. 
			Inserting this into the ODE leads to 
			<me>
				[-4A\cos(2t) - 4B\sin(2t)] + [-2A\sin(2t) + 2B\cos(2t)] = \sin(2t)
			</me>. 
			This is true for all time if and only if we match the coefficients of like trig functions:
			<me>
				-4A + 2B = 0, \qquad -4B-2A = 1 
			</me>. 
			The solution of these equations is <m>A=-1/10</m>, <m>B=-1/5</m>.
		</p>
	</example>
	<p>
		The examples above are the fundamental ones. There are rules for more intricate combinations of the same functions, but we won't go into them here. 
	</p>
	<p>
		We have to repeat the warning from the first time we saw this method: occasionally it fails, given our rules.
	</p>
	<example xml:id="so-ex-noundeter">
		<p>
			The equation <m>x''+x=\cos(\omega t)</m> suggests the particular solution <m>x_p=A\cos(\omega t)+B\sin(\omega t)</m>. Upon substitution,
			<md>
				<mrow>[-\omega^2 A\cos(\omega t) - \omega^2 B\sin(\omega t) ] + [ A\cos(\omega t) + B\sin(\omega t)] \amp =\cos(\omega t),</mrow>
			</md>. 
			which leads to the conclusion that <m>B=0</m> and, if <m>\omega^2 \neq 1</m>, <m>A=1/(1-\omega^2)</m>. However, if <m>\omega = 1</m>, the substitution would leave us with <m>0=\cos(t)</m>, which is impossible to satisfy for all <m>t</m>.
		</p>
	</example>
	<p>
		The failure of <xref ref="so-ex-noundeter"/> at <m>\omega = 1</m> was due to the fact that the <m>x_p</m> we picked is actually a homogeneous solution. There are additional rules to cover this case, but here is an interesting workaround instead. Suppose we seek a particular solution for <m>\omega=1</m> as the limit of a particular solution as <m>\omega\to 1</m>. We won't use the exact <m>x_p</m> found in the example, but instead subtract a homogeneous solution from it to get 
		<me>
			\frac{ \cos(\omega t) - \cos(t) } {1-\omega^2},
		</me>
		which is also a particular solution. Then L'Hopital applies in the limit to give us 
		<men xml:id="so-eq-secular">
			x_p(t) = \frac{ -t \sin(\omega t) }{ -2 \omega } = \frac{1}{2}t\sin(t). 
		</men>
		Now, to verify that this is a particular solution in the limiting case, we compute 
		<md>
		  <mrow> x_p \amp = \frac{1}{2}t\sin(t) </mrow>
		  <mrow> x_p' \amp = \frac{1}{2}[\sin(t)+t\cos(t)] </mrow>
		  <mrow> x_p'' \amp = \frac{1}{2}[\cos(t) + \cos(t) - t\sin(t)], </mrow>
		</md>
		so that <m>x_p''+x_p=\cos(t)</m> as required.
	</p>
</section>

<section xml:id="so-oscillators">
	<title>Oscillators</title>
	<introduction>
		<p>
			The most common use of second-order equations is the modeling of oscillatory or other periodic behavior.
		</p>
	</introduction>
	<subsection xml:id="so-os-mechanical">
		<title>Mechanical</title>
		<p>
			As we have already seen, a mass hanging from a spring satisfies the ODE
			<me>mx'' + b x' + kx = F(t)</me>.
			(As is typical, all the named constants are nonnegative.) Here <m>m</m>> is the mass, <m>b</m>>is a coefficient of friction or other damping forces, and <m>k</m>> is a characteristic constant of the spring. The term <m>F(t)</m> represents an external driving force. If we have initial conditions, they give values for <m>x</m> and <m>x'</m>.
		</p>
		<p>
			The gravitational force on a pendulum bob (think child on a swing) has a related equation. The proper ODE is 
			<me>\theta'' + \gamma \theta + \frac{g}{L}\sin \theta = F(t)</me>,
			where <m>\theta(t)</m> is the angle made with vertical-down and <m>L</m> is the arm length. This equation is nonlinear. But if the angle remains small, then a reasonable approximation is 
			<me>\theta'' + \gamma \theta + \frac{g}{L}\theta = F(t)</me>, 
			which is a linear oscillator. 
		</p>
	</subsection>
	<subsection>
		<title>Electrical oscillators</title>
		<p>
			An AC circuit often has elements of resistance, capacitance, and inductance. These analogize perfectly to friction/damping, spring constant, and mass. If these elements are wired in series with a generator, the governing ODE is
			<me>LI'' + RI' + \frac{1}{C}I = \mathcal{E}'(t)</me>,
			where <m>L</m> is inductance, <m>R</m> is resistance, <m>C</m> is capacitance, and <m>I(t)</m> is the current flowing through the circuit. The forcing term represents the generator whose voltage (EMF) is <m>\mathcal{E}(t)</m>. For alternating current this is modeled as a cosine (or complex exponential) function.  
		</p>
		<p>
			In DC circuits we have Ohm's Law, <m>I=V/R</m>, for a resistance <m>R</m>. In the AC case, if <m>\mathcal{E}(t)=Ve^{i\omega t}</m>, then <m>e^{i\omega t}</m> appears in the current as well, and it still makes sense to talk about <m>V/I</m> as a kind of resistance, now called <term>impedance</term> and dependent on the frequency <m>\omega</m>. 
		</p>
	</subsection>
	<subsection xml:id="so-os-notation">
		<title>Unifying notation</title>
		<p>
			When you have lots of different versions of the same root problem, with different symbols and units, you have three options. You can solve each new problem from scratch, you can derive custom formulas for each application, or you can try to find a minimal set of parameters and express each problem using those. Let's consider the last pathway here.
		</p>
		<p>
			The mechanical oscillator <m>mx'' + bx' + kx = F(t)</m> has, like all linear ODEs, a homogeneous part and a forced part. For now we consider only the homogeneous part, taking up the forcing term in the next section. Ignoring the degenerate case of zero mass, we write  
			<me> 
				x'' + \frac{b}{m} x' + \frac{k}{m} x = 0
			</me>.
			One reason to prefer this form is dimensional analysis: both <m>b/m</m> and <m>\sqrt{k/m}</m> have units of inverse time. We introduce new parameters 
			<me>
				\omega_0 = \sqrt{\frac{k}{m}} \gt 0, \quad Z = \frac{b/m}{2\omega_0} = \frac{b}{\sqrt{4km}} \ge 0
			</me>.
			<m>\omega_0</m> is known as the <idx>natural frequency</idx><term>natural frequency</term>, with units of inverse time, and <m>Z</m> is a dimensionless <idx>damping coefficient</idx><term>damping coefficient</term> describing the relative intensity of the damping. Now the unforced ODE becomes 
			<men xml:id="so-eq-unified"> 
				x'' + 2 Z \omega_0\, x' + \omega_0^2\, x = 0
			</men>.
			A similar derivation can be done starting from the electrical circuit equation.
		</p>
		<aside>
			<p>
				In math we usually use <q>frequency</q> to mean the multiplier of <m>t</m> in a sin or cos function. In some fields this is called <term>angular frequency</term> and <q>frequency</q> is used to mean the number of cycles per time unit, as in Hz. 
			</p>
		</aside>
		<p>
			The eigenvalues of the homogeneous equation are  
			<me>
				\lambda_{1,2} = -Z \omega_0 \pm \omega_0 \sqrt{Z^2-1}
			</me>. 
			The discussion now splits into four cases, marked by increasing values of <m>Z</m>. 
		</p>
	</subsection>
	<subsection xml:id="so-os-undamped">
		<title>Undamped</title>
		<p>
			The first case for <xref ref="so-eq-unified"/> is <m>Z=0</m>, which is the idealization of no mechanical damping or friction (or resistance, in a circuit). The homogeneous equation is <m>x''+\omega_0^2x=0</m>, which has eigenvalues <m>\pm i\omega_0</m> and a general solution that can be expressed three ways:
			<md>
			  <mrow> x_h(t) \amp = \Re \left[  c_1 e^{i\omega_0 t} \right] </mrow>
			  <mrow>  \amp = a_1 \cos(\omega_0 t) + a_2 \sin(\omega_0 t) </mrow>
			  <mrow> \amp =  R \cos(\omega_0 t - \theta) </mrow>
			</md>
			Thus the undamped case results in pure oscillation at frequency <m>\omega_0</m>. This is known as <idx>simple harmonic motion</idx><term>simple harmonic motion</term>. 
		</p>
		<example>
			<p>
				When a 2 kg mass is hung on a spring, the spring stretches by 0.25 m. What is the natural frequency of the mass-spring system? Suppose the mass is pulled down 0.2 m past equilibrium and then thrown upward at 1 m/s. What is the amplitude of the motion? 
			</p>
			<p>
				Hooke's Law for a spring states that <m>F=k x</m>, so we find the spring constant from <m>k=F/x=2g/0.25=8g</m>, where <m>g=9.8</m> m per second squared. The ODE for free motion of the system is thus 
				<md>
				  <mrow> 2 x'' + 8g x  \amp = 0 </mrow> 
				  <mrow> x'' + 4g x  \amp = 0, </mrow> 
				</md>
				from which we identify the natural frequency
				<me>
					\omega_0 = \sqrt{4g} = 2\sqrt{g} \approx 6.26 \text{s}^{-1}. 
				</me>
				We can apply the initial conditions directly to the amplitude-phase form: 
				<md>
				  <mrow> -0.2 \amp = x(0) = R\cos(0-\theta) = R\cos(\theta) </mrow>
				  <mrow> 1 \amp = x'(0) = -\omega_0 R\sin(0-\theta) = \omega_0 R\sin(\theta) </mrow>
				</md>
				Therefore, 
				<me>
					R = \sqrt{ [R\cos(\theta)]^2 + [R\sin(\theta)]^2 } = \sqrt{ 0.04 + \omega_0^{-2} } \approx 0.256 \text{m}.
				</me>
				We could go on to find the phase by taking a ratio to get 
				<me>
					\tan(\theta) = \frac{1}{-0.2\omega_0}.
				</me>
				In solving for <m>\theta</m>, you have to do a <q>four-quadrant</q> arctan. Observe that <m>(R\cos\theta,R\sin\theta)</m> lies in the second quadrant, so <m>\theta</m> has to be between <m>\pi/2</m> and <m>\pi</m>. One finds that <m>\theta\approx 2.47</m>. 
			</p>
		</example>
	</subsection>
	<subsection xml:id="so-os-underdamped">
		<title>Underdamped</title>
		<p>
			For <m>0\lt Z \lt 1</m> the eigenvalues of <xref ref="so-eq-unified"/> are complex:
			<me>
				\lambda_{1,2} = -Z \omega_0 \pm i \omega_0 \sqrt{1-Z^2}
			</me>. 
			Defining <m>\omega_d=\omega_0 \sqrt{1-Z^2}</m>, the general homogeneous solution is therefore 
			<me>
				x_h(t) = e^{-Z\omega_0 t} \left( c_1 e^{i\omega_d t} + c_2 e^{-i\omega_d t} \right)
			</me>, 
			or the equivalent real form with cos and sin. This solution is pseudoperiodic, combining oscillation at frequency <m>\omega_d \lt \omega_0</m> inside an exponential decay envelope. This situation is called an <idx>underdamped oscillator</idx><term>underdamped oscillator</term>. As in the undamped case, we have three different ways to express the real unforced solution in the underdamped case, corresponding to <xref ref="so-eq-homogdistinct"/>, <xref ref="so-eq-homogcomplex"/>, and <xref ref="so-eq-homogphaseamp"/>.
		</p>
	</subsection>
	<subsection xml:id="so-os-critical">
		<title>Critically damped</title>
		<p>
			At <m>Z=1</m> the complex eigenvalues collapse to a double eigenvalue at 
			<me>
				\lambda_1 = \lambda_2 = -\omega_0
			</me>,
			with general homogeneous solution 
			<me>
				x_h(t) = e^{-\omega_0 t} (c_1 + c_2 t)
			</me>. 
			There is no longer any oscillation present, and we have a <idx>critically damped oscillator</idx><term>critically damped</term> system. The linear growth of <m>c_2 t</m> doesn't make much of a difference against the exponential decay. 
		</p>
	</subsection>
	<subsection xml:id="so-os-overdamped">
		<title>Overdamped</title>
		<p>
			For <m>Z>1</m> the eigenvalues 
			<me>
				\lambda_{1,2} = -Z \omega_0 \pm \omega_0 \sqrt{Z^2-1}
			</me> 
			are negative and real, thus giving an exponentially decaying homogeneous solution. In this case we have an <idx>overdamped oscillator</idx><term>overdamped oscillator</term>. 
		</p>
	</subsection>
	<conclusion>
		<p>
			<xref ref="so-tab-damping"/> summarizes the four major cases for damping. If the damping coefficient <m>Z</m> is nonzero, the eigenvalues of the problem have negative real part, and therefore all the homogeneous solutions decay exponentially in amplitude as <m>t\to\infty</m>. For this reason the homogeneous solution is often called a <idx>transient solution</idx><term>transient solution</term> or transient response. 
		</p>
		<table xml:id="so-tab-damping">
		  <title>Damping coefficient and eigenvalues</title>
		  <tabular>
			<row>
			  <cell>Damping coefficient</cell> 
			  <cell>Eigenvalue property</cell>
			  <cell>Description</cell>
			</row>
			<row>
				<cell><m>Z=0</m></cell>
				<cell>imaginary</cell>
				<cell>undamped</cell>
			</row>
			<row>
				<cell><m>0 \lt Z \lt 1</m></cell>
				<cell>complex</cell>
				<cell>underdamped</cell>
			</row>
			<row>
				<cell><m>Z=1</m></cell>
				<cell>real, negative, repeated</cell>
				<cell>critically damped</cell>
			</row>
			<row>
				<cell><m>Z \gt 1</m></cell>
				<cell>real, negative </cell>
				<cell>overdamped</cell>
			</row>
		  </tabular>
		</table>
		<example>
			<p>
				A 5 kg mass is hung on a spring with constant <m>11</m> N per m and connected to a dashpot that provides 8 N-sec per meter of resistance. Is this system underdamped, overdamped, or critically damped? 
			</p>
			<p>
				The ODE for the mass-spring system is 
				<md>
				  <mrow> 5 x'' + 8x' + 11 x  \amp = 0 </mrow> 
				  <mrow> x'' + 1.6 x' + 2.2 x  \amp = 0, </mrow> 
				</md>
				from which we identify the natural frequency
				<me>
					\omega_0 = \sqrt{2.2} \approx 1.483 \text{s}^{-1}, \qquad 
				</me>
				The damping coefficient is therefore
				<me>
					Z = \frac{1.6}{2\omega_0} \approx 0.539.
				</me>
				Since this value is less than one, the system is underdamped. 
			</p>
		</example>
		<example>
			<p>
				Suppose the system from the last example is initially at equilibrium when the mass is suddenly pushed downward at 0.5 m/sec. Find the motion of the mass.
			</p>
			<p>
				The ODE is, as before, <m>x'' + 1.6 x' + 2.2 x = 0</m>. The eigenvalues are the roots of <m>\lambda^2 + 1.6\lambda + 2.2</m>, which are found numerically to be 
				<me>
					\lambda \approx -0.8000 \pm 1.2490i.
				</me>
				Note that the imaginary part is smaller than the natural frequency found in the last example, as it must be. From here I will treat these rounded values as though they are exact. Choosing the sin-cos form of the general solution, we have 
				<md>
					<mrow>x(t) \amp = a_1 e^{-0.8 t} \cos(1.249 t) + a_2 e^{-0.8 t} \sin(1.249 t).</mrow>
				</md>
				(It is not necessary to use the general formula derived above for this; the solution comes entirely from the eigenvalues.) We apply the initial conditions <m>x(0)=0</m>, <m>x'(0)=-0.5</m> to find 
				<md>
					<mrow> 0 \amp = x(0) = a_1, </mrow> 
					<mrow> -0.5 \amp = x'(0) = a_1( -0.8 ) + a_2 (1.249 ), </mrow> 
				</md>
				thus <m>a_2 = -0.4003</m>. The motion is therefore given by <m>x(t)=-0.4003\, e^{-0.8 t} \sin(1.249 t)</m>. 
			</p>
		</example>
	</conclusion>
</section>

<section xml:id="so-expforcing">
	<title>Driven oscillator</title>
	<introduction>
		<p>
			We next want to examine the linear oscillator in standardized form <xref ref="so-eq-unified"/> when there is an exponential forcing function present: 
			<men xml:id="so-eq-unifiedexp"> 
				x'' + 2 Z \omega_0\, x' + \omega_0^2\, x = e^{rt}
			</men>.
			We're typically interested in the undamped and underdamped cases, <m>0\le Z \lt 1</m>. 
		</p>
		<p>
			We appeal to the method of undetermined coefficients, positing a solution in the form <m>x_p(t) = A e^{rt}</m> to derive
			<me>
				(r^2  + 2 Z \omega_0 r + \omega_0^2)Ae^{rt} = e^{rt}
			</me>. 
			Hence 
			<men xml:id="so-eq-coeff">
				x_p(t) = A e^{rt}, \qquad A(r) = \frac{1}{r^2  + 2 Z \omega_0 r + \omega_0^2}
			</men>
			gives a particular solution unless the denominator, which is really just the characteristic polynomial, is zero. In that case, <m>r</m> is an eigenvalue; we consider that situation later. 
		</p>
	</introduction>
	<subsection xml:id="so-ef-harmonic">
		<title>Harmonic forcing</title>
		<p>
			Exponential forcing is particularly important when <m>r</m> is imaginary. Set <m>r=i\omega</m> in the above. The particular solution 
			<me>
				x_p(t) = A(i\omega) e^{i\omega t}
			</me>			
			is complex; say we decompose it as <m>x_p(t)=u_p(t)+i\, v_p(t)</m>. Using operator notation to write <m>\opA[x_p]=e^{i\omega t}</m>, linearity implies
			<me>
				\opA[u_p] + i\,\opA[v_p] = \cos(\omega t) + i\sin(\omega t)
			</me>. 
			Matching real and imaginary parts gives us
			<md>
				<mrow>\opA[u_p] \amp = \cos(\omega t), </mrow>
				<mrow>\opA[v_p]  \amp = \sin(\omega t). </mrow>
			</md>
			In words, the particular solution for imaginary-exponential forcing gives us the solutions we need for all cosine and sine forcing. All combinations of such forcing functions can be described as <idx>harmonic forcing</idx><term>harmonic forcing</term>. 
		</p>
		<p>
			It's often useful to rewrite the complex factor <m>A(i\omega)</m> from <xref ref="so-eq-coeff"/> in polar form: 
			<me>
				A(i\omega) = \frac{1}{\omega_0^2-\omega^2  + 2 Z i \omega_0 \omega } = g(\omega) e^{-i\phi(\omega)}
			</me>,
			where <m>g=|A|</m> is the <idx>gain</idx><term>gain</term> and <m>\phi</m> is the <idx>phase lag</idx><term>phase lag</term>. Note that the particular solution is 
			<me>
				x_p(t) = g(\omega) e^{-i\phi(\omega)} e^{i\omega t} = g e^{i(\omega t-\phi)}
			</me>. 
			This means that as we pass from the input forcing to the output solution, <m>g</m> is the scaling of the amplitude and <m>\phi</m> changes the phase of our trip around the complex unit circle. 
		</p>
	</subsection>
	<subsection xml:id="so-ef-resonance">
		<title>Resonance</title>
		<p>
			Consider forcing <m>e^{i\omega t}</m> in the undamped case with <m>Z=0</m>. Then 
			<me>
				A(i\omega) = \frac{1}{\omega_0^2-\omega^2}
			</me>
			is purely real, and the general solution is 
		</p>
		<fact>
			<title>Solution for standardized undamped driven oscillator (not resonant)</title>
			<p>
				<me>	
					x(t) = x_h(t) + x_p(t) = c_1 e^{i\omega_0 t} + c_2 e^{-i\omega_0 t} + \frac{1}{\omega_0^2-\omega^2} e^{i\omega t}
				</me>
				(or a real equivalent), provided <m>\omega\neq\omega_0</m>
			</p>
		</fact>
		<p>
			If <m>\omega\approx \omega_0</m>, then the amplitude of the particular solution is very large. 
		</p>
		<p>
			What is the meaning if <m>\omega = \omega_0</m>? This is exactly when <m>i\omega</m> is an eigenvalue of the homogeneous problem, so the method of undetermined coefficients will actually fail like it did in <xref ref="so-ex-noundeter"/>. If we use the same trick that gave us <xref ref="so-eq-secular"/>, we end up with the following.  
		</p>
		<fact>
			<title>Solution for standardized undamped driven oscillator at resonance</title>
			<p>
				<me>	
					x(t) = c_1 e^{i\omega_0 t} + c_2 e^{-i\omega_0 t} -\frac{i}{2} t e^{i\omega_0 t}
				</me>
				(or a real equivalent)
			</p>
		</fact>
		<p>
			This particular solution oscillates at the natural/driving frequency but with a linearly growing amplitude. This situation is called <idx>resonance</idx><term>resonance</term> and is one of the most important phenomena in physics. 
		</p>
	</subsection>
	<subsection>
		<title>Forcing the underdamped oscillator</title>
		<p>
			The eternally growing amplitude seen in resonance is an unphysical byproduct of the idealized undamped model. What happens if <m>Z\gt 0</m>? We are back to <m>x_p=Ae^{i\omega t}</m> with amplitude from <xref ref="so-eq-coeff"/>:
			<me>
				A(i\omega) = \frac{1}{\omega_0^2-\omega^2  + 2 Z i \omega_0\,\omega }
			</me>. 
			Of particular interest is the gain, 
		</p>
		<fact>
			<title>Gain for damped driven oscillator</title>
			<p>
				<me>
					g(\omega) = |A(i\omega)| = \frac{1}{[(\omega^2-\omega_0^2)^2  + 4 Z^2 \omega_0^2\,\omega^2]^{1/2} }
				</me>
			</p>
		</fact>
		<p>
			This formula is a bit easier to parse in terms of the frequency ratio <m>\rho=\omega/\omega_0</m>:
			<me>
				g(\rho) =  \frac{1}{\omega_0^2 [(1-\rho^2)^2  + 4 Z^2 \rho^2]^{1/2} }
			</me>. 
			In the presence of any damping, the gain always remains finite, and there is no longer a perfect resonance. But as a function of the frequency ratio, the denominator is minimized, and thus the gain is maximized, when 
			<men xml:id="so-eq-rhomax">
				\rho_\text{max} = \begin{cases} 
				\sqrt{1-2Z^2}, \amp \text{ if } 0 \lt Z^2 \le \frac{1}{2},\\ 0, \amp \text{ if } \frac{1}{2} \lt Z^2 \lt 1.
				\end{cases}
			</men>,
			Since <m>\rho_\text{max} \lt 1</m>, the <q>most resonant</q> frequency is always lower than the natural frequency. For <m>Z^2 \le \frac{1}{2}</m> the maximum gain is 
			<men xml:id="so-eq-gainmax">
				g(\rho_\text{max}) = \frac{1}{2Z\omega_0^2\sqrt{1-Z^2}}
			</men>,
			which is finite but large for <m>Z\approx 0</m>. 
		</p>
		<p>
			<xref ref="so_oscgain"/> shows the gain as a function of damping <m>Z</m> and forcing frequency <m>\omega</m> when <m>\omega_0=1</m>. The white curve shows the path of greatest resonance. For high values of the damping, the gain can never be larger than one. But as <m>Z\to 0</m>, the resonance peak becomes proportional to <m>1/Z</m>. 
		</p>
		<sidebyside>
			<listing  xml:id="so_oscgain">
				<caption>Gain for natural frequency equal to one</caption>
	 			<program language="matlab">
	    			<input>
A = @(omega,Z) 1./(-omega.^2 + 2i*Z.*omega + 1);
log_g = @(omega,Z) log10(abs(A(omega,Z)));
fsurf( log_g,[0 1.5 1e-3 1])
set(gca,'ydir','rev')
view(20,20)
xlabel('driving frequency')
ylabel('damping coefficient')
zlabel('log_{10}(gain)')
hold on
Z = linspace(1e-3,1,300);
rhomax = real(sqrt(1-2*Z.^2));
plot3(rhomax,Z,log_g(rhomax,Z),'w','linew',2)
					</input>
				</program>
				</listing>
				<image source="figures/so_oscgain.svg"/>	  
			</sidebyside>

	</subsection>
</section>

<section xml:id="so-laplace">
	<title>Laplace transform methods</title>
	<introduction>
		<p>
			We return to our linear, second-order, constant-coefficient problem in the simpler notation 
			<me>
				x'' + bx' + cx = f(t)
			</me>. 
			We cast this problem as a first-order system in two dimensions, found that the eigenvalues are the roots of <m>\lambda^2+b\lambda + c</m>, and then constructed exponential solutions for the homogeneous part (except for the double eigenvalue case). We also found a particular solution for the case of an exponential <m>f(t)</m>. There is an alternative way to derive the same results using the method of Laplace transforms to systematize the manipulations.
		</p>
		<table xml:id="so-tab-laplace">
			<title>Laplace transforms (complete)</title>
			<tabular>
				<row>
					<cell>Function</cell>
					<cell>Transform</cell>
				</row>
			<row>
				<cell><m>x'(t)</m></cell>
				<cell><m>sX(s)-x(0)</m></cell>
			</row>
			<row>
				<cell><m>x''(t)</m></cell>
				<cell><m>s^2 X(s)-s x(0) - x'(0)</m></cell>
			</row>
			<row>
				<cell>1</cell>
				<cell><m>\dfrac{1}{s}</m></cell>
			</row>
			<row>
				<cell><m>e^{at}</m></cell>
				<cell><m>\dfrac{1}{s-a}</m></cell>
			</row>
			<row>
				<cell><m>\cos(\omega t)</m></cell>
				<cell><m>\dfrac{s}{s^2+\omega^2}</m></cell>
			</row>
			<row>
				<cell><m>\sin(\omega t)</m></cell>
				<cell><m>\dfrac{\omega}{s^2+\omega^2}</m></cell>
			</row>
			<row>
				<cell><m>H(t-T)</m></cell>
				<cell><m>\dfrac{e^{-sT}}{s}</m></cell>
			</row>
			<row>
				<cell><m>\delta(t-T)</m></cell>
				<cell><m>e^{-sT}</m></cell>
			</row>
			<row>
				<cell><m>t^n, \quad n=1,2,3,\dots</m></cell>
				<cell><m>\dfrac{n!}{s^{n+1}}</m></cell>
			</row>
			<row>
				<cell><m>H(t-T)f(t-T)</m></cell>
				<cell><m>e^{-sT}F(s)</m> (shift theorem)</cell>
			</row>
			<row>
				<cell><m>t^n f(t), \quad n=1,2,3,\dots</m></cell>
				<cell><m>(-1)^n F^{(n)}(s)</m></cell>
			</row>
			<row>
				<cell><m>e^{at} f(t)</m></cell>
				<cell><m>F(s-a)</m></cell>
			</row>

			</tabular>
		</table>
		<p>
			<xref ref="so-tab-laplace"/> updates our table of Laplace transforms with additional entries. In this section and the next we examine how to use the new entries.
		</p>

	</introduction>

	<subsection>
		<title>Second derivatives</title>
		<p>
			Recall that the transform of <m>x'(t)</m> is <m>sX(s)-x(0)</m>. Applying this formula twice shows that 
			<me>
				\lx[x''(t)] = s \lx[x'(t)] - x'(0) = s^2X(s) - sx(0) - x'(0)
			</me>.
			When we apply this to <m>x''+bx'+cx=f(t)</m> with constant coefficients, we obtain 
			<me>
				[s^2X(s) - sx(0)-x'(0)] + b[sX(s)-x(0)] + cX(s) = F(s)
			</me>.
			It is now trivial to solve for <m>X(s)</m>: 
			<me>
				X(s) = \frac{ sx(0)+x'(0)}{s^2 +bs+c} +  \frac{F(s)}{s^2 +bs+c}
			</me>.
			The first fraction above is what leads to the free or unforced response, while the second is the effect of the forcing. The denominator in both cases is simply the characteristic polynomial of the ODE. 
		</p>
		<p>
			If the goal is to solve an initial-value problem with the transform, then it's necessary to find the inverse transform of <m>X(s)</m>, which typically involves some partial fraction efforts. 
		</p>
		<example>
			<p>
				Let's solve <m>x''-x=24 e^{3t}</m>, with <m>x(0)=4</m>, <m>x'(0)=12</m>. Transforming both sides gives 
				<me> 
					[s^2X(s) - 4s - 12] - X(s) = \frac{24}{s-3}
				</me>. 
				We easily solve for <m>X</m>:
				<me>
					X(s) = \frac{4s+12}{(s^2-1)} + \frac{24}{(s^2-1)(s-3)} 
				</me>. 
				In order to avoid multiple uses of partial fractions, we combine everything into a single fraction: 
				<me>
					X(s) = \frac{4s^2-12}{(s-1)(s+1)(s-3)}
				</me>
				The PFD takes the form 
				<me>
					\frac{4s^2-12}{(s-1)(s+1)(s-3)} = \frac{A}{s-1} + \frac{B}{s+1} + \frac{C}{s-3}, 
				</me>
				which is the same as the identity 
				<me>
					4s^2-12 = A(s+1)(s-3) + B(s-1)(s-3) + C(s-1)(s+1).
				</me>
				One way to handle this is to expand the right-hand side and equate the powers of <m>s</m>. In this case it's easier to choose three strategic values of <m>s</m>.   
				<md>
				  <mrow> s = 1 \amp \:\Rightarrow\: -8 = -4A  </mrow>
				  <mrow> s = -1 \amp\: \Rightarrow\: -8 = 8B </mrow>
				  <mrow> s = 3 \amp\: \Rightarrow\: 24 = 8C </mrow>
				</md>, 
				so that 
				<me>
					X(s) = \frac{2}{s-1} + \frac{-1}{s+1} + \frac{3}{s-3}.
				</me>
				Now it's clear that <m>x(t) = 2e^t - e^{-t} + 3e^{3t}</m>.
			</p>
		</example>
		<p>
			If we are after a general solution in which the initial values are unknown or arbitrary, there is a way to use them to shorten the process. 
		</p>
		<example>
			<p>
				We reconsider <m>x''-x=24 e^{3t}</m> with the goal of finding a general solution. As before, we obtain 
				<me>
					X(s) = \frac{sx(0) + x'(0)}{(s-1)(s+1)} + \frac{24}{(s-1)(s+1)(s-3)}.
				</me>
				Without manipulating this further for now, we know that we seek a PFD in the form 
				<me>
					X(s) = \frac{A}{s-1} + \frac{B}{s+1} + \frac{C}{s-3}.
				</me>
				Consider that this inverts to 
				<me>
					x(t) = Ae^t + Be^{-t} + Ce^{3t}.
				</me>
				The first two terms are due to the eigenvalues and are part of the homogeneous solution. Their coefficients are arbitrary in the general solution. It's the coefficient <m>C</m> that is of interest to us; i.e., a particuar solution. Suppose we chose the initial conditions in a way that zeroed out the homogeneous terms. Then 
				<me>
					\frac{sx(0) + x'(0)}{(s-1)(s+1)} + \frac{24}{(s-1)(s+1)(s-3)} = \frac{C}{s-3}.
				</me>
				Clearing the denominators leads to 
				<me>
					(sx(0)+x'(0))(s-3) + 24 = C(s-1)(s+1). 
				</me>
				We don't actually care about the numerical values of the initial conditions. When we set <m>s=3</m>, they disappear and we get <m>24=8C</m>. Right away, then, we know that the general solution is 
				<me>
					x(t) = x_h(t) + x_p(t) = c_1 e^{t} + c_2 e^{-t} + 3 e^{3t}. 
				</me>
			</p>
		</example>
		<p>
			In light of the last example, let's look again at the general case 
			<me>
				X(s) = \frac{ sx(0)+x'(0)}{s^2+bs+c} +  \frac{F(s)}{s^2 +bs+c}
			</me>.
			The denominators contain the characteristic polynomial, whose roots are the eigenvalues of the homogeneous problem. We refer to zeros in the denominator as <idx>poles</idx><term>poles</term> of the transform. Each eigenvalue pole will contribute a term <m>1/(s-\lambda)</m> to the partial fraction decomposition. Upon inversion, this term contributes <m>e^{\lambda t}</m>, which is part of the homogeneous solution that we know has an arbitrary coefficient. When chasing any particular solution, we may zero those terms out when finding the decomposition.
		</p>
		<p>
			The forcing function will contribute one or more of its own poles in <m>F(s)</m>. These also generate terms in the partial fraction decomposition, and these terms invert to contribute to a particular solution. The coefficients of these terms are <em>not</em> arbitrary and must be found from the decomposition in transform space. 
		</p>
		
		<example>
			<p>
				To find the general soluton of <m>x'' - 4x + 3x = 6</m>, we note that the characteristic polynomial is <m>s^2-4s+3=(s-3)(s-1)</m>. We take transforms to obtain 
				<me>
					X(s) = \frac{sx(0)+x'(0)}{(s-3)(s-1)} + \frac{6}{s(s-3)(s-1)} .
				</me>
				The PFD of this has the form 
				<me>
					X(s) = \frac{A}{s-3} + \frac{B}{s-1} + \frac{C}{s}.
				</me>
				The first two terms constitute the homogeneous solution, and we zero out their coefficients and replace <m>X</m> with <m>X_p</m>. Equating to the earlier expression and clearing denominators gives 
				<me>
					(sx(0)+x'(0))(s) + 6 = C(s-3)(s-1).
				</me>
				We don't care about the initial values, so we choose <m>s=0</m> to find that <m>6=3C</m>. Thus <m>X_p(s)=2/s</m> and <m>x_p(t)=2</m>. Finally, 
				<me>
					x(t) = c_1 e^{3t} + c_2 t e^{t} + 2. 
				</me>
			</p>
		</example>
	</subsection>

	<subsection> 
		<title>Imaginary poles</title>
		<p>
			If we transform <m>e^{i\omega t}</m>, we can take the real and imaginary parts to get some new formulas:
			<me>
				\lx[\cos(\omega t)] = \text{Re}\left( \frac{1}{s-i\omega} \frac{s+i\omega}{s+i\omega} \right) = \frac{s}{s^2+\omega^2}
			</me>,
			and 
			<me>
				\lx[\sin(\omega t)] = \text{Im}\left( \frac{1}{s-i\omega} \frac{s+i\omega}{s+i\omega} \right) = \frac{\omega}{s^2+\omega^2}
			</me>.
			Conversely, a transform <m>X(s)</m> that has <m>s^2+\omega^2</m> as a factor in the denominator has poles at <m>\pm i\omega</m>. The simplest case is 
			<me>
				X_p(s) = \frac{A+Bs}{s^2+\omega^2}  \quad \Rightarrow \quad x_p(t)=\frac{A}{\omega}\sin(\omega t) + B\cos(\omega t)
			</me>. 
		</p>
		<example>
			<p>
				Let's solve <m>x''+9x=\delta(t-2)</m> with <m>x(0)=0</m>, <m>x'(0)=-3</m>. Taking transforms leads to
				<me>
					X(s) = \frac{-3}{s^2+9} + \frac{e^{-2s}}{s^2+9} 
				</me>. 
				The first term inverts easily to <m>-\sin(3t)</m>. The second has the form <m>e^{-2s} F(s)</m>, where <m>F(s)</m> is the transform of 
				<me>
					f(t) = \frac{1}{3}\sin(3t)
				</me>. So the shift theorem implies
				<me>
					x(t) = -\sin(3t) + \frac{1}{3} H(t-2) \sin(3t-6)
				</me>. 
			</p>
		</example>
		<p>
			A pair of imaginary poles, plus one real pole, implies the PFD  
			<me>
				X(s) = \frac{A+Bs}{s^2+\omega^2} + \frac{C}{s-r} 
			</me>,
			corresponding to 
			<m>
				x(t) = \frac{A}{\omega}\sin(\omega t) + B\cos(\omega t) + C e^{rt} 
			</m>. 
			In an initial-value problem you usually have to find all three constants, whereas for the general solution we may be able to zero out the homogeneous solution. 
		</p>
		<example>
			<p>
				To find the general soluton of <m>x'' - 4x + 3x = 10\cos(t)</m>, we note that the characteristic polynomial is <m>s^2-4s+3=(s-3)(s-1)</m>. We take transforms to obtain 
				<me>
					X(s) = \frac{sx(0)+x'(0)}{(s-3)(s-1)} + \frac{10s}{(s^2+1)(s-3)(s-1)} .
				</me>
				The PFD of this has the real form 
				<me>
					X(s) = \frac{A}{s-3} + \frac{B}{s-1} + \frac{Cs+D}{s^2+1}.
				</me>
				The first two terms constitute the homogeneous solution, and we zero out their coefficients and replace <m>X</m> with <m>X_p</m>. Equating to the earlier expression and clearing denominators gives 
				<me>
					(sx(0)+x'(0))(s^2+1) + 10s = (Cs+D)(s-3)(s-1).
				</me>
				We don't care about the initial values, so we evaluate both sides at <m>s=i</m>: 
				<md>
					<mrow> 10i \amp = (Ci+D)(i-3)(i-1)  = (Ci+D)(2-4i) = (4C+2D) + i(2C-4D)  </mrow>
				</md>.
				Equating the real and imaginary parts gives <m>4C+2D=0</m>, <m>2C-4D=10</m>, which are easily solved for <m>C=1,D=-2</m>. Hence our particular solution satisfies
				<me>
					X_p(s) = 1 \, \frac{s}{s^2+1} - 2\, \frac{1}{s^2+1}. 
				</me>
				Thus 
				<me>
					x_p(t) = \cos(t) - 2 \sin(t),
				</me>
				and finally 
				<me>
					x(t) = c_1 e^{3t} + c_2 t e^{t} + \cos(t) - 2 \sin(t). 
				</me>
			</p>
		</example>
		<example>
			<p>
				The undamped oscillator <m>x''+16x = 40e^{-2t}</m> with <m>x(0)=x'(0)=0</m> has a solution whose transform is 
				<me>
					X(s) = \frac{40}{(s+2)(s^2+16)} = \frac{A+Bs}{s^2+16^2} + \frac{C}{s+2}
				</me>. 
				Clearing the denominators gives the identity
				<me>
					(A+Bs)(s+2) + C(s^2+16) = 40
				</me>. 
				We need three equations to determine the unknown coefficients. Setting <m>s=-2</m> quickly gives <m>20C=40</m>, so <m>C=2</m>. Now the identity reads 
				<me>
					Bs^2 + (A+2B)s + 2A = 8 - 2s^2. 
				</me>
				Equating the coefficients of <m>1</m> and <m>s^2</m> immediately tells us <m>2A=8</m> and <m>B=-2</m>. Hence 
				<me>
					X(s) = \frac{4-2s}{s^2+16^2} + \frac{2}{s+2} 
				</me>, 
				and finally 
				<me>
					x_p(t) = \sin(4t) - 2\cos(4 t) + 2e^{-2t}
				</me>.
			</p>
		</example>
		<p>
			We may also have to deal with two sets of imaginary poles. 
		</p>
		<example>
			<p>
				The driven, undamped oscillator <m>x''+\omega_0^2 x = \cos(\omega t)</m> yields 
				<me> 
					X(s) = \frac{sx(0)+x'(0)}{(s^2+\omega_0^2)} + \frac{s}{(s^2+\omega_0^2)(s^2+\omega^2)} 
				</me>, 
				which has a PFD with four real constants: 
				<me>
					X(s) = \frac{As+B}{s^2+\omega_0^2} + \frac{Cs+D}{s^2+\omega^2}. 
				</me>
				The first term is homogeneous and tells us nothing new about the general solution. So we zero out <m>A,B</m> for a particular solution and clear the denominators to get
				<me>
					(sx(0)+x'(0))(s^2+\omega^2) + s = (Cs+D)(s^2+\omega_0^2). 
				</me>
				To avoid dealing with the initial conditions, set <m>s=i\omega</m>: 
				<me>
					i\omega = (iC\omega + D)(\omega_0^2-\omega^2). 
				</me>
				The real part of this equation tells us that <m>D=0</m>, and the imaginary part tells us <m>C=1/(\omega_0^2-\omega^2)</m>. Hence
				<me>
					x_p(t) = \frac{1}{\omega_0^2-\omega^2}  \cos(\omega t). 
				</me>
				This is combined with <m>\cos(\omega_0 t)</m> and <m>\sin(\omega_0 t)</m> to get the general solution.
			</p> 
		</example>
		<p>
			If all this is starting to smell like the method of undetermined coefficients to you, there's good reason. The <m>x_p</m> we targeted in these examples is the same one that that method produces, and the algebra is virually identical, though arrived at differently. 
		</p>
		<p>
			We do have to be aware of when the shortcut of zeroing the homogeneous terms does not apply. When delayed by the Shift Theorem, for example, those terms become geniunely different and have to be accounted for.
		</p>
		<example>
			<p>
				To find the general solution of <m>x''+x=5H(t-4)</m>, we take transforms to obtain 
				<me>
					X(s) = \frac{sx(0)-x'(0)}{s^2+1} + \frac{5e^{-4s}}{s(s^2+1)}. 
				</me>
				The first two terms give a combination of <m>\cos(t)</m> and <m>\sin(t)</m> to contribute to the homogeneous solution. The second is in the form <m>5e^{-4s}F(s)</m>, where 
				<me>
					F(s) = \frac{1}{s(s^2+1)} = \frac{A}{s} + \frac{Bs+C}{s^2+1}. 
				</me>
				We would love to discard the second PF term, but because we are going to use the shift theorem on <m>f(t)</m>, we have to keep them all. Clearing denominators we get 
				<me>
					1 = A(s^2+1) + (Bs+C)(s). 
				</me>
				Setting <m>s=0</m> gives us <m>A=1</m> right away. Then <m>-s^2=Bs^2+Cs</m>, which tells us that <m>B=-1</m> and <m>C=0</m>. Hence 
				<me>
					f(t) = 1 -\cos(t). 
				</me>
				So by the shift theorem, a particular solution is 
				<me>
					x_p(t) = 5H(t-4)f(t-4) = 5 H(t-4)[1-\cos(t-4)]. 
				</me>
				The general solution is <m>c_1\cos(t) + c_2\sin(t) + x_p(t)</m>. 
			</p>
		</example>
	</subsection>
	
</section>

<section xml:id="so-laplace-repeated">
	<title>Repeated and complex poles</title>

	<subsection>
		<title>Complex poles</title>
		<p> 
			Complex poles in the transform correspond to complex exponentials in the time domain. When they occur in conjugate pairs, the entire expression can be cast in real terms. It can help to use the shift formula 
			<me>
				\lx[e^{at}f(t)] = F(s-a)
			</me>.
			We should have enough information from the ODE to know what to choose as the real shift <m>a</m>. 
		</p>
		<example>
			<p>
				Suppose <m>x''+2x'+10x=6\delta(t-1)</m>, with zero initial conditions. Then  
				<me>
					X(s) = \frac{6e^{-s}}{s^2+2s+10}
				</me>.
				The quadratic formula tells us the homogeneous eigenvalues <m>-1\pm 3i</m>, so we assume a shift of <m>-1</m> and designate <m>X_p(s)=G(s+1)</m>. Thus
				<me>
					G(s) = X_p(s-1) = 6e \frac{e^{-s}}{s^2-2s+1+2s-2+10} = 6e \frac{e^{-s}}{s^2+9}.
				</me>
				Now we can apply the Shift Theorem to <m>G</m> and get 
				<me>
					g(t) = 6e H(t-1) v(t-1),
				</me>
				where <m>v(t)=\sin(3t)/3</m>. When we unwind the definitions, we get 
				<me>
					x_p(t) = e^{-t} g(t) = 2 e^{-t+1} H(t-1)\sin(3t-3).
				</me>
				Are we having fun yet?
			</p>
			<p>
				Now consider <m>x''+2x'+10x=40</m>. We get 
				<me>
					X_p(s) = \frac{40}{s(s^2+2s+10)} 
				</me>. 
				We have the same eigenvalues and so define <m>X_p(s)=G(s+1)</m> to get 
				<me>
					G(s) = X_p(s-1) = \frac{40}{(s-1)(s^2+9)}.
				</me>
				This is amenable to the PFD 
				<me>
					G(s) = \frac{A}{s-1} + \frac{B+Cs}{s^2+9}
				</me>. 
				Clearing denominators, we get 
				<me> 
					40 = A(s^2+9) + (B+Cs) (s-1)
				</me>.  
				If we set <m>s=1</m> we immediately see that <m>A=4</m>. That leaves 
				<me>
					40 = (4+C)s^2 +(B-C)s + 36-B
				</me>. 
				Now we can read off <m>B=-4</m>, <m>C=-4</m>. Thus 
				<me>
					g(t) = 4e^{t} - 4 \cos(3t) - \frac{4}{3}\sin(3t). 
				</me>
				Finally, 
				<me>
					x_p(t) = e^{-t}g(t) = 4 - 4  e^{-t}\cos(3t) - \frac{4}{3} e^{-t}\sin(3t).
				</me>
			</p>
			<p>
				For a harmonically forced case such as <m>x''+2x'+10x=\cos(2t)</m>, we have 
				<me>
					X_p(s) = \frac{s}{(s^2+4)(s^2+2s+10)}
				</me>. 
				The best first step is to decompose 
				<me> 
					X_p(s) = \frac{A+Bs}{s^2+4} + \frac{C+Es}{s^2+2s+10}
				</me>. 
				The first term leads directly to <m>\sin(2t)</m> and <m>\cos(2t)</m>, while the second would once again be shifted to get <m>s^2+9</m> in the denominator. The details are left to the proverbial reader.
			</p>
		</example>
		<p> 
			The drawback to Laplace transforms as a one-stop solver is what we observed earlier: the particular solution we find virtually always includes some terms from the homogeneous solution. Mathematically, the poles of <m>X_p(s)</m> include those of the transfer function <em>and</em> the forcing function. This combination makes the partial fraction decompositions lengthier, to say the least. Undetermined coefficients is a less sexy but simpler way to solve concrete problems in most cases, especially if one seeks a general solution without initial conditions to consider at the end.
		</p>
	</subsection>

	<subsection>
		<title>Repeated poles and resonance</title>
		<p>
			Repeated poles in the Laplace transform of a solution appear in the form of denominator terms such as <m>(s-a)^k</m> or <m>(s^2+\omega^2)^k</m> with <m>k \gt 1</m>. They can arise in two ways: from repeated eigenvalues in the homogeneous or unforced ODE, or because the forcing function has a pole in common with one of the eigenvalues (e.g., perfect resonance). 
		</p>
		<p>
			Consider inverting the double real pole
			<me>
				X(s) = \frac{1}{(s-a)^2}
			</me>. 
			The key is to note that <m>X(s)=-F'(s)</m>, where <m>F(s) = (s-a)^{-1}</m> is the transform of <m>e^{at}</m>. From the transform definition we compute
			<md>
				<mrow>-F'(s) \amp = -\frac{d}{ds} \int_0^\infty f(t) e^{-s t}\, dt</mrow>
				<mrow>\amp = \int_0^\infty t f(t) e^{-s t}\, dt</mrow>
				<mrow>\amp = \lx[ t f(t) ] = \lx[ t e^{at} ] </mrow>
			</md>. 
			If we keep differentiating in <m>s</m> we find 
			<men xml:id="so-eq-repeatedexp">
				\lx[t^n e^{at} ] = \frac{n!}{(s-a)^{n+1}}.
			</men>
			The special case <m>a=0</m> allows us to transform any power of <m>t</m>, and therefore any polynomial. 
		</p>
		<example>
			<p>
				The critically damped oscillator <m>x'' + 6x' + 9=0</m> has a double eigenvalue at <m>-3</m>. To find the solution with <m>x(0)=0</m>, <m>x'(0)=6</m>, we use the transform to find 
				<me>
					[s^2X(s) - 6] + 6[sX(s)] + 9[X(s)] = 0 \quad \Rightarrow \quad X(s) = \frac{6}{(s+3)^2}.
				</me>
				Hence <xref ref="so-eq-repeatedexp"/> applies with <m>n=1</m>, and 
				<me>
					x(t) = 6t e^{-3t}
				</me>.			
			</p>
		</example>
		<p>
			If you look carefully at the derivation of <xref ref="so-eq-repeatedexp"/>, you will see that the identity of <m>f(t)</m> played no role in the key steps. As shown in <xref ref="so-tab-laplace"/>, we can repeat the calculation for a generic <m>F(s)</m>: 
			<me>
				\lx[t^n f(t) ] = (-1)^n F^{(n)}(s), \qquad n=1,2,3,\dots.
			</me>
			Take a moment to appreciate this. Earlier we found that a derivative in <m>t</m> essentially corresponds to multiplying by <m>s</m>. Up to a change in sign, a derivative in <m>s</m> corresponds to multiplying by <m>t</m>! 
		</p>
		<example>
			<p>
				The undamped oscillator <m>x''+9x=\sin(3t)</m> has a perfect resonance. The transform leads to 
				<me>
					X(s) = \frac{sx(0)+x'(0)}{s^2+9} + \frac{3}{(s^2+9)^2}
				</me>. 
				We are close to being able to use 
				<me>
					\frac{d}{ds} \left[ \frac{3}{s^2+9} \right] = \frac{-6s}{(s^2+9)^2}.
				</me>
				In fact, if we designate <m>G(s)=2sX_p(s)</m>, then we know that  
				<me>
					\lx[ t \sin(3t) ] = -\frac{d}{ds} \left[ \frac{3}{s^2+9} \right] = G(s),
				</me>
				and therefore <m>g(t)=t\sin(3t)</m>. Next, note that 
				<me>
					2\lx[x_p'(t)] = 2[sX_p(s)-X_p(0)] = G(s).
				</me>
				Hence <m>x_p'(t) = \frac{1}{2} t \sin(3t)</m>, and integration by parts yields
				<me>
					x_p(t) = \frac{1}{18} [ \sin(3t) - 3t\cos(3t) ]
				</me>.
			</p>
		</example>
		<example>
			<p>
				To find the general soluton of <m>x'' - 4x + 4x = 5\cos(t)</m>, we note that the characteristic polynomial is <m>s^2-4s+4=(s-2)^2</m>. We take transforms to obtain 
				<me>
					X(s) = \frac{sx(0)+x'(0)}{(s-2)^2} + \frac{5s}{(s^2+1)(s-2)^2} .
				</me>
				The PFD of this has the real form 
				<me>
					X(s) = \frac{A}{s-2} + \frac{B}{(s-2)^2} + \frac{Cs+D}{s^2+1}.
				</me>
				The first two terms constitute the homogeneous solution, and we zero out their coefficients and replace <m>X</m> with <m>X_p</m>. Equating to the earlier expression and clearing denominators gives 
				<me>
					(sx(0)+x'(0))(s^2+1) + 5s = (Cs+D)(s-2)^2.
				</me>
				We don't care about the initial values and want to make them vanish. So we choose an imaginary value of <m>s</m>: 
				<md>
					<mrow> s = i \amp \Rightarrow 5i = (Ci+D)(i-2)^2  = (3-4i)(D+Ci) = (3D+4C) + i(3C-4D)  </mrow>
				</md>.
				Equating the real and imaginary parts gives <m>4C+3D=0</m>, <m>3C-4D=5</m>. Now our particular solution satisfies
				<me>
					X_p(s) = \frac{3}{5} \frac{s}{s^2+1} - \frac{4}{5} \frac{1}{s^2+1}. 
				</me>
				Hence 
				<me>
					x_p(t) = \frac{3}{5} \cos(t) - \frac{4}{5} \sin(t)
				</me>
				and finally 
				<me>
					x(t) = c_1 e^{2t} + c_2 t e^{2t} + \frac{3}{5} \cos(t) - \frac{4}{5} \sin(t). 
				</me>
			</p>
		</example>

		<p>
			As the preceding example shows, dealing with repeated poles is no easy feat, especially when quadratic factors are in the denominator. Another tactic is to break up those factors as imaginary poles and use standard partial fractions. 
		</p>
		<example>
			<p>
				Again we consider how to invert
				<me>
					X_p(s) = \frac{3}{(s^2+9)^2} = \frac{3}{(s-3i)^2(s+3i)^2} 
				</me>. 
				The PFD has four terms that work out as 
				<me>
					X_p(s) = \frac{1}{36} \left[ \frac{i}{s+3i} - \frac{i}{s-3i} - \frac{3}{(s+3i)^2} - \frac{3}{(s-3i)^2} \right]. 
				</me>
				These easily invert using complex exponentials: 
				<me>
					x_p(t) = \frac{1}{36} \left[ i\bigl(e^{-3it}-e^{3it}\bigr) - 3 t \bigl(e^{-3it}+e^{3it}\bigr)  \right]. 
				</me>
				Now Euler' formula leads back to the same result,
				<me>
					x_p(t) = \frac{1}{18} [ \sin(3t) - 3t\cos(3t) ]
				</me>.
			</p>
		</example>
	</subsection>
</section>

<section xml:id="so-convolutions">
	<title>Convolutions</title>
	<introduction>
		<p>
			
		</p>
	</introduction>
	
	<subsection>
		<title>Moving averages</title>
		<p>
			The following grabs and plots the closing price of Bitcoin for the last 31 days.
		</p>
			<sidebyside>
			<listing  xml:id="so_conv_bitcoin1">
	 			<program language="matlab">
	    			<input>
%bc = webread('https://api.coindesk.com/v1/bpi/historical/close.json');
%data = jsondecode(bc);
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-o')
					</input>
				</program>
				</listing>
				<image source="figures/so_conv_bitcoin1.svg"/>	  
			</sidebyside>
			<p>
				It's a noisy curve. We can smooth that out by taking 4-day moving averages, for example. I don't claim that this is the best way, but we can do this by:
			</p>
			<sidebyside>
				<listing  xml:id="so_conv_bitcoin2">
	 			<program language="matlab">
	    			<input>
%bc = webread('https://api.coindesk.com/v1/bpi/historical/close.json');
%data = jsondecode(bc);
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-ko')
for i = 4:31
    z(i) = (v(i) + v(i-1) + v(i-2) + v(i-3)) / 4;
end
z(1:3) = NaN;  % not a number
hold on, plot(z,'-o')
					</input>
				</program>
				</listing>
				<image source="figures/so_conv_bitcoin2.svg"/>	  
			</sidebyside>

			<p>
				We might decide, however, to weight the most recent values more heavily. Here's how this might look.
			</p>
			<sidebyside>
				<listing xml:id="so_conv_bitcoin3">
	 			<program language="matlab">
	    			<input>
%bc = webread('https://api.coindesk.com/v1/bpi/historical/close.json');
%data = jsondecode(bc);
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-ko')
w = [4 3 2 1];
for i = 4:31
    z(i) = (w(1)*v(i) + w(2)*v(i-1) + w(3)*v(i-2) + w(4)*v(i-3)) / sum(w);
end
z(1:3) = NaN;  % not a number
hold on, plot(z,'-o')
					</input>
				</program>
				</listing>
				<image source="figures/so_conv_bitcoin3.svg"/>	  
			</sidebyside>
			<p>
				Note that each new value <m>z_i</m> is a linear combination of the elements of <m>v</m>, weighted by the values in <m>w</m>. And we always go forward 1,2,3,4 in <m>w</m>, while on <m>v</m> we have a sliding group going backward: <m>i,i-1,i-2,i-3</m>. Put concisely, the indices add up to the constant value <m>i+1</m>. 
			</p>
	</subsection>

	<subsection>
		<title>Convolution integral</title>
		<p>
			The moving average above gave us a certain way to multiply together vectors <m>v</m> and <m>w</m> to get a new vector <m>z</m>: 
			<me>
				z_i =  \sum_{j} w_j v_{i-j} 
			</me>
			(although I reindexed the output vector from <m>i+1</m> to <m>i</m> compared to the computer code). This is analogous to a different way to multiply two <em>functions</em>, by the <term>convolution integral</term>
			<me> 
				[f*g](t) = \int_0^t f(t-\tau)g(\tau)\, d\tau
			</me>. 
			This too is like a weighted average, with the values of <m>g(\tau)</m> being multiplied against different windows of the function <m>f</m> such that the arguments sum to <m>t</m>. 
		</p>
		<p> 
			An interesting fact is that while I interpreted <m>f*g</m> as <m>g</m> acting on <m>f</m>, in fact the operation is symmetric:
			<md>
				<mrow>[g*f](t) \amp = \int_0^t f(t-\tau)g(\tau)\, d\tau</mrow>
				<mrow>\amp = \int_t^0 f(u)g(t-u)\, (-du)  </mrow>
				<mrow>\amp = \int_0^t g(t-u)f(u)\, du = [f*g](t)</mrow>
			</md>. 
			We can also easily prove some more properties we really like to have:
			<md>
				<mrow>f * ( g * h ) = ( f * g) * h </mrow>
				<mrow>f * (g+h)=(f * g)+(f*h)</mrow> 
				<mrow>f * 0=0</mrow>
			</md>. 
			It is <em>not</em> true, however, that <m>f*1=f</m>, unless <m>f</m> is the zero function. Instead, the right formula is <m> f * \delta=f</m>: 
			<me> 
				[f*\delta](t) = \int_0^t f(t-\tau) \delta(\tau)\, d\tau = f(t)
			</me>. 
		</p>
	</subsection>

	<subsection>
		<title>Convolution theorem</title>
		<p>
			Here is what makes convolution a big deal. 
		</p>
		<theorem>
			<p>
				Suppose <m>\lx[f] = F(s)</m>, <m>\lx[g] = G(s)</m>, <m>h=f*g</m>, and <m>\lx[h] = H(s)</m>. Then <m>H(s)=F(s)G(s)</m>.
			</p>
		</theorem> 
		<p> 
			Why does this matter? Consider finding a particular solution of <m>x''+bx'+cx=f(t)</m>. After taking transforms we get  
			<me> 
				X_p(s) = \frac{1}{s^2+bs+c} F(s) = G(s) F(s)
			</me>, 
			where <m>G(s)</m> is the transfer function. Right away then, we know that <m>x_p(t)=f(t)*g(t)</m> is a particular solution. 
		</p>
		<p>
			But wait, there's more! Consider the problem of an impulse response, <m>x''+bx'+cx=\delta(t)</m>, with zero initial conditions. The solution of this problem has <m>X(s)=G(s)</m>. That is, 
		</p>
		<fact>
			<p>The impulse response of a linear constant-coefficient equation is the inverse Laplace transform of the transfer function.</p>
		</fact>
		<p> 
			Put it all together and you have some important theoretical connections. The solution to any forced problem is a convolution of the forcing function with the impulse response in the time domain, and the product of the forcing and the transfer function in the <m>s</m> domain. Finally, the transfer function is just the reciprocal of the characteristic polynomial. 
		</p>
	</subsection>
</section>


</chapter>