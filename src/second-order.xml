<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch4-second-order-linear" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>Second-order ODEs</title>

<section xml:id="so-convert">
	<title>Order equals dimension</title>
	<p>
		We now turn our attention to second-order ODEs of the form 
		<me>
			x''=f(t,x,x')
		</me>. 
		Most of what we develop extends easily to vector-valued <m>x</m>  and equations of order higher than two, but we focus on this problem to keep notation simple. 
	</p>
	<p>
		Our first observation is that this problem can always be converted to a first-order problem in two dimensions. Define a vector <m>\bu</m> by <m>u_1=x</m>, <m>u_2=x'</m>. By definition, <m>u_1'=u_2</m>. Since <m>u_2'=x''=f(t,x,x')</m>, we can get the system 
		<md>
			<mrow> u_1' \amp =  u_2</mrow>
			<mrow> u_2' \amp =  f(t,u_1,u_2).</mrow>
		</md>
		This is a first-order system <m>\bu'=\mathbf{g}(t,\bu)</m> of two scalar equations for the two components. 
	</p>
	<example>
		<p>
			A pendulum is governed by the nonlinear equation <m>\theta''+ b\theta' + \frac{g}{L}\sin(\theta)=0</m>. Define <m>u_1=\theta</m>, <m>u_2=\theta'</m>. Then 
			<me>
				\twovec{u_1'}{u_2'} = \twovec{u_2}{-bu_2 - \frac{g}{L}\sin(u_1)}
			</me>
			is an equivalent system.
		</p>
	</example>
	<p>
		Our main item of business is the linear, constant-coefficient model problem 
		<men xml:id="so-eq-linearmodel">
			x'' + bx' + cx = f(t)
		</men>,
		for which the conversion process with <m>u_1=x</m>, <m>u_2=x'</m> yields 
		<men xml:id="so-eq-matrixmodel">
			\twovec{u_1'}{u_2'} = \twovec{u_2}{f(t)-cx_1-bx_2} = \twomat{0}{1}{-c}{-b} \bu + \twovec{0}{f(t)}
		</men>,
		which has the form <m>\bu'=\bA\bu+\bff(t)</m>. Thus everything from the last chapter is again in play.
	</p>
</section>

<section xml:id="so-homogeneous">
	<title>Homogeneous problem</title>
	<p>
		The general solution of <m>x'' + bx' + cx = f(t)</m> consists of the general solution of the homogeneous (unforced) problem, and a particular solution of the forced problem. For the homogeneous part the relevant matrix is from <xref ref="so-eq-matrixmodel"/>: 
		<me>
			\bA = \twomat{0}{1}{-c}{-b}
		</me>.
		We have the characteristic polynomial 
		<me>
			|\bA-\lambda\bI| = \twodet{-\lambda}{1}{-c}{-b-\lambda} = \lambda^2 + b\lambda + c
		</me>.
		(This is easily remembered in terms of the original ODE <m>x'' + bx' + cx=0</m>.) The eigenvalues can therefore be found from the quadratic formula. 
	</p>
	<p>
		To find the eigenvectors we look for the nullspaces of <m>\bA-\lambda_j\bI</m> for <m>j=1,2</m>. These have the basis vectors <m>[1,\,\lambda_j]</m>, if the eigenvalues are distinct. Thus the general homogeneous solution is 
		<me>
			\bu_h(t) = c_1 e^{\lambda_1 t} \twovec{1}{\lambda_1} + c_2 e^{\lambda_2 t} \twovec{1}{\lambda_2}
		</me>. 
		In terms of the original variable <m>x=u_1</m>, this is simply 
		<me>
			x_h(t) = c_1 e^{\lambda_1 t} + c_2 e^{\lambda_2 t}, \qquad \text{if } \lambda_1\neq\lambda_2
		</me>.
	</p>
	<p>
		We need to look more carefully at the case of a repeated eigenvalue. In terms of the original ODE, this occurs when <m>b^2=4c</m>, and then <m>\lambda_1=\lambda_2=-b/2</m>. We recall the trick used in <xref ref="fs-me-defective"/> and define 
		<me>
			\bB = \bA + \frac{b}{2}\bI = \twomat{b/2}{1}{-c}{-b/2} = \twomat{b/2}{1}{-b^2/4}{-b/2}
		</me>. 
		You can easily check that <m>\bB^2=\bzero</m>. Hence 
		<me>
			e^{t\bA} = e^{-(bt/2)\bI} e^{t\bB} = e^{-bt/2}(\bI + t\bB) 
		</me>. 
		The general solution is <m>e^{t\bA}\bc</m>, but again we need only the first row to get the original <m>x=u_1</m>: 
		<me>
			x_h(t) = e^{-bt/2} \left[ c_1 + t\left(c_2 + c_1\frac{b}{2}\right) \right] = e^{-bt/2} \left( c_1 + C_2 t\right)
		</me>. 
	</p>
</section>

<section xml:id="so-undetermined">
	<title>Undetermined coefficients</title>
	<p>
		As always in linear equations, the general solution of an ODE with a forcing term is the homogeneous solution plus a particular solution. Since a second-order problem has a first-order system equivalent, we can use variation of parameters for a system, as in <xref ref="fs-varparam"/>, to find a particular solution. However, the algebra for doing this is quite messy. For many common second-order problems with constant coefficients, it can be much easier to return to the method of undetermined coefficients described in <xref ref="fl-undeter"/>. 
	</p>
	<p>
		Specifically, the ODE <m>L[x] = ax''(t)+bx'(t)+cx(t)=f(t)</m> can be solved with this method when <m>f</m> is a polynomial, exponential, sin, or cos function. The appropriate form for the particular <m>x_p</m>  for each case is given in <xref ref="fl-tb-undeter"/>. This form is plugged into <m>L[x]=f</m>, and the equality is used to deduce the undetermined coefficients in <m>x_p</m>. 
	</p>
	<example>
		<p>Let's find a particular solution of <m>x'' +4x'+4x=8t^2</m>. The correct form of <m>x_p</m> is a quadratic polynomial, like <m>f</m>, so 
		<me>
			x_p = A + Bt + Ct^2
		</me>. 
		Plugging that into the ODE yields
		<md>
			<mrow>2C + 4(B+2Ct) + 4(A+Bt+Ct^2) \amp = 8t^2</mrow>
			<mrow>4C t^2 + (8C+4B) + (2C+4B+4A) \amp = 8t^2</mrow>
		</md>. This has to be an identity for all <m>t</m>. Matching like powers, we can read off <m>C=2</m>, then <m>B=-4</m>, and finally <m>A=3</m>. This provides us with <m>x_p=2t^2+4t+3</m>. 
		</p>
	</example>
	<!--example>
		<p>Let's try for <m>y'' -2y'-3y=10te^{4t}</m>. For much the same reasons as in the last case, we try 
		<me>y_p = (A + Bt)e^{4t}</me>. There's nothing to do now except grind out some derivatives. 
		<md>
			<mrow>8(2A+B+2Bt)e^{4t} - 2(4A+B+4Bt)e^{4t} -3 (A + Bt)e^{4t} \amp =10t e^{4t}</mrow>
			<mrow> 8(2A+B) - 2(4A+B) -3A + t(16B-8B-3B) =  10t</mrow>
		</md>. From this, by matching powers, we deduce <m>B=2</m> and then <m>5A+12=0</m>. So <m>y_p = [2t - (12/5)]e^{4t}</m>.
		</p>
	</example-->
	<example>
		<p>
			For <m>x'' - 2x'-3x=10e^{4t}</m>, the proper choice is 
			<me>
				x_p = Ae^{4t}
			</me>. Everything else is just basic manipulations.  
			<md>
				<mrow>16A e^{4t} - 2 (4A e^{4t}) - 3 (Ae^{4t}) \amp =10 e^{4t}</mrow>
				<mrow> 16A -8A -3A =  10</mrow>
			</md>. From this it's clear that <m>A=2</m>.
			</p>
	</example>
	<example>
		<p>
			For <m>x''+x'=\sin(2t)</m>, we use 
			<me>
				x_p = A\cos(2t) + B\sin(2t)
			</me>. 
			Inserting this into the ODE leads to 
			<me>
				[-4A\cos(2t) - 4B\sin(2t)] + [-2A\sin(2t) + 2B\cos(2t)] = \sin(2t)
			</me>. 
			This is true for all time if and only if we match the coefficients of like trig functions:
			<me>
				-4A + 2B = 0, \qquad -4B-2A = 1 
			</me>. 
			The solution of these equations is <m>A=-1/10</m>, <m>B=-1/5</m>.
		</p>
	</example>
	<p>
		The examples above are the basic ones. There are rules for more intricate combinations of the same functions, but we won't go into them here. 
	</p>
	<p>
		We have to repeat the warning from the first time we saw this method: occasionally it fails.
	</p>
	<example xml:id="so-ex-noundeter">
		<p>
			The equation <m>x''+x=\cos(t)</m> is supposed to suggest the particular solution <m>x_p=A\cos(t)+B\sin(t)</m>. But look at what happens after substitution:
			<md>
				<mrow>[-A\cos(t) - B\sin(t) ] + [ A\cos(t) + B\sin(t)] \amp =\cos(t)</mrow>
				<mrow> 0 = \cos(t)</mrow>
			</md>. This leads us nowhere good. 
		</p>
	</example>
	<p>
		The failure of <xref ref="so-ex-noundeter"/> was due to the fact that the <m>x_p</m> we picked is actually a homogeneous solution (a fact we inadvertently proved). There is a way to fix this method in that circumstance, but we will appeal to alternative routes. 
	</p>
</section>

<section xml:id="so-oscillators">
	<title>Oscillators</title>
	<introduction>
		<p>
			The most common use of second-order equations is the modeling of oscillatory or other periodic behavior.
		</p>
	</introduction>
	<subsection xml:id="so-os-mechanical">
		<title>Mechanical</title>
		<p>
			As we have already seen, a mass hanging from a spring satisfies the ODE
			<me>mx'' + b x' + kx = F(t)</me>.
			(As is typical, all the named constants are nonnegative.) Here <m>m</m>> is the mass, <m>b</m>>is a coefficient of friction or other damping forces, and <m>k</m>> is a characteristic constant of the spring. The term <m>F(t)</m> represents an external driving force. If we have initial conditions, they give values for <m>x</m> and <m>x'</m>.
		</p>
		<p>
			The gravitational force on a pendulum bob (think child on a swing) has a related equation. The proper ODE is 
			<me>\theta'' + \gamma \theta + \frac{g}{L}\sin \theta = F(t)</me>,
			where <m>\theta(t)</m> is the angle made with vertical-down and <m>L</m> is the arm length. This equation is nonlinear. But if the angle remains small, then a reasonable approximation is 
			<me>\theta'' + \gamma \theta + \frac{g}{L}\theta = F(t)</me>, 
			which is a linear oscillator. 
		</p>
	</subsection>
	<subsection>
		<title>Electrical oscillators</title>
		<p>
			An AC circuit often has elements of resistance, capacitance, and inductance. These analogize perfectly to friction/damping, spring constant, and mass. If these elements are wired in series with a generator, the governing ODE is
			<me>LI'' + RI' + CI = \mathcal{E}'(t)</me>,
			where <m>L</m> is inductance, <m>R</m> is resistance, <m>C</m> is capacitance, and <m>I(t)</m> is the current flowing through the circuit. The forcing term represents the generator whose voltage (EMF) is <m>\mathcal{E}(t)</m>. For alternating current this is modeled as a cosine (or complex exponential) function.  
		</p>
		<p>
			In DC circuits we have Ohm's Law, <m>I=V/R</m>, for a resistance <m>R</m>. In the AC case, if <m>\mathcal{E}(t)=Ve^{i\omega t}</m>, then <m>e^{i\omega t}</m> appears in the current as well, and it still makes sense to talk about <m>V/I</m> as a kind of resistance, now called <term>impedance</term> and dependent on the frequency <m>\omega</m>. 
		</p>
	</subsection>
	<subsection xml:id="so-os-notation">
		<title>Unifying notation</title>
		<p>
			When you have lots of different versions of the same root problem, with different symbols and units, you have three options. You can solve each new problem from scratch, you can derive custom formulas for each application, or you can try to find a minimal set of parameters and express each problem using those. Let's consider the last pathway here.
		</p>
		<p>
			The mechanical oscillator <m>mx'' + bx' + kx = F(t)</m> is more conveniently expressed as
			<me> 
				x'' + \frac{b}{m} x' + \frac{k}{m} x = \frac{1}{m} F(t)
			</me>.
			One reason to prefer this form is dimensional analysis: <m>x</m> must have the same units as  <m>F/m</m>, and both <m>b/m</m> and <m>\sqrt{k/m}</m> have units of inverse time. We introduce new parameters 
			<me>
				\omega_0 = \sqrt{\frac{k}{m}} \gt 0, \quad Z = \frac{1}{2} \frac{b/m}{\omega_0} = \frac{b}{\sqrt{4km}}\ge 0
			</me>.
			<m>\omega_0</m> is known as the <idx>natural frequency</idx><term>natural frequency</term>, with units of inverse time, and <m>Z</m> is a dimensionless number describing the intensity of the damping. Now the ODE becomes 
			<men xml:id="so-eq-unified"> 
				x'' + 2 Z \omega_0 x' + \omega_0^2 x = \omega_0^2 F(t)
			</men>.
			A similar derivation can be done starting from the electrical circuit equation.
		</p>
		<aside>
			<p>
				In math we usually use <q>frequency</q> to mean the multiplier of <m>t</m> in a sin or cos function. In some fields this is called <term>angular frequency</term> and <q>frequency</q> is used to mean the number of cycles per time unit, as in Hz. 
			</p>
		</aside>
		<p>
			The eigenvalues of the homogeneous form of this equation are  
			<me>
				\lambda_{1,2} = -Z \omega_0 \pm \omega_0 \sqrt{Z^2-1}
			</me>. 
			The discussion now splits into four cases marked by increasing values of <m>Z</m>. 
		</p>
	</subsection>
	<subsection xml:id="so-os-undamped">
		<title>Undamped</title>
		<p>
			The first case for <xref ref="so-eq-unified"/> is <m>Z=0</m>, which is the idealization of no mechanical damping or friction (or resistance, in a circuit). The homogeneous equation is <m>x''+\omega_0^2x=0</m>, which has eigenvalues <m>\pm i\omega_0</m> and general solution 
			<me>
				x_h(t) = c_1 e^{i\omega_0 t} + c_2 e^{-i\omega_0 t} = a_1 \cos(\omega_0 t) + a_2 \sin(\omega_0 t)
			</me>. 
			Thus the undamped case results in pure oscillation at frequency <m>\omega_0</m>. 
		</p>
	</subsection>
	<subsection xml:id="so-os-underdamped">
		<title>Underdamped</title>
		<p>
			For <m>0\lt Z \lt 1</m> the eigenvalues of <xref ref="so-eq-unified"/> are complex:
			<me>
				\lambda_{1,2} = -Z \omega_0 \pm i \omega_0 \sqrt{1-Z^2}
			</me>. 
			Defining <m>\omega_d=\omega_0 \sqrt{1-Z^2}</m>, the general homogeneous solution is therefore 
			<me>
				x_h(t) = e^{-Z\omega_0 t} \left( c_1 e^{i\omega_d t} + c_2 e^{-i\omega_d t} \right)
			</me>, 
			or the equivalent real form with cos and sin. This solution is pseudoperiodic, combining oscillation at frequency <m>\omega_d \lt \omega_0</m> inside an exponential decay envelope. This situation is called an <idx>underdamped oscillator</idx><term>underdamped oscillator</term>. 
		</p>
	</subsection>
	<subsection xml:id="so-os-critical">
		<title>Critically damped</title>
		<p>
			At <m>Z=1</m> the complex eigenvalues collapse to a double eigenvalue at 
			<me>
				\lambda_1 = \lambda_2 = -\omega_0
			</me>,
			with general homogeneous solution 
			<me>
				x_h(t) = e^{-\omega_0 t} (c_1 + c_2 t)
			</me>. 
			There is no longer any oscillation present, and we have a <idx>critically damped oscillator</idx><term>critically damped oscillator</term>. The linear growth of <m>c_2 t</m> doesn't make much difference against the exponential decay. 
		</p>
	</subsection>
	<subsection xml:id="so-os-overdamped">
		<title>Overdamped</title>
		<p>
			For <m>Z>1</m> the eigenvalues 
			<me>
				\lambda_{1,2} = -Z \omega_0 \pm \omega_0 \sqrt{Z^2-1}
			</me> 
			are negative and real, thus giving an exponentially decaying homogeneous solution. In this case we have an <idx>overdamped oscillator</idx><term>overdamped oscillator</term>. 
		</p>
	</subsection>
</section>

<section xml:id="so-expforcing">
	<title>Driven oscillator</title>
	<introduction>
		<p>
			We next want to examine <xref ref="so-eq-unified"/> when the forcing function is an exponential: 
			<men xml:id="so-eq-unifiedexp"> 
				x'' + 2 Z \omega_0 x' + \omega_0^2 x = \omega_0^2 e^{rt}
			</men>.
			We're interested in the undamped and underdamped cases, <m>0\le Z \lt 1</m>. 
		</p>
		<p>
			It's possible to use variation of parameters as in <xref ref="fs-varparam"/> to derive a particular solution, but it's overkill here. Instead we just appeal to the method of undetermined coefficients, positing a solution in the form <m>x_p(t) = A e^{rt}</m> to derive
			<me>
				(r^2  + 2 Z \omega_0 r + \omega_0^2)Ae^{rt} = \omega_0^2 e^{rt}
			</me>. 
			Hence 
			<men xml:id="so-eq-coeff">
				A(r) = \frac{\omega_0^2}{r^2  + 2 Z \omega_0 r + \omega_0^2}
			</men>
			gives a particular solution unless the denominator is zero, i.e., <m>r</m> is an eigenvalue (since the denominator is the characteristic polynomial). More on that situation later. 
		</p>
	</introduction>
	<subsection xml:id="so-ef-harmonic">
		<title>Harmonic forcing</title>
		<p>
			Exponential forcing <m>F(t)=e^{rt}</m> is particulary important when <m>r</m> is imaginary. Say <m>r=i\omega</m>. Then by Euler's identity we have 
			<me>
				F(t) = \cos(\omega t) + i\sin(\omega t)
			</me>. 
			Thanks to linearity, if given a cos or sin forcing, we can solve the complex-valued problem and then take a real or imaginary part. That is, if <m>x_p(t) = A e^{i\omega t}</m> as above, then
			<md>
				<mrow>[\Re x_p]'' + 2 Z \omega_0 [\Re x_p]' + \omega_0^2 [\Re x_p] \amp = \omega_0^2 \cos(\omega t), </mrow>
				<mrow>[\Im x_p]'' + 2 Z \omega_0 [\Im x_p]' + \omega_0^2 [\Im x_p]  \amp = \omega_0^2 \sin(\omega t). </mrow>
			</md>
		</p>
		<p>
			It's often useful to rewrite the complex factor <m>A(i\omega)</m> from <xref ref="so-eq-coeff"/> in polar form: 
			<me>
				A(i\omega) = \frac{\omega_0^2}{\omega_0^2-\omega^2  + 2 Z i \omega_0 \omega } = g(\omega) e^{-i\phi(\omega)}
			</me>,
			where <m>g=|A|</m> is the <idx>gain</idx><term>gain</term> and <m>\phi</m> is the <idx>phase lag</idx><term>phase lag</term>. Note that the particular solution is 
			<me>
				x_p(t) = g(\omega) e^{-i\phi(\omega)} e^{i\omega t} = g e^{i(\omega t-\phi)}
			</me>. 
			This means that as we pass from the input forcing to the output solution, <m>g</m> is the scaling of the amplitude and <m>\phi</m> changes the phase of our trip around the complex unit circle. 
		</p>
	</subsection>
	<subsection xml:id="so-ef-resonance">
		<title>Resonance</title>
		<p>
			Consider forcing <m>e^{i\omega t}</m> in the undamped case with <m>Z=0</m>. Then 
			<me>
				A(i\omega) = \frac{\omega_0^2}{\omega_0^2-\omega^2}
			</me>
			is purely real, and the general solution is 
		</p>
		<fact>
			<title>Solution for undamped driven oscillator (not resonant)</title>
			<p>
				<me>	
					x(t) = x_h(t) + x_p(t) = c_1 e^{i\omega_0 t} + c_2 e^{-i\omega_0 t} + \frac{\omega_0^2}{\omega_0^2-\omega^2} e^{i\omega t}
				</me>
				(or a real equivalent), provided <m>\omega\neq\omega_0</m>
			</p>
		</fact>
		<p>
			If <m>\omega\approx \omega_0</m>, then the amplitude of the particular solution is very large. 
		</p>
		<p>
			What is the meaning if <m>\omega = \omega_0</m>? This is exactly when <m>i\omega</m> is an eigenvalue of the homogeneous problem, so the method of undetermined coefficients will actually fail like in <xref ref="so-ex-noundeter"/>. But there is a sneaky shortcut. For a certain choice of <m>c_1</m> and <m>c_2</m>, we get a different particular solution 
			<me>
				\hat{x}_p(t) = \frac{\omega_0^2}{\omega_0^2-\omega^2} \left( e^{i\omega t} - e^{i\omega_0 t} \right)
			</me>, 
			which has an indeterminate form in the limit <m>\omega \to \omega_0</m>, allowing us to use L'Hopital's Rule: 
			<me>
				\lim_{\omega \to \omega_0} \hat{x}_p(t) = \lim_{\omega \to \omega_0} \omega_0^2 \frac{it e^{i\omega t}}{-2\omega} = -\frac{i\omega_0^2}{2} t e^{i\omega_0 t}
			</me>. 
			(The derivation is subtle because the derivatives here are with respect to the limited variable <m>\omega</m> and not <m>t</m>.) This leads to the general solution 
		</p>
		<fact>
			<title>Solution for undamped driven oscillator at resonance</title>
			<p>
				<me>	
					x(t) = c_1 e^{i\omega_0 t} + c_2 e^{-i\omega_0 t} -\frac{i\omega_0^2}{2} t e^{i\omega_0 t}
				</me>
				(or a real equivalent)
			</p>
		</fact>
		<p>
			This particular solution oscillates at the natural/driving frequency but with a linearly growing amplitude. This situation is called <idx>resonance</idx><term>resonance</term> and is one of the most important phenomena in physics. 
		</p>
		<p>
			The eternally growing amplitude in resonance is a byproduct of the idealized undamped model. What happens if <m>Z\gt 0</m>? We are back to <m>x_p=Ae^{i\omega t}</m>, with
			<me>
				A(i\omega) = \frac{\omega_0^2}{\omega_0^2-\omega^2  + 2 Z i \omega_0\,\omega }
			</me>. 
			Of particular interest is the gain, 
		</p>
		<fact>
			<title>Gain for damped driven oscillator</title>
			<p>
				<me>
					g(\omega) = |A(i\omega)| = \frac{\omega_0^2}{[(\omega^2-\omega_0^2)^2  + 4 Z^2 \omega_0^2\,\omega^2]^{1/2} }
				</me>
			</p>
		</fact>
		<p>
			This formula is a bit easier to parse in terms of the frequency ratio <m>\rho=\omega/\omega_0</m>:
			<me>
				g(\rho) =  \frac{1}{[(1-\rho^2)^2  + 4 Z^2 \rho^2]^{1/2} }
			</me>. 
			Basic calculus shows that the denominator is minimized, and thus the gain is maximized, when 
			<me>
				\rho_\text{max} = \sqrt{1-2Z^2}, \quad  \text{ if } Z^2 \le \frac{1}{2}
			</me>,
			and <m>\rho=0</m> if <m>\frac{1}{2} \lt Z^2 \lt 1</m>. Since <m>\rho_\text{max} \lt 1</m>, the <q>most resonant</q> frequency is lower than the natural frequency. For <m>Z^2 \le \frac{1}{2}</m> the maximum gain is 
			<me>
				g(\rho_\text{max}) = \frac{1}{2Z\sqrt{1-Z^2}}
			</me>,
			which is finite but large for <m>Z\approx 0</m>. 
		</p>
	</subsection>
</section>

<section xml:id="so-laplace">
	<title>Laplace transform methods</title>
	<introduction>
		<p>
			We return to our linear, second-order, constant-coefficient problem in the simpler notation 
			<me>
				x'' + bx' + cx = f(t)
			</me>. 
			We cast this problem as a first-order system in two dimensions, found that the eigenvalues are the roots of <m>\lambda^2+b\lambda + c</m>, and then constructed exponential solutions for the homogeneous part (except for the double eigenvalue case). We also found a particular solution for the case of an exponential <m>f(t)</m>. There is an alternative way to derive the same results using the method of Laplace transforms to systematize the manipulations.
		</p>
	</introduction>

	<subsection>
		<title>Second derivatives</title>
		<p>
			Recall that the transform of <m>x'(t)</m> is <m>sX(s)-x(0)</m>. Applying this formula twice shows that 
			<me>
				\lx[x''(t)] = s \lx[x'(t)] - x'(0) = s^2X(s) - sx(0) - x'(0)
			</me>.
			Let's look at an example first. 
		</p>
		<example>
			<p>
				Let's observe what happens in a fundamental case, <m>x''-x=e^{rt}</m>.  Transforming both sides gives 
				<me> 
					[s^2X(s) - sx(0)-x'(0)] - X(s) = \frac{1}{s-r}
				</me>. 
				We easily solve for <m>X</m>:
				<me>
					X(s) = x'(0)\frac{1}{(s^2-1)} + x(0)\frac{s}{(s^2-1)} + \frac{1}{(s^2-1)(s-r)} 
				</me>. 
				This is doable, but we are going to lighten the load a bit. Even before we took the transform, we knew how to find the general homogeneous solution, <m>c_1e^t+c_2e^{-t}</m>. So let's use the transform to find any particular solution of the nonhomogeneous problem. Based on the expression above, we're best off with finding the particular solution with <m>x(0)=x'(0)=0</m>, leaving us with
				<me>
					X_p(s) = \frac{1}{(s+1)(s-1)(s-r)} 
				</me>. 
				The partial fraction mneomic gives us
				<me> 
					\frac{1}{(s+1)(s-1)(s-r)} = \frac{1}{(s+1)(-1-1)(-1-r)} + \frac{1}{(1+1)(s-1)(1-r)}  + \frac{1}{(r+1)(r-1)(s-r)} 
				</me>. 
				Note that this requires <m>r\neq -1</m> and <m>r \neq 1</m> (i.e., no resonance). From here we get 
				<me>
					x_p(t) = \frac{1}{2(1+r)}e^{-t} + \frac{1}{2(1-r)}e^{t} + \frac{1}{r^2-1}e^{rt}
				</me>. 
				With this we can construct a general solution <m>x_n+x_p</m>, and then apply initial conditions to solve for the constants, if those are given.
			</p>
		</example>
		<p>
			This example is the template for our use of Laplace transforms. In the general case we get 
			<me>
				[s^2X(s) - sx(0)-x'(0)] + b[sX(s)-x(0)] + cX(s) = F(s)
			</me>. 
			We choose to find a particular solution with zero initial conditions, so that 
			<me>
				X_p(s) = \frac{F(s)}{s^2+bs+c}
			</me>. 
			The real effort comes in finding the inverse transform of the right-hand side. Note that the denominator has roots at the eigenvalues of the homogeneous problem. These are called <idx>poles</idx><term>poles</term> of the transform <m>X_p</m>. Typically <m>F(s)</m> will contribute one or more of its own poles. Each pole at <m>s=r</m> corresponds to an exponential solution <m>e^{rt}</m> in time, but the exact expression depends on a partial fraction decomposition and perhaps other manipulations. 
		</p>
	</subsection>

	<subsection> 
		<title>Imaginary poles</title>
		<p>
			If we transform <m>e^{i\omega t}</m>, we can take the real and imaginary parts to get some new formulas:
			<me>
				\lx[\cos(\omega t)] = \text{Re}\left( \frac{1}{s-i\omega} \frac{s+i\omega}{s+i\omega} \right) = \frac{s}{s^2+\omega^2}
			</me>,
			and 
			<me>
				\lx[\sin(\omega t)] = \text{Im}\left( \frac{1}{s-i\omega} \frac{s+i\omega}{s+i\omega} \right) = \frac{\omega}{s^2+\omega^2}
			</me>.
			Conversely, a transform <m>X(s)</m> that has <m>s^2+\omega^2</m> as a factor in the denominator has poles at <m>\pm i\omega</m>. The simplest case is 
			<me>
				X_p(s) = \frac{A+Bs}{s^2+\omega^2}  \quad \Rightarrow \quad x_p(t)=(A/\omega)\sin(\omega t) + B\cos(\omega t)
			</me>. 
		</p>
		<example>
			<p>
				Let's solve <m>x''+9x=\delta(t-2)</m>. A particular solution has transform
				<me>
					X_p(s) = \frac{e^{-2s}}{s^2+9} = e^{-2s} F(s)
				</me>,
				where <m>F(s)</m> is the transform of <m>\sin(3t)/3</m>. So the shift theorem implies
				<me>
					x_p(t) = \frac{1}{3} H(t-2) \sin(3t-6)
				</me>. 
			</p>
		</example>

		<p>
			Things are not too bad if there is an additional real pole: 
			<me>
				X_p(s) = \frac{A+Bs}{s^2+\omega^2} + \frac{C}{s-c} \quad x_p(t)=(A/\omega)\sin(\omega t) + B\cos(\omega t) + C e^{ct}
			</me>. 
			Usually there is some grunt (or computer) work involved in finding the values of <m>A</m>, <m>B</m>, and <m>C</m>. 	
		</p>
		<example>
			<p>
				The undamped oscillator <m>x''+16x=40e^{-2t}</m> has a particular solution whose transform is 
				<me>
					X_p(s) = \frac{40}{(s+2)(s^2+16)} = \frac{A+Bs}{s^2+16^2} + \frac{C}{s+2}
				</me>. 
				Clearing the denominators gives the identity
				<me>
					(A+Bs)(s+2) + C(s^2+16) = 40
				</me>. 
				We need three equations to determine the unknown coefficients. Setting <m>s=-2</m> quickly gives <m>20C=40</m>, so <m>C=2</m>. Then setting <m>s=\pm 4</m> gives the system
				<md>
					<mrow>(A+4B)(6) \amp = 40 - 64 = -24</mrow>
					<mrow>(A-4B)(-2) \amp = -24</mrow>
				</md>,
				or <m>A+4B=-4</m> and <m>A-4B=12</m>. We can add and subtract these equations to get <m>2A=8</m>, <m>8B=-16</m>, so finally 
				<me>
					X_p(s) = \frac{4-2s}{s^2+16^2} + \frac{2}{s+2} 
				</me>. 
				Hence
				<me>
					x_p(t) = \sin(4t) - 2\cos(4 t) + 2e^{-2t}
				</me>.
			</p>
		</example>

		<p>
			Finally, consider the driven, undamped oscillator <m>x''+\omega_0^2 x = \cos(\omega t)</m>. It leads to 
			<me> 
				X_p(s) = \frac{s}{(s^2+\omega_0^2)(s^2+1)} = \frac{1}{\omega_0^2-\omega^2} \left( \frac{s}{s^2+\omega^2} - \frac{s}{s^2+\omega_n^2} \right)
			</me>, 
			which is readily inverted into a combination of <m>\cos(\omega t)</m> and <m>\cos(\omega_0 t)</m>. However, at this point we are probably better off using the method of undetermined coefficients instead. 
		</p>
	</subsection>
	
</section>

<section xml:id="so-laplace-repeated">
	<title>Repeated and complex poles</title>

	<subsection>
		<title>Repeated poles and resonance</title>
		<p>
			We know by now that repeated poles in <m>X_p(s)</m> arise in two ways: from repeated characteristic roots (i.e. poles of the transfer function), or perfect resonance with the forcing function. We can anticipate the effect on the transform, because we know the answer in the time domain is something like <m>te^{rt}</m>. 
			<!-- The fiendlishly clever approach is to notice that this function is a derivative of <m>e^{rt}</m> <em>with respect to <m>r</m></em>:
			<me>
				\lx \left[ \frac{d}{dr} e^{rt} \right] = \frac{d}{dr}  \lx \left[ e^{rt} \right] = \frac{d}{dr}\left[ \frac{1}{s-r} \right] 
			</me>,
			from which we conclude 
			<me>
				\lx \left[ t e^{rt} \right] = \frac{1}{(s-r)^2} 
			</me>.-->
		</p>

		<p>
			In fact if we have any <m>f(t)</m> with transform <m>F(s)</m>, then from the transform definition we compute
			<md>
				<mrow>F'(s) \amp = \frac{d}{ds} \int_0^\infty f(t) e^{-st}\, dt</mrow>
				<mrow>\amp = -\int_0^\infty t f(t) e^{-st}\, dt</mrow>
				<mrow>\amp = - \lx[ tf(t) ]</mrow>
			</md>. 
			Take a moment to appreciate this. Earlier we found that a derivative in <m>t</m> corresponds to multiplying by <m>s</m>. Up to a change in sign, a derivative in <m>s</m> corresponds to multiplying by <m>t</m>!
		</p>
		<example>
			<p>
				The undamped oscillator <m>x''+9x=\sin(3t)</m> has a perfect resonance. The transform suggests the particular solution 
				<me>
					X_p(s) = \frac{3}{(s^2+9)^2}
				</me>. 
				This doesn't look like anything easy. However, note that
				<me>
					sX_p(s) = \frac{3s}{(s^2+9)^2} = -\frac{3}{2} \frac{d}{ds} \left[ \frac{1}{s^2+9} \right] 
				</me>. 
				Therefore, <m>sX_p(s)</m> is the transform of <m>\frac{1}{2} t \sin(3t)</m>. So this must be <m>x_p'(t)</m>. An integration by parts then recovers
				<me>
					x_p(t) = \frac{1}{18} [ \sin(3t) - 3t\cos(3t) ]
				</me>.
			</p>
		</example>
		<p>
			If we keep differentiating in <m>s</m> we find 
			<me> 
				\frac{d^n}{ds^n} \lx[ f(t) ] = (-1)^n \lx[ t^n f(t) ]
			</me>.
			Of interest is the special case of an exponential function <m>e^{rt}</m>, whose transform is <m>1/(s-r)</m>: 
			<me>
				\lx \left[ t^n e^{rt} \right] = \frac{n!}{(s-r)^{n+1}} 
			</me>.
			The even specialer case <m>r=0</m> allows us to transform any power of <m>t</m>, and therefore any polynomial. 
		</p>
	</subsection>

	<subsection>
		<title>Complex poles</title>
		<p> 
			Complex poles in the transform correspond to complex exponentials in the time domain. When they occur in conjugate pairs, the entire expression can be cast in real terms. We have 
			<me>
				\frac{1}{s-(a+ib)} + \frac{1}{s-(a-ib)} = \frac{2(s-a)}{(s-a)^2+b^2} 
			</me>. 
			Since we also know that 
			<me>
				e^{(a+ib)t} + e^{(a-ib)t} = 2 e^{at} \cos( b t)
			</me>,
			the conclusion is 
			<me>
				\lx[ e^{at} \cos( b t) ] = \frac{s-a}{(s-a)^2+b^2}
			</me>. 
			In similar fashion we can show that <m>e^{at} \sin( b t)</m> transforms to <m>b/[(s-a)^2+b^2]</m>. In principle we can always find quadratic denominators like these for a partial fraction conversion. However, the details can get pretty intense for hand calculation.
		</p>
		<example>
			<p>
				Suppose <m>x''+2x'+10x=6\delta(t-1)</m>. A particular solution has 
				<me>
					X_p(s) = \frac{6e^{-s}}{s^2+2s+10}
				</me>.
				We can use the trick of completing the square to write this as 
				<me>
					X_p(s) = e^{-s} \frac{6}{(s+1)^2+9} = e^{-s} F(s)
				</me>, 
				where we can now see that <m>f(t) = 2e^{-t}\sin(3t)</m>. By the shift theorem, <m>x_p(t)=H(t-1)f(t-1)</m>. 
			</p>
			<p>
				Now consider <m>x''+2x'+10x=40</m>. We get 
				<me>
					X_p(s) = \frac{40}{s(s^2+2s+10)} = \frac{40}{s[(s+1)^2+9]}
				</me>. 
				We decompose into 
				<me>
					X_p(s) = \frac{A}{s} + \frac{3B+C(s+1)}{(s+1)^2+9}
				</me>. 
				Once we find the constants, we know that the inverse of this is <m>x_p(t) = A + Be^{-t}\sin(3t) + C e^{-t}\cos(3t)</m>.  To find the constants we clear denominators and get 
				<me> 
					40 = A[(s+1)^2+9] + (3B+Cs+C) s
				</me>.  
				With <m>s=0</m> we conclude that <m>A=4</m>. From this we find that <m>40 = Cs^2+(3B+C)s + 4s^2 +8s + 40</m>, which tells us <m>C=-4</m> and <m>B=-4/3</m>. Hence
				<me>
					x_p(t) = 4 - \frac{4}{3} e^{-t} \sin(3t) - 4 e^{-t} \cos(3t)
				</me>.
			</p>
			<p>
				For a harmonically forced case such as <m>x''+2x'+10x=\cos(2t)</m>, we have 
				<me>
					X_p(s) = \frac{s}{(s^2+4)(s^2+2s+10)}
				</me>. 
				We could find a decomposition in the form 
				<me> 
					X_p(s) = \frac{2A+Bs}{s^2+4} + \frac{3C+E(s+1)}{(s+1)^2+9}
				</me>, 
				which is eminently invertible. The details are left to the proverbial reader.
			</p>
		</example>
		<p> 
			The drawback to Laplace transforms as a one-stop solver is that either we include the initial conditions from the start, which forces us to do more partial fraction decompositions, or else the particular solution we find almost always includes some terms from the homogeneous solution. Mathematically, the poles of <m>X_p(s)</m> include those of the transfer function <em>and</em> the forcing function. Undetermined coefficients is a less sexy but simpler way to solve concrete problems in many cases. 
		</p>
	</subsection>
</section>

<section xml:id="so-convolutions">
	<title>Convolutions</title>
	<introduction>
		<p>
			
		</p>
	</introduction>
	
	<subsection>
		<title>Moving averages</title>
		<p>
			The following grabs and plots the closing price of Bitcoin for the last 31 days.
		</p>
			<sidebyside>
			<listing  xml:id="so_conv_bitcoin1">
	 			<program language="matlab">
	    			<input>
%bc = webread('https://api.coindesk.com/v1/bpi/historical/close.json');
%data = jsondecode(bc);
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-o')
					</input>
				</program>
				</listing>
				<image source="figures/so_conv_bitcoin1.svg"/>	  
			</sidebyside>
			<p>
				It's a noisy curve. We can smooth that out by taking 4-day moving averages, for example. I don't claim that this is the best way, but we can do this by:
			</p>
			<sidebyside>
				<listing  xml:id="so_conv_bitcoin2">
	 			<program language="matlab">
	    			<input>
%bc = webread('https://api.coindesk.com/v1/bpi/historical/close.json');
%data = jsondecode(bc);
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-ko')
for i = 4:31
    z(i) = (v(i) + v(i-1) + v(i-2) + v(i-3)) / 4;
end
z(1:3) = NaN;  % not a number
hold on, plot(z,'-o')
					</input>
				</program>
				</listing>
				<image source="figures/so_conv_bitcoin2.svg"/>	  
			</sidebyside>

			<p>
				We might decide, however, to weight the most recent values more heavily. Here's how this might look.
			</p>
			<sidebyside>
				<listing xml:id="so_conv_bitcoin3">
	 			<program language="matlab">
	    			<input>
%bc = webread('https://api.coindesk.com/v1/bpi/historical/close.json');
%data = jsondecode(bc);
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-ko')
w = [4 3 2 1];
for i = 4:31
    z(i) = (w(1)*v(i) + w(2)*v(i-1) + w(3)*v(i-2) + w(4)*v(i-3)) / sum(w);
end
z(1:3) = NaN;  % not a number
hold on, plot(z,'-o')
					</input>
				</program>
				</listing>
				<image source="figures/so_conv_bitcoin3.svg"/>	  
			</sidebyside>
			<p>
				Note that each new value <m>z_i</m> is a linear combination of the elements of <m>v</m>, weighted by the values in <m>w</m>. And we always go forward 1,2,3,4 in <m>w</m>, while on <m>v</m> we have a sliding group going backward: <m>i,i-1,i-2,i-3</m>. Put concisely, the indices add up to the constant value <m>i+1</m>. 
			</p>
	</subsection>

	<subsection>
		<title>Convolution integral</title>
		<p>
			The moving average above gave us a certain way to multiply together vectors <m>v</m> and <m>w</m> to get a new vector <m>z</m>: 
			<me>
				z_i =  \sum_{j} w_j v_{i-j} 
			</me>
			(although I reindexed the output vector from <m>i+1</m> to <m>i</m> compared to the computer code). This is analogous to a different way to multiply two <em>functions</em>, by the <term>convolution integral</term>
			<me> 
				[f*g](t) = \int_0^t f(t-\tau)g(\tau)\, d\tau
			</me>. 
			This too is like a weighted average, with the values of <m>g(\tau)</m> being multiplied against different windows of the function <m>f</m> such that the arguments sum to <m>t</m>. 
		</p>
		<p> 
			An interesting fact is that while I interpreted <m>f*g</m> as <m>g</m> acting on <m>f</m>, in fact the operation is symmetric:
			<md>
				<mrow>[g*f](t) \amp = \int_0^t f(t-\tau)g(\tau)\, d\tau</mrow>
				<mrow>\amp = \int_t^0 f(u)g(t-u)\, (-du)  </mrow>
				<mrow>\amp = \int_0^t g(t-u)f(u)\, du = [f*g](t)</mrow>
			</md>. 
			We can also easily prove some more properties we really like to have:
			<md>
				<mrow>f * ( g * h ) = ( f * g) * h </mrow>
				<mrow>f * (g+h)=(f * g)+(f*h)</mrow> 
				<mrow>f * 0=0</mrow>
			</md>. 
			It is <em>not</em> true, however, that <m>f*1=f</m>, unless <m>f</m> is the zero function. Instead, the right formula is <m> f * \delta=f</m>: 
			<me> 
				[f*\delta](t) = \int_0^t f(t-\tau) \delta(\tau)\, d\tau = f(t)
			</me>. 
		</p>
	</subsection>

	<subsection>
		<title>Convolution theorem</title>
		<p>
			Here is what makes convolution a big deal. 
		</p>
		<theorem>
			<p>
				Suppose <m>\lx[f] = F(s)</m>, <m>\lx[g] = G(s)</m>, <m>h=f*g</m>, and <m>\lx[h] = H(s)</m>. Then <m>H(s)=F(s)G(s)</m>.
			</p>
		</theorem> 
		<p> 
			Why does this matter? Consider finding a particular solution of <m>x''+bx'+cx=f(t)</m>. After taking transforms we get  
			<me> 
				X_p(s) = \frac{1}{s^2+bs+c} F(s) = G(s) F(s)
			</me>, 
			where <m>G(s)</m> is the transfer function. Right away then, we know that <m>x_p(t)=f(t)*g(t)</m> is a particular solution. 
		</p>
		<p>
			But wait, there's more! Consider the problem of an impulse response, <m>x''+bx'+cx=\delta(t)</m>, with zero initial conditions. The solution of this problem has <m>X(s)=G(s)</m>. That is, 
		</p>
		<fact>
			<p>The impulse response of a linear constant-coefficient equation is the inverse Laplace transform of the transfer function.</p>
		</fact>
		<p> 
			Put it all together and you have some important theoretical connections. The solution to any forced problem is a convolution of the forcing function with the impulse response in the time domain, and the product of the forcing and the transfer function in the <m>s</m> domain. Finally, the transfer function is just the reciprocal of the characteristic polynomial. 
		</p>
	</subsection>
</section>


</chapter>