<?xml version="1.0" encoding="UTF-8" ?>

<chapter xml:id="ch4-second-order-linear" xmlns:xi="http://www.w3.org/2001/XInclude">
<title>Second-order ODEs</title>

<section xml:id="so-convert">
	<title>Order equals dimension</title>
	<p>
		We now turn our attention to second-order ODEs of the form 
		<me>
			x''=f(t,x,x')
		</me>. 
		Most of what we develop extends easily to vector-valued <m>u(t)</m>  and equations of order higher than two, but we focus on this problem to keep notation simple. 
	</p>
	<p>
		Our first observation is that this problem can always be converted to a first-order problem in two dimensions. Define a vector <m>\bu</m> by <m>u_1=x</m>, <m>u_2=x'</m>. By definition, <m>u_1'=u_2</m>. Since <m>u_2'=x''=f(t,x,x')</m>, we get the system 
		<md>
			<mrow> u_1' \amp =  u_2</mrow>
			<mrow> u_2' \amp =  f(t,u_1,u_2).</mrow>
		</md>
		This is a first-order system <m>\bu'=\mathbf{g}(t,\bu)</m> of two scalar equations for the two components. 
	</p>
	<example>
		<p>
			A pendulum is governed by the nonlinear equation <m>\theta''+ b\theta' + \frac{g}{L}\sin(\theta)=0</m>, where <m>\theta(t)</m> is the angle made by the pendulum from the straight-down position. Define <m>u_1=\theta</m>, <m>u_2=\theta'</m>. Then 
			<me>
				\twovec{u_1'}{u_2'} = \twovec{u_2}{-bu_2 - \frac{g}{L}\sin(u_1)}
			</me>
			is an equivalent system.
		</p>
	</example>
	<p>
		The technique above generalizes to any system of equations of any order. 
	</p>
	<example>
		<p>
			Two pendulums hanging from a bar and swinging in parallel planes are coupled through torsion on the bar, according to 
			<md>
			  <mrow> \theta_1''+ b\theta_1' + \frac{g}{L}\sin(\theta_1) + \kappa(\theta_1-\theta_2) \amp = 0 </mrow>
			  <mrow> \theta_2''+ b\theta_2' + \frac{g}{L}\sin(\theta_2) + \kappa(\theta_2-\theta_1) \amp = 0 </mrow>
			</md>
			We define 
			<me>
				u_1 = \theta_1, \quad u_2 = \theta_1', \quad u_3 = \theta_2, \quad u_4 = \theta_2'.
			</me>
			Two differential equations, <m>u_1'=u_2</m> and <m>u_3'=u_4</m>, follow automatically from the definitions. The remaining two come from substituting into the original ODEs: 
			<md>
			  <mrow>u_2' = \theta_1'' \amp = - b u_2 - \frac{g}{L}\sin(u_1) - \kappa(u_1-u_3)</mrow>
			  <mrow>u_4' = \theta_2'' \amp = - b u_4 - \frac{g}{L}\sin(u_3) - \kappa(u_3-u_1)</mrow>
			</md>
			The result is a four-dimensional, first-order system <m>\bu'=\mathbf{g}(t,\bu)</m>.
		</p>
	</example>
	<p>
		Our main item of business in this chapter is the linear, constant-coefficient model problem 
		<men xml:id="so-eq-linearmodel">
			x'' + bx' + cx = f(t)
		</men>,
		for which the conversion process with <m>u_1=x</m>, <m>u_2=x'</m> yields 
		<men xml:id="so-eq-matrixmodel">
			\twovec{u_1'}{u_2'} = \twovec{u_2}{f(t)-cu_1-bu_2} = \twomat{0}{1}{-c}{-b} \bu + \twovec{0}{f(t)}
		</men>,
		which has the form <m>\bu'=\bA\bu+\bff(t)</m> for the <idx>companion matrix</idx><term>companion matrix</term> 
		<men xml:id="so-eq-companion">
			\bA = \twomat{0}{1}{-c}{-b}
		</men>.
		Thus everything we learned from the last chapter is again in play. However, the applications of <xref ref="so-eq-linearmodel"/> are important and numerous enough to consider the solutions in detail, and the structure of the companion matrix is such that the conclusions can often be rewritten more simply than in the general case. 
	</p>
	<p>
		As a result of the equivalence to a first-order system, we know that the general solution of <xref ref="so-eq-linearmodel"/> consists of the general solution of the homogeneous (unforced) problem, and a particular solution of the forced problem: <m>x(t)=x_h(t)+x_p(t)</m>. Our next steps will be to translate the system results for each of these parts to the new situation. 
	</p>
	<exercises xml:id="so-exer-dimension">
		<exercise>
			<title>Solar system</title>
			<statement>
			  <p>
				  Suppose you want to model the eight planets of the solar system, with the Sun fixed as the origin of the coordinate system. You restrict motion to the the plane of the solar system and apply Newton's laws to derive ODEs for the planets based on gravitation. What is the dimension of the equivalent first-order system? 				
			  </p>
			</statement>
			<answer>
				<p>
					32
				</p>
			</answer>
			<solution>
			  <p>
					There are 8 planets, with two scalar variables to describe each position, for 16 total variables. These are subjected to second-order equations through <m>F=ma</m>, so the dimension doubles when converting to first order. 
			  </p>
			</solution>
		</exercise>	
	</exercises>
	<solutions divisional="solution"></solutions>
</section>

<section xml:id="so-homogeneous">
	<title>Homogeneous problem</title>
	<introduction>
		<p>
			Referring to <xref ref="so-eq-companion"/>, the homogeneous equation <m>x''+bx'+cx=0</m> can be recast as a first-order homogeneous system <m>\bu'=\bA\bu</m> through the definitions 
			<me>
				\bu = \twovec{x}{x'},\qquad  \bA = \twomat{0}{1}{-c}{-b}
			</me>.
			Note that an initial-value problem for the system specifies <m>\bu(0)</m>, or equivalently, both <m>x(0)</m> and <m>x'(0)</m>. Hence <alert>a second-order scalar problem requires two initial conditions to have a unique solution.</alert>
		</p>
		<p>
			The matrix in this system has the characteristic polynomial 
			<me>
				|\bA-\lambda\bI| = \twodet{-\lambda}{1}{-c}{-b-\lambda} = \lambda^2 + b\lambda + c
			</me>.
			This is easily remembered in terms of the original ODE <m>x'' + bx' + cx=0</m>. The eigenvalues can therefore be found from the quadratic formula. 
		</p>
	</introduction>
	<subsection>
		<title>Distinct eigenvalues</title>
		<p>
			In the nondefective case, we find the eigenvectors from the nullspaces of <m>\bA-\lambda_j\bI</m> for <m>j=1,2</m>. These have the basis vectors <m>[1,\,\lambda_j]</m>. Thus the general homogeneous solution is 
			<me>
				\bu_h(t) = c_1 e^{\lambda_1 t} \twovec{1}{\lambda_1} + c_2 e^{\lambda_2 t} \twovec{1}{\lambda_2}
			</me>. 
			In terms of the original variable <m>x=u_1</m>, this is more simply 
			<men xml:id="so-eq-homogdistinct">
				x_h(t) = c_1 e^{\lambda_1 t} + c_2 e^{\lambda_2 t}, \qquad \text{if } \lambda_1\neq\lambda_2
			</men>.
			For real problems with complex eigenvalues, the second term in the sum is the conjugate of the first, and we could write instead 
			<me>
				x_h(t) = \Re \left[ c_1 e^{\lambda_1 t} \right] 
			</me>
			We might want an alternative real-only form. Suppose the eigenvalues are written as <m>\alpha \pm i \beta</m>. Starting with 
			<me>
				e^{(\alpha + i\beta)t} = e^{\alpha t} [\cos(\beta t) + i \sin(\beta t)],
			</me>
			we can take real and imaginary parts to get independent real solutions. Thus 
			<men xml:id="so-eq-homogcomplex">
				x_h(t) = e^{\alpha t} \bigl[ a_1 \cos(\beta t) + a_2 \sin(\beta t) \bigr], \qquad \text{if } \lambda=\alpha \pm i\beta
			</men>.
			There is a third equivalent form in the complex case: the <idx>phase-amplitude form</idx><term>phase-amplitude form</term>, 
			<men xml:id="so-eq-homogphaseamp">
				x_h(t) = R e^{\alpha t} \cos(\beta t - \theta)
			</men>. 
			To see the equivalence to <xref ref="so-eq-homogcomplex"/>, we can use a trig identity on the phase-shifted cosine: 
			<me>
				R e^{\alpha t} \cos(\beta t - \theta) = R e^{\alpha t} \cos(\theta) \cos(\beta t) + R e^{\alpha t} \sin(\theta) \sin(\beta t).
			</me>
			We get back to <xref ref="so-eq-homogcomplex"/> by choosing 
			<me>
				R\cos(\theta) = a_1, \qquad R\sin(\theta) = a_2.
			</me>
			More simply, we can think of <m>(a_1,a_2)</m> as Cartesian coordinates and express the same point in polar form to get <m>(R,\theta)</m>. Usually the amplitude, <m>R=\sqrt{a_1^2+a_2^2}</m>, is of the most interest. Whichever form is chosen for the general solution, it has two real constants that can be determined by the initial conditions, if supplied. 
		</p>
		<example>
			<p>
				Let's find the general solution of <m>x''+5x'+6x=0</m>. The characteristic polynomial is <m>\lambda^2+5\lambda+6</m>, which has roots <m>\lambda_1=-2</m>, <m>\lambda_2=-3</m>. Thus <m>x(t)=c_1 e^{-2t}+c_2e^{-3t}</m>. 
			</p>
		</example>
		<example>
			<p>
				The characteristic polynomial of <m>x''-2x'=0</m> is <m>\lambda^2 - 2\lambda</m>, which has roots <m>\lambda_1=0</m>, <m>\lambda_2=2</m>. Thus <m>x(t)=c_1 + c_2e^{2t}</m>.
			</p>
		</example>
		<example>
			<p>
				To solve <m>3x''+6x'+30x=0</m>, we first should divide through by 3 to get the standard form. Then the characteristic polynomial is <m>\lambda^2 + 2\lambda + 10</m>, which has roots <m>-1\pm 3i</m>. So <m>x(t) = e^{-t}[c_1 e^{3it} + \overline{c_1}e^{-3it}]</m>. Equivalently, <m>x(t) = e^{-t}[a_1 \cos(3t) + a_2\sin(3t)]</m>, or <m>x(t) = R e^{-t}\cos(3t-\theta)</m>.
			</p>
		</example>
	</subsection>
	<subsection>
		<title>Repeated eigenvalue</title>
		<p>
			If the companion matrix has a double eigenvalue, then it is defective (unless <m>b=c=0</m>, which leaves the truly trivial problem <m>x''=0</m>). In terms of the original ODE, this occurs when <m>b^2=4c</m>, and then <m>\lambda_1=\lambda_2=-b/2</m>. Recalling the trick used in <xref ref="fs-me-defective"/>, we define 
		<me>
			\bB = \bA + \frac{b}{2}\bI = \twomat{b/2}{1}{-c}{-b/2} = \twomat{b/2}{1}{-b^2/4}{-b/2}
		</me>. 
		You can easily check that <m>\bB^2=\bzero</m>. Hence the power series for the matrix exponential has just two terms:
		<me>
			e^{t\bA} = e^{-(bt/2)\bI} e^{t\bB} = e^{-bt/2}(\bI + t\bB) 
		</me>. 
		The general solution is <m>e^{t\bA}\bc</m>, but again we need only the first row to get the solution for the original <m>x=u_1</m>: 
		<me>
			x_h(t) = e^{-bt/2} \left[ c_1 + t\left(c_2 + c_1\frac{b}{2}\right) \right] = e^{-bt/2} \left( \tilde{c}_1 + c_2 t\right)
		</me>, 
		where <m>\tilde{c}_1</m> is just a different arbitrary constant. We can rename the constants and express the result as 
		<men xml:id="so-eq-homogdefect">
			x_h(t) = e^{-bt/2} \left( c_1 + c_2 t\right) , \qquad \text{if } \lambda_1 = \lambda_2 \neq 0.
		</men>
	</p>
	<example>
		<p>
			The characteristic polynomial of <m>x''-4x'+4x=0</m> is <m>\lambda^2 - 4\lambda + 4</m>, which has a double root at 2. Hence <m>x(t)=e^{2t}(c_1+c_2 t)</m> is the general solution. 
		</p>
	</example>
</subsection>
<exercises xml:id="so-exer-homog">
	<exercise>
		<title>Inverted pendulum</title>
		<statement>
		  <p>
				The nonlinear pendulum equation <m>\theta''+\gamma \theta' + \frac{g}{L}\sin(\theta) = 0</m> has an equilibrium at <m>\theta=0</m>, which is the downward position. But it also has an equilibrium at the upward or inverted position, <m>\theta=\pi</m>. Show that the change of variable <m>\phi=\pi-\theta</m> leads to 
				<me>
					\phi'' + \gamma \phi' - \frac{g}{L}\sin(\phi) = 0,
				</me>
				which has an equilibrium at <m>\phi=0</m>. Making the small-angle approximation for sin gives the linear equation 
				<me>
					\phi'' + \gamma \phi' - \frac{g}{L} \phi = 0.
				</me>
				Show that for any <m>\gamma \ge 0</m>, the equilibrium is an unstable saddle. 
		  </p>
		</statement>
		<solution>
		  <p>
				The chain rule gives <m>\phi'=-\theta'</m> and <m>\phi''=\theta''</m>. The ODE follows from the identity <m>\sin(\pi-\phi)=\sin(\phi)</m>. The eigenvalues from the characteristic equation are 
				<me>
					\frac{-\gamma \pm \sqrt{\gamma^2 + 4(g/L)}}{2}. 
				</me>
				The radical gives a real value greater than <m>\gamma</m>, so the plus sign gives a positive real eigenvalue and the minus sign gives a negative one. (If you pull the pendulum a little away from the top and push it <em>just</em> right, it will gently return to the inverted position, in theory!) 
		  </p>
		</solution>
	</exercise>
</exercises>
<solutions divisional="solution"></solutions>
</section>

<section xml:id="so-undetermined">
	<title>Undetermined coefficients</title>
	<p>
		We saw two ways to get particular solutions for first-order equations: variation of parameters and undetermined coefficients. Since a second-order problem has a first-order system equivalent, we can use variation of parameters for a system, as in <xref ref="fs-varparam"/>, to find a particular solution. However, for many common second-order problems with constant coefficients, it is much easier to adapt the method of undetermined coefficients. 
	</p>
	<p>
		Specifically, the ODE <m>x''+bx'+cx=f(t)</m> can be solved with this method when <m>f</m> is a polynomial, exponential, sin, or cos function. The appropriate form for the particular <m>x_p</m>  for each case is given in <xref ref="fl-tb-undeter"/>. This form is plugged into the ODE, and the resulting identity is used to deduce the undetermined coefficients. 
	</p>
	<example>
		<p>Let's find a particular solution of <m>x'' +4x'+4x=8t^2</m>. The correct form of <m>x_p</m> is a quadratic polynomial, so 
		<me>
			x_p = A + Bt + Ct^2
		</me>. 
		Plugging that into the ODE yields
		<md>
			<mrow>2C + 4(B+2Ct) + 4(A+Bt+Ct^2) \amp = 8t^2</mrow>
			<mrow>4C t^2 + (8C+4B) + (2C+4B+4A) \amp = 8t^2</mrow>
		</md>. This has to be an identity for all <m>t</m>. Matching like powers, we can read off <m>C=2</m>, then <m>B=-4</m>, and finally <m>A=3</m>. This provides us with <m>x_p=2t^2+4t+3</m>. 
		</p>
	</example>
	<!--example>
		<p>Let's try for <m>y'' -2y'-3y=10te^{4t}</m>. For much the same reasons as in the last case, we try 
		<me>y_p = (A + Bt)e^{4t}</me>. There's nothing to do now except grind out some derivatives. 
		<md>
			<mrow>8(2A+B+2Bt)e^{4t} - 2(4A+B+4Bt)e^{4t} -3 (A + Bt)e^{4t} \amp =10t e^{4t}</mrow>
			<mrow> 8(2A+B) - 2(4A+B) -3A + t(16B-8B-3B) =  10t</mrow>
		</md>. From this, by matching powers, we deduce <m>B=2</m> and then <m>5A+12=0</m>. So <m>y_p = [2t - (12/5)]e^{4t}</m>.
		</p>
	</example-->
	<example>
		<p>
			For <m>x'' - 2x'-3x=10e^{4t}</m>, the proper choice is 
			<me>
				x_p = Ae^{4t}
			</me>. Everything else is just basic manipulations.  
			<md>
				<mrow>16A e^{4t} - 2 (4A e^{4t}) - 3 (Ae^{4t}) \amp =10 e^{4t}</mrow>
				<mrow> 16A -8A -3A =  10</mrow>
			</md>. From this it's clear that <m>A=2</m>.
			</p>
	</example>
	<example>
		<p>
			For <m>x''+x'=\sin(2t)</m>, we use 
			<me>
				x_p = A\cos(2t) + B\sin(2t)
			</me>. 
			Inserting this into the ODE leads to 
			<me>
				[-4A\cos(2t) - 4B\sin(2t)] + [-2A\sin(2t) + 2B\cos(2t)] = \sin(2t)
			</me>. 
			This is true for all time if and only if we match the coefficients of like trig functions:
			<me>
				-4A + 2B = 0, \qquad -4B-2A = 1 
			</me>. 
			The solution of these equations is <m>A=-1/10</m>, <m>B=-1/5</m>.
		</p>
	</example>
	<p>
		The examples above are the fundamental ones. There are rules for more intricate combinations of the same functions, but we won't go into them here. 
	</p>
	<p>
		We have to repeat the warning from the first time we saw this method: occasionally it fails, given our rules.
	</p>
	<example xml:id="so-ex-noundeter">
		<p>
			The equation <m>x''+x=\cos(\omega t)</m> suggests the particular solution <m>x_p=A\cos(\omega t)+B\sin(\omega t)</m>. Upon substitution,
			<md>
				<mrow>[-\omega^2 A\cos(\omega t) - \omega^2 B\sin(\omega t) ] + [ A\cos(\omega t) + B\sin(\omega t)] \amp =\cos(\omega t),</mrow>
			</md>. 
			which leads to the conclusion that <m>B=0</m> and, if <m>\omega^2 \neq 1</m>, <m>A=1/(1-\omega^2)</m>. However, if <m>\omega = 1</m>, the substitution would leave us with <m>0=\cos(t)</m>, which is impossible to satisfy for all <m>t</m>.
		</p>
	</example>
	<p>
		The failure of <xref ref="so-ex-noundeter"/> at <m>\omega = 1</m> was due to the fact that the <m>x_p</m> we picked is actually a homogeneous solution. There are additional rules to cover this case, but here is an interesting workaround instead. Suppose we seek a particular solution for <m>\omega=1</m> as the limit of a particular solution as <m>\omega\to 1</m>. We won't use the exact <m>x_p</m> found in the example, but instead subtract a homogeneous solution from it to get 
		<me>
			\frac{ \cos(\omega t) - \cos(t) } {1-\omega^2},
		</me>
		which is also a particular solution. Then L'Hopital applies in the limit to give us 
		<men xml:id="so-eq-secular">
			x_p(t) = \frac{ -t \sin(\omega t) }{ -2 \omega } = \frac{1}{2}t\sin(t). 
		</men>
		Now, to verify that this is a particular solution in the limiting case, we compute 
		<md>
		  <mrow> x_p \amp = \frac{1}{2}t\sin(t) </mrow>
		  <mrow> x_p' \amp = \frac{1}{2}[\sin(t)+t\cos(t)] </mrow>
		  <mrow> x_p'' \amp = \frac{1}{2}[\cos(t) + \cos(t) - t\sin(t)], </mrow>
		</md>
		so that <m>x_p''+x_p=\cos(t)</m> as required.
	</p>
	<exercises xml:id="so-exer-muc">
		<exercise>
			<title>Driven inverted pendulum</title>
			<statement>
			  <p>
				  Find the general solution of the driven undamped inverted pendulum, 
				  <me>
					  \phi'' - \frac{g}{L} \phi = \cos(\omega t).
				  </me>		
				  Are there any exceptional (i.e., resonant) values of <m>\omega</m>?
			  </p>
			</statement>
			<answer>
				<p>
					<me>
						\phi(t) = c_1 e^{\alpha t} + c_2 e^{-\alpha t} + \frac{1}{\omega^2 + \alpha^2} \cos(\omega t),
					</me>
					where <m>\alpha = \sqrt{g/L}</m>. Resonance is not possible. 
				</p>
			</answer>
		</exercise>	
	</exercises>
	<solutions divisional="solution"></solutions>
</section>

<section xml:id="so-oscillators">
	<title>Oscillators</title>
	<introduction>
		<p>
			The most common use of second-order equations is the modeling of oscillatory or other periodic behavior.
		</p>
	</introduction>
	<subsection xml:id="so-os-mechanical">
		<title>Mechanical</title>
		<p>
			As we have already seen, a mass hanging from a spring satisfies the ODE
			<me>mx'' + b x' + kx = F(t)</me>.
			(As is typical, all the named constants are nonnegative.) Here <m>m</m>> is the mass, <m>b</m>>is a coefficient of friction or other damping forces, and <m>k</m>> is a characteristic constant of the spring. The term <m>F(t)</m> represents an external driving force. If we have initial conditions, they give values for <m>x</m> and <m>x'</m>.
		</p>
		<p>
			The gravitational force on a pendulum bob (think child on a swing) has a related equation. The proper ODE is 
			<me>\theta'' + \gamma \theta + \frac{g}{L}\sin \theta = F(t)</me>,
			where <m>\theta(t)</m> is the angle made with vertical-down and <m>L</m> is the arm length. This equation is nonlinear. But if the angle remains small, then a reasonable approximation is 
			<me>\theta'' + \gamma \theta + \frac{g}{L}\theta = F(t)</me>, 
			which is a linear oscillator. 
		</p>
	</subsection>
	<subsection>
		<title>Electrical oscillators</title>
		<p>
			An AC circuit often has elements of resistance, capacitance, and inductance. These analogize perfectly to friction/damping, spring constant, and mass. If these elements are wired in series with a generator, the governing ODE is
			<me>LI'' + RI' + \frac{1}{C}I = \mathcal{E}'(t)</me>,
			where <m>L</m> is inductance, <m>R</m> is resistance, <m>C</m> is capacitance, and <m>I(t)</m> is the current flowing through the circuit. The forcing term represents the generator whose voltage (EMF) is <m>\mathcal{E}(t)</m>. For alternating current this is modeled as a cosine (or complex exponential) function.  
		</p>
		<p>
			In DC circuits we have Ohm's Law, <m>I=V/R</m>, for a resistance <m>R</m>. In the AC case, if <m>\mathcal{E}(t)=Ve^{i\omega t}</m>, then <m>e^{i\omega t}</m> appears in the current as well, and it still makes sense to talk about <m>V/I</m> as a kind of resistance, now called <term>impedance</term> and dependent on the frequency <m>\omega</m>. 
		</p>
	</subsection>
	<subsection xml:id="so-os-notation">
		<title>Unifying notation</title>
		<p>
			When you have lots of different versions of the same root problem, with different symbols and units, you have three options. You can solve each new problem from scratch, you can derive custom formulas for each application, or you can try to find a minimal set of parameters and express each problem using those. Let's consider the last pathway here.
		</p>
		<p>
			The mechanical oscillator <m>mx'' + bx' + kx = F(t)</m> has, like all linear ODEs, a homogeneous part and a forced part. For now we consider only the homogeneous part, taking up the forcing term in the next section. Ignoring the degenerate case of zero mass, we write  
			<me> 
				x'' + \frac{b}{m} x' + \frac{k}{m} x = 0
			</me>.
			One reason to prefer this form is dimensional analysis: both <m>b/m</m> and <m>\sqrt{k/m}</m> have units of inverse time. We introduce new parameters 
			<me>
				\omega_0 = \sqrt{\frac{k}{m}} \gt 0, \quad Z = \frac{b/m}{2\omega_0} = \frac{b}{\sqrt{4km}} \ge 0
			</me>.
			<m>\omega_0</m> is known as the <idx>natural frequency</idx><term>natural frequency</term>, with units of inverse time, and <m>Z</m> is a dimensionless <idx>damping coefficient</idx><term>damping coefficient</term> describing the relative intensity of the damping. Now the unforced ODE becomes 
			<men xml:id="so-eq-unified"> 
				x'' + 2 Z \omega_0\, x' + \omega_0^2\, x = 0
			</men>.
			A similar derivation can be done starting from the electrical circuit equation.
		</p>
		<aside>
			<p>
				In math we usually use <q>frequency</q> to mean the multiplier of <m>t</m> in a sin or cos function. In some fields this is called <term>angular frequency</term> and <q>frequency</q> is used to mean the number of cycles per time unit, as in Hz. 
			</p>
		</aside>
		<p>
			The eigenvalues of the homogeneous equation are  
			<me>
				\lambda_{1,2} = -Z \omega_0 \pm \omega_0 \sqrt{Z^2-1}
			</me>. 
			The discussion now splits into four cases, marked by increasing values of <m>Z</m>. 
		</p>
	</subsection>
	<subsection xml:id="so-os-undamped">
		<title>Undamped</title>
		<p>
			The first case for <xref ref="so-eq-unified"/> is <m>Z=0</m>, which is the idealization of no mechanical damping or friction (or resistance, in a circuit). The homogeneous equation is <m>x''+\omega_0^2x=0</m>, which has eigenvalues <m>\pm i\omega_0</m> and a general solution that can be expressed three ways:
			<md>
			  <mrow> x_h(t) \amp = \Re \left[  c_1 e^{i\omega_0 t} \right] </mrow>
			  <mrow>  \amp = a_1 \cos(\omega_0 t) + a_2 \sin(\omega_0 t) </mrow>
			  <mrow> \amp =  R \cos(\omega_0 t - \theta) </mrow>
			</md>
			Thus the undamped case results in pure oscillation at frequency <m>\omega_0</m>. This is known as <idx>simple harmonic motion</idx><term>simple harmonic motion</term>. 
		</p>
		<example>
			<p>
				When a 2 kg mass is hung on a spring, the spring stretches by 0.25 m. What is the natural frequency of the mass-spring system? Suppose the mass is pulled down 0.2 m past equilibrium and then thrown upward at 1 m/s. What is the amplitude of the motion? 
			</p>
			<p>
				Hooke's Law for a spring states that <m>F=k x</m>, so we find the spring constant from <m>k=F/x=2g/0.25=8g</m>, where <m>g=9.8</m> m per second squared. The ODE for free motion of the system is thus 
				<md>
				  <mrow> 2 x'' + 8g x  \amp = 0 </mrow> 
				  <mrow> x'' + 4g x  \amp = 0, </mrow> 
				</md>
				from which we identify the natural frequency
				<me>
					\omega_0 = \sqrt{4g} = 2\sqrt{g} \approx 6.26 \text{s}^{-1}. 
				</me>
				We can apply the initial conditions directly to the amplitude-phase form: 
				<md>
				  <mrow> -0.2 \amp = x(0) = R\cos(0-\theta) = R\cos(\theta) </mrow>
				  <mrow> 1 \amp = x'(0) = -\omega_0 R\sin(0-\theta) = \omega_0 R\sin(\theta) </mrow>
				</md>
				Therefore, 
				<me>
					R = \sqrt{ [R\cos(\theta)]^2 + [R\sin(\theta)]^2 } = \sqrt{ 0.04 + \omega_0^{-2} } \approx 0.256 \text{m}.
				</me>
				We could go on to find the phase by taking a ratio to get 
				<me>
					\tan(\theta) = \frac{1}{-0.2\omega_0}.
				</me>
				In solving for <m>\theta</m>, you have to do a <q>four-quadrant</q> arctan. Observe that <m>(R\cos\theta,R\sin\theta)</m> lies in the second quadrant, so <m>\theta</m> has to be between <m>\pi/2</m> and <m>\pi</m>. One finds that <m>\theta\approx 2.47</m>. 
			</p>
		</example>
	</subsection>
	<subsection xml:id="so-os-underdamped">
		<title>Underdamped</title>
		<p>
			For <m>0\lt Z \lt 1</m> the eigenvalues of <xref ref="so-eq-unified"/> are complex:
			<me>
				\lambda_{1,2} = -Z \omega_0 \pm i \omega_0 \sqrt{1-Z^2}
			</me>. 
			Defining <m>\omega_d=\omega_0 \sqrt{1-Z^2}</m>, the general homogeneous solution is therefore 
			<me>
				x_h(t) = e^{-Z\omega_0 t} \left( c_1 e^{i\omega_d t} + c_2 e^{-i\omega_d t} \right)
			</me>, 
			or the equivalent real form with cos and sin. This solution is pseudoperiodic, combining oscillation at frequency <m>\omega_d \lt \omega_0</m> inside an exponential decay envelope. This situation is called an <idx>underdamped oscillator</idx><term>underdamped oscillator</term>. As in the undamped case, we have three different ways to express the real unforced solution in the underdamped case, corresponding to <xref ref="so-eq-homogdistinct"/>, <xref ref="so-eq-homogcomplex"/>, and <xref ref="so-eq-homogphaseamp"/>.
		</p>
	</subsection>
	<subsection xml:id="so-os-critical">
		<title>Critically damped</title>
		<p>
			At <m>Z=1</m> the complex eigenvalues collapse to a double eigenvalue at 
			<me>
				\lambda_1 = \lambda_2 = -\omega_0
			</me>,
			with general homogeneous solution 
			<me>
				x_h(t) = e^{-\omega_0 t} (c_1 + c_2 t)
			</me>. 
			There is no longer any oscillation present, and we have a <idx>critically damped oscillator</idx><term>critically damped</term> system. The linear growth of <m>c_2 t</m> doesn't make much of a difference against the exponential decay. 
		</p>
	</subsection>
	<subsection xml:id="so-os-overdamped">
		<title>Overdamped</title>
		<p>
			For <m>Z>1</m> the eigenvalues 
			<me>
				\lambda_{1,2} = -Z \omega_0 \pm \omega_0 \sqrt{Z^2-1}
			</me> 
			are negative and real, thus giving an exponentially decaying homogeneous solution. In this case we have an <idx>overdamped oscillator</idx><term>overdamped oscillator</term>. 
		</p>
	</subsection>
	<conclusion>
		<p>
			<xref ref="so-tab-damping"/> summarizes the four major cases for damping. If the damping coefficient <m>Z</m> is nonzero, the eigenvalues of the problem have negative real part, and therefore all the homogeneous solutions decay exponentially in amplitude as <m>t\to\infty</m>. For this reason the homogeneous solution is often called a <idx>transient solution</idx><term>transient solution</term> or transient response. 
		</p>
		<table xml:id="so-tab-damping">
		  <title>Damping coefficient and eigenvalues</title>
		  <tabular>
			<row>
			  <cell>Damping coefficient</cell> 
			  <cell>Eigenvalue property</cell>
			  <cell>Description</cell>
			</row>
			<row>
				<cell><m>Z=0</m></cell>
				<cell>imaginary</cell>
				<cell>undamped</cell>
			</row>
			<row>
				<cell><m>0 \lt Z \lt 1</m></cell>
				<cell>complex</cell>
				<cell>underdamped</cell>
			</row>
			<row>
				<cell><m>Z=1</m></cell>
				<cell>real, negative, repeated</cell>
				<cell>critically damped</cell>
			</row>
			<row>
				<cell><m>Z \gt 1</m></cell>
				<cell>real, negative </cell>
				<cell>overdamped</cell>
			</row>
		  </tabular>
		</table>
		<example>
			<p>
				A 5 kg mass is hung on a spring with constant <m>11</m> N per m and connected to a dashpot that provides 8 N-sec per meter of resistance. Is this system underdamped, overdamped, or critically damped? 
			</p>
			<p>
				The ODE for the mass-spring system is 
				<md>
				  <mrow> 5 x'' + 8x' + 11 x  \amp = 0 </mrow> 
				  <mrow> x'' + 1.6 x' + 2.2 x  \amp = 0, </mrow> 
				</md>
				from which we identify the natural frequency
				<me>
					\omega_0 = \sqrt{2.2} \approx 1.483 \text{s}^{-1}, \qquad 
				</me>
				The damping coefficient is therefore
				<me>
					Z = \frac{1.6}{2\omega_0} \approx 0.539.
				</me>
				Since this value is less than one, the system is underdamped. 
			</p>
		</example>
		<example>
			<p>
				Suppose the system from the last example is initially at equilibrium when the mass is suddenly pushed downward at 0.5 m/sec. Find the motion of the mass.
			</p>
			<p>
				The ODE is, as before, <m>x'' + 1.6 x' + 2.2 x = 0</m>. The eigenvalues are the roots of <m>\lambda^2 + 1.6\lambda + 2.2</m>, which are found numerically to be 
				<me>
					\lambda \approx -0.8000 \pm 1.2490i.
				</me>
				Note that the imaginary part is smaller than the natural frequency found in the last example, as it must be. From here I will treat these rounded values as though they are exact. Choosing the sin-cos form of the general solution, we have 
				<md>
					<mrow>x(t) \amp = a_1 e^{-0.8 t} \cos(1.249 t) + a_2 e^{-0.8 t} \sin(1.249 t).</mrow>
				</md>
				(It is not necessary to use the general formula derived above for this; the solution comes entirely from the eigenvalues.) We apply the initial conditions <m>x(0)=0</m>, <m>x'(0)=-0.5</m> to find 
				<md>
					<mrow> 0 \amp = x(0) = a_1, </mrow> 
					<mrow> -0.5 \amp = x'(0) = a_1( -0.8 ) + a_2 (1.249 ), </mrow> 
				</md>
				thus <m>a_2 = -0.4003</m>. The motion is therefore given by <m>x(t)=-0.4003\, e^{-0.8 t} \sin(1.249 t)</m>. 
			</p>
		</example>
	</conclusion>
</section>

<section xml:id="so-expforcing">
	<title>Driven oscillator</title>
	<introduction>
		<p>
			We next want to examine the linear oscillator in standardized form <xref ref="so-eq-unified"/> when there is an exponential forcing function present: 
			<men xml:id="so-eq-unifiedexp"> 
				x'' + 2 Z \omega_0\, x' + \omega_0^2\, x = e^{rt}
			</men>.
			We're typically interested in the undamped and underdamped cases, <m>0\le Z \lt 1</m>. 
		</p>
		<p>
			We appeal to the method of undetermined coefficients, positing a solution in the form <m>x_p(t) = A e^{rt}</m> to derive
			<me>
				(r^2  + 2 Z \omega_0 r + \omega_0^2)Ae^{rt} = e^{rt}
			</me>. 
			Hence 
			<men xml:id="so-eq-coeff">
				x_p(t) = A e^{rt}, \qquad A(r) = \frac{1}{r^2  + 2 Z \omega_0 r + \omega_0^2}
			</men>
			gives a particular solution unless the denominator, which is really just the characteristic polynomial, is zero. In that case, <m>r</m> is an eigenvalue; we consider that situation later. 
		</p>
	</introduction>
	<subsection xml:id="so-ef-harmonic">
		<title>Harmonic forcing</title>
		<p>
			Exponential forcing is particularly important when <m>r</m> is imaginary. Set <m>r=i\omega</m> in the above. The particular solution 
			<me>
				x_p(t) = A(i\omega) e^{i\omega t}
			</me>			
			is complex; say we decompose it as <m>x_p(t)=u_p(t)+i\, v_p(t)</m>. Using operator notation to write <m>\opA[x_p]=e^{i\omega t}</m>, linearity implies
			<me>
				\opA[u_p] + i\,\opA[v_p] = \cos(\omega t) + i\sin(\omega t)
			</me>. 
			Matching real and imaginary parts gives us
			<md>
				<mrow>\opA[u_p] \amp = \cos(\omega t), </mrow>
				<mrow>\opA[v_p]  \amp = \sin(\omega t). </mrow>
			</md>
			In words, the particular solution for imaginary-exponential forcing gives us the solutions we need for all cosine and sine forcing. All combinations of such forcing functions can be described as <idx>harmonic forcing</idx><term>harmonic forcing</term>. 
		</p>
		<p>
			It's often useful to rewrite the complex factor <m>A(i\omega)</m> from <xref ref="so-eq-coeff"/> in polar form: 
			<me>
				A(i\omega) = \frac{1}{\omega_0^2-\omega^2  + 2 Z i \omega_0 \omega } = g(\omega) e^{-i\phi(\omega)}
			</me>,
			where <m>g=|A|</m> is the <idx>gain</idx><term>gain</term> and <m>\phi</m> is the <idx>phase lag</idx><term>phase lag</term>. Note that the particular solution is 
			<me>
				x_p(t) = g(\omega) e^{-i\phi(\omega)} e^{i\omega t} = g e^{i(\omega t-\phi)}
			</me>. 
			This means that as we pass from the input forcing to the output solution, <m>g</m> is the scaling of the amplitude and <m>\phi</m> changes the phase of our trip around the complex unit circle. 
		</p>
	</subsection>
	<subsection xml:id="so-ef-resonance">
		<title>Resonance</title>
		<p>
			Consider forcing <m>e^{i\omega t}</m> in the undamped case with <m>Z=0</m>. Then 
			<me>
				A(i\omega) = \frac{1}{\omega_0^2-\omega^2}
			</me>
			is purely real, and the general solution is 
		</p>
		<fact>
			<title>Solution for standardized undamped driven oscillator (not resonant)</title>
			<p>
				<me>	
					x(t) = x_h(t) + x_p(t) = c_1 e^{i\omega_0 t} + c_2 e^{-i\omega_0 t} + \frac{1}{\omega_0^2-\omega^2} e^{i\omega t}
				</me>
				(or a real equivalent), provided <m>\omega\neq\omega_0</m>
			</p>
		</fact>
		<p>
			If <m>\omega\approx \omega_0</m>, then the amplitude of the particular solution is very large. 
		</p>
		<p>
			What is the meaning if <m>\omega = \omega_0</m>? This is exactly when <m>i\omega</m> is an eigenvalue of the homogeneous problem, so the method of undetermined coefficients will actually fail like it did in <xref ref="so-ex-noundeter"/>. If we use the same trick that gave us <xref ref="so-eq-secular"/>, we end up with the following.  
		</p>
		<fact>
			<title>Solution for standardized undamped driven oscillator at resonance</title>
			<p>
				<me>	
					x(t) = c_1 e^{i\omega_0 t} + c_2 e^{-i\omega_0 t} -\frac{i}{2} t e^{i\omega_0 t}
				</me>
				(or a real equivalent)
			</p>
		</fact>
		<p>
			This particular solution oscillates at the natural/driving frequency but with a linearly growing amplitude. This situation is called <idx>resonance</idx><term>resonance</term> and is one of the most important phenomena in physics. 
		</p>
	</subsection>
	<subsection>
		<title>Forcing the underdamped oscillator</title>
		<p>
			The eternally growing amplitude seen in resonance is an unphysical byproduct of the idealized undamped model. What happens if <m>Z\gt 0</m>? We are back to <m>x_p=Ae^{i\omega t}</m> with amplitude from <xref ref="so-eq-coeff"/>:
			<me>
				A(i\omega) = \frac{1}{\omega_0^2-\omega^2  + 2 Z i \omega_0\,\omega }
			</me>. 
			Of particular interest is the gain, 
		</p>
		<fact>
			<title>Gain for damped driven oscillator</title>
			<p>
				<me>
					g(\omega) = |A(i\omega)| = \frac{1}{[(\omega^2-\omega_0^2)^2  + 4 Z^2 \omega_0^2\,\omega^2]^{1/2} }
				</me>
			</p>
		</fact>
		<p>
			In the presence of any damping, the gain always remains finite, and there is no longer a perfect resonance. But as a function of the driving frequency, the denominator is minimized, and thus the gain is maximized, when 
			<men xml:id="so-eq-rhomax">
				\omega_\text{max} = \begin{cases} 
				\omega_0 \sqrt{1-2Z^2}, \amp \text{ if } 0 \lt Z^2 \le \frac{1}{2},\\ 0, \amp \text{ if } \frac{1}{2} \lt Z^2 \lt 1.
				\end{cases}
			</men>.
			Note that the <q>most resonant</q> frequency is always lower than the natural frequency. For <m>Z^2 \le \frac{1}{2}</m> the maximum gain is 
			<men xml:id="so-eq-gainmax">
				g(\omega_\text{max}) = \frac{1}{2Z\omega_0^2\sqrt{1-Z^2}}
			</men>,
			which is finite but large for <m>Z\approx 0</m>. 
		</p>
		<p>
			<xref ref="so_oscgain"/> shows the gain as a function of damping <m>Z</m> and forcing frequency <m>\omega</m> when <m>\omega_0=1</m>. The white curve shows the path of greatest resonance. For high values of the damping, the gain can never be larger than one. But as <m>Z\to 0</m>, the resonance peak becomes proportional to <m>1/Z</m>. 
		</p>
		<sidebyside>
			<listing  xml:id="so_oscgain">
				<caption>Gain for natural frequency equal to one</caption>
	 			<program language="matlab">
	    			<input>
A = @(omega,Z) 1./(-omega.^2 + 2i*Z.*omega + 1);
log_g = @(omega,Z) log10(abs(A(omega,Z)));
fsurf( log_g,[0 1.5 1e-3 1])
set(gca,'ydir','rev')
view(20,20)
xlabel('driving frequency')
ylabel('damping coefficient')
zlabel('log_{10}(gain)')
hold on
Z = linspace(1e-3,1,300);
rhomax = real(sqrt(1-2*Z.^2));
plot3(rhomax,Z,log_g(rhomax,Z),'w','linew',2)
					</input>
				</program>
				</listing>
				<image source="figures/so_oscgain.svg"/>	  
			</sidebyside>

	</subsection>
</section>

<section xml:id="so-laplace">
	<title>Laplace transform methods</title>
	<introduction>
		<p>
			We return to our linear, second-order, constant-coefficient problem in the simpler notation 
			<me>
				x'' + bx' + cx = f(t)
			</me>. 
			We cast this problem as a first-order system in two dimensions, found that the eigenvalues are the roots of <m>\lambda^2+b\lambda + c</m>, and then constructed exponential solutions for the homogeneous part (except for the double eigenvalue case). We also found a particular solution for the case of an exponential <m>f(t)</m>. There is an alternative way to derive the same results using the method of Laplace transforms to systematize the manipulations.
		</p>
		<table xml:id="so-tab-laplace">
			<title>Laplace transforms (complete)</title>
			<tabular>
				<row>
					<cell>Function</cell>
					<cell>Transform</cell>
				</row>
			<row>
				<cell><m>x'(t)</m></cell>
				<cell><m>sX(s)-x(0)</m></cell>
			</row>
			<row>
				<cell><m>x''(t)</m></cell>
				<cell><m>s^2 X(s)-s x(0) - x'(0)</m></cell>
			</row>
			<row>
				<cell>1</cell>
				<cell><m>\dfrac{1}{s}</m></cell>
			</row>
			<row>
				<cell><m>e^{at}</m></cell>
				<cell><m>\dfrac{1}{s-a}</m></cell>
			</row>
			<row>
				<cell><m>\cos(\omega t)</m></cell>
				<cell><m>\dfrac{s}{s^2+\omega^2}</m></cell>
			</row>
			<row>
				<cell><m>\sin(\omega t)</m></cell>
				<cell><m>\dfrac{\omega}{s^2+\omega^2}</m></cell>
			</row>
			<row>
				<cell><m>H(t-T)</m></cell>
				<cell><m>\dfrac{e^{-sT}}{s}</m></cell>
			</row>
			<row>
				<cell><m>\delta(t-T)</m></cell>
				<cell><m>e^{-sT}</m></cell>
			</row>
			<row>
				<cell><m>t^n, \quad n=1,2,3,\dots</m></cell>
				<cell><m>\dfrac{n!}{s^{n+1}}</m></cell>
			</row>
			<row>
				<cell><m>H(t-T)f(t-T)</m></cell>
				<cell><m>e^{-sT}F(s)</m> (shift theorem)</cell>
			</row>
			<row>
				<cell><m>t^n f(t), \quad n=1,2,3,\dots</m></cell>
				<cell><m>(-1)^n F^{(n)}(s)</m></cell>
			</row>
			<row>
				<cell><m>e^{at} f(t)</m></cell>
				<cell><m>F(s-a)</m></cell>
			</row>

			</tabular>
		</table>
		<p>
			<xref ref="so-tab-laplace"/> updates our table of Laplace transforms with additional entries. In this section we examine how to use the new entries.
		</p>

	</introduction>

	<subsection>
		<title>Second derivatives</title>
		<p>
			Recall that the transform of <m>x'(t)</m> is <m>sX(s)-x(0)</m>. Applying this formula twice shows that 
			<me>
				\lx[x''(t)] = s \lx[x'(t)] - x'(0) = s^2X(s) - sx(0) - x'(0)
			</me>.
			When we apply this to <m>x''+bx'+cx=f(t)</m> with constant coefficients, we obtain 
			<me>
				[s^2X(s) - sx(0)-x'(0)] + b[sX(s)-x(0)] + cX(s) = F(s)
			</me>.
			It is now trivial to solve for <m>X(s)</m>: 
			<me>
				X(s) = \frac{ (s+b)x(0)+x'(0)}{s^2 +bs+c} +  \frac{F(s)}{s^2 +bs+c}
			</me>.
			The first fraction above is what leads to the free or unforced response, while the second is the effect of the forcing when the initial conditions are zero. The denominator in both cases is simply the characteristic polynomial of the ODE. 
		</p>
		<p>
			If the goal is to solve an initial-value problem with the transform, then it's necessary to find the inverse transform of <m>X(s)</m>, which typically involves some partial fraction efforts. 
		</p>
		<example>
			<p>
				Let's solve <m>x''-x=24 e^{3t}</m>, with <m>x(0)=4</m>, <m>x'(0)=12</m>. Transforming both sides gives 
				<me> 
					[s^2X(s) - 4s - 12] - X(s) = \frac{24}{s-3}
				</me>. 
				We easily solve for <m>X</m>:
				<me>
					X(s) = \frac{4s+12}{(s^2-1)} + \frac{24}{(s^2-1)(s-3)} 
				</me>. 
				In order to avoid multiple uses of partial fractions, we combine everything into a single fraction: 
				<me>
					X(s) = \frac{4s^2-12}{(s-1)(s+1)(s-3)}
				</me>
				The PFD takes the form 
				<me>
					\frac{4s^2-12}{(s-1)(s+1)(s-3)} = \frac{A}{s-1} + \frac{B}{s+1} + \frac{C}{s-3}, 
				</me>
				which is the same as the identity 
				<me>
					4s^2-12 = A(s+1)(s-3) + B(s-1)(s-3) + C(s-1)(s+1).
				</me>
				One way to handle this is to expand the right-hand side and equate the powers of <m>s</m>. In this case it's easier to choose three strategic values of <m>s</m>.   
				<md>
				  <mrow> s = 1 \amp \:\Rightarrow\: -8 = -4A  </mrow>
				  <mrow> s = -1 \amp\: \Rightarrow\: -8 = 8B </mrow>
				  <mrow> s = 3 \amp\: \Rightarrow\: 24 = 8C </mrow>
				</md>, 
				so that 
				<me>
					X(s) = \frac{2}{s-1} + \frac{-1}{s+1} + \frac{3}{s-3}.
				</me>
				Now it's clear that <m>x(t) = 2e^t - e^{-t} + 3e^{3t}</m>.
			</p>
		</example>
		<p>
			In light of the last example, let's look again at the general case 
			<me>
				X(s) = \frac{ (s+b)x(0)+x'(0)}{s^2+bs+c} +  \frac{F(s)}{s^2 +bs+c}
			</me>.
			The denominators contain the characteristic polynomial, whose roots are the eigenvalues of the homogeneous problem. We refer to zeros in the denominator as <idx>poles</idx><term>poles</term> of the transform. Each eigenvalue pole will contribute a term <m>1/(s-\lambda)</m> to the partial fraction decomposition. Upon inversion, this term contributes <m>e^{\lambda t}</m>, which is part of the homogeneous solution. The forcing function will contribute one or more of its own poles in <m>F(s)</m>, and they generate new terms in the partial fraction decomposition. 
		</p>
		<example>
			<p>
				To solve <m>x'' - 4x + 3x = 6</m> with <m>x(0)=-2</m>, <m>x'(0)=4</m>,  we take transforms to obtain 
				<me>
					X(s) = \frac{-2s + 8 + 4}{(s-3)(s-1)} + \frac{6}{s(s-3)(s-1)} .
				</me>
				The PFD of this has the form 
				<me>
					X(s) = \frac{A}{s-3} + \frac{B}{s-1} + \frac{C}{s}.
				</me>
				Equating and clearing the denominators leads to 
				<me>
					(s)(12-2s) + 6 = As(s-1) + Bs(s-3) + C(s-1)(s-3).
				</me>
				The values <m>s=0,s=1,s=3</m>, which are simply the poles of <m>X</m>, leave especially simple equations: <m>6=3C</m>, <m>10+6=-2B</m>, and <m>18+6=6A</m>. Hence  
				<me>
					X(s) = 4\, \frac{1}{s-3} - 8\, \frac{1}{s-1} + 2\, \frac{1}{s}.
				</me>
				This inverts easily to give <m>x(t) = 4e^{3t} -  8e^t + 2</m>. 
			</p>
		</example>
	</subsection>

	<subsection> 
		<title>Imaginary poles</title>
		<p>
			If we transform <m>e^{i\omega t}</m>, we can take the real and imaginary parts to get some new formulas:
			<me>
				\lx[\cos(\omega t)] = \text{Re}\left( \frac{1}{s-i\omega} \frac{s+i\omega}{s+i\omega} \right) = \frac{s}{s^2+\omega^2}
			</me>,
			and 
			<me>
				\lx[\sin(\omega t)] = \text{Im}\left( \frac{1}{s-i\omega} \frac{s+i\omega}{s+i\omega} \right) = \frac{\omega}{s^2+\omega^2}
			</me>.
			Conversely, a transform <m>X(s)</m> that has <m>s^2+\omega^2</m> as a factor in the denominator has poles at <m>\pm i\omega</m>. The simplest case is 
			<me>
				X(s) = \frac{A+Bs}{s^2+\omega^2}  \quad \Rightarrow \quad x(t)=\frac{A}{\omega}\sin(\omega t) + B\cos(\omega t)
			</me>. 
		</p>
		<example>
			<p>
				Let's solve <m>x''+9x=\delta(t-2)</m> with <m>x(0)=0</m>, <m>x'(0)=-3</m>. Taking transforms leads to
				<me>
					X(s) = \frac{-3}{s^2+9} + \frac{e^{-2s}}{s^2+9} 
				</me>. 
				The first term inverts easily to <m>-\sin(3t)</m>. The second has the form <m>e^{-2s} F(s)</m>, where <m>F(s)</m> is the transform of 
				<me>
					f(t) = \frac{1}{3}\sin(3t)
				</me>. So the shift theorem implies
				<me>
					x(t) = -\sin(3t) + \frac{1}{3} H(t-2) \sin(3t-6)
				</me>. 
			</p>
		</example>
		<example>
			<p>
				To find the general soluton of <m>x'' - 4x' + 3x = 10\cos(t)</m> with zero initial conditions, we take transforms to obtain 
				<me>
					X(s) =  \frac{10s}{(s^2+1)(s-3)(s-1)} .
				</me>
				The PFD of this has the real form 
				<me>
					X(s) = \frac{A}{s-3} + \frac{B}{s-1} + \frac{Cs+D}{s^2+1}.
				</me>
				Equating and clearing denominators gives 
				<me>
					10s = A(s-1)(s^2+1) + B(s-3)(s^2+1) + (Cs+D)(s-3)(s-1).
				</me>
				We could equate powers of <m>s</m> to get four linear equations for the constants. However, life is again easier if we evaluate the identity at the poles. 
				We don't care about the initial values, so we evaluate both sides at <m>s=i</m>: 
				<md>
					<mrow> s=1 \amp \:\Rightarrow\: 10  = -4B  </mrow>
					<mrow> s=3 \amp \:\Rightarrow\: 30 = 20A </mrow>
					<mrow> s=i \amp \:\Rightarrow\: 10i  = (i-1)(i-3)(iC+D) = (2-4i)(iC+D) = (4C+2D) + i(2C-4D)  </mrow>
				</md>.
				Equating the real and imaginary parts in the last line gives <m>4C+2D=0</m>, <m>2C-4D=10</m>, which are easily solved for <m>C=1,D=-2</m>. Hence 
				<me>
					X(s) = \frac{3/2}{s-3} + \frac{-5/2}{s-1} + \frac{s-2}{s^2+1},
				</me>
				
				and finally 
				<me>
					x(t) = \frac{3}{2} e^{3t} -\frac{5}{2} e^{t} + \cos(t) - 2 \sin(t). 
				</me>
			</p>
		</example>
		<example>
			<p>
				The undamped oscillator <m>x''+16x = 40e^{-2t}</m> with <m>x(0)=x'(0)=0</m> has a solution whose transform is 
				<me>
					X(s) = \frac{40}{(s+2)(s^2+16)} = \frac{A+Bs}{s^2+16^2} + \frac{C}{s+2}
				</me>. 
				Clearing the denominators gives the identity
				<me>
					(A+Bs)(s+2) + C(s^2+16) = 40
				</me>. 
				We need three equations to determine the unknown coefficients. Setting <m>s=-2</m> quickly gives <m>20C=40</m>, so <m>C=2</m>. Now the identity reads 
				<me>
					Bs^2 + (A+2B)s + 2A = 8 - 2s^2. 
				</me>
				Equating the coefficients of <m>1</m> and <m>s^2</m> immediately tells us <m>2A=8</m> and <m>B=-2</m>. Hence 
				<me>
					X(s) = \frac{4-2s}{s^2+16^2} + \frac{2}{s+2} 
				</me>, 
				and finally 
				<me>
					x_p(t) = \sin(4t) - 2\cos(4 t) + 2e^{-2t}
				</me>.
			</p>
		</example>
		<example>
			<p>
				To find the general solution of <m>x''+x=5H(t-4)</m> starting from rest, we take transforms to obtain 
				<me>
					X(s) = \frac{5e^{-4s}}{s(s^2+1)} = 5e^{-4s}F(s) 
				</me>, 
				where 
				<me>
					F(s) = \frac{1}{s(s^2+1)} = \frac{A}{s} + \frac{Bs+C}{s^2+1}. 
				</me>
				Clearing denominators we get 
				<me>
					1 = A(s^2+1) + (Bs+C)(s). 
				</me>
				Setting <m>s=0</m> gives us <m>A=1</m> right away. Then <m>-s^2=Bs^2+Cs</m>, which tells us that <m>B=-1</m> and <m>C=0</m>. Hence 
				<me>
					f(t) = 1 -\cos(t). 
				</me>
				So by the shift theorem, the solution is 
				<me>
					x(t) = 5H(t-4)f(t-4) = 5 H(t-4)[1-\cos(t-4)]. 
				</me>
			</p>
		</example>
	</subsection>
	<subsection>
		<title>Complex poles</title>
		<p> 
			Complex poles in the transform correspond to complex exponentials in the time domain. When they occur in conjugate pairs, the entire expression can be cast in real terms. It can help to use the transform-shift formula 
			<me>
				\lx[e^{at}f(t)] = F(s-a)
			</me>.
			We should have enough information from the ODE to know what to choose as the real shift <m>a</m>. 
		</p>
		<example>
			<p>
				Suppose <m>x''+2x'+10x=6\delta(t-1)</m>, with zero initial conditions. Then  
				<me>
					X(s) = \frac{6e^{-s}}{s^2+2s+10}
				</me>.
				The quadratic formula tells us the homogeneous eigenvalues <m>-1\pm 3i</m>, which will contribute an amplitude <m>e^{-t}</m> in time. This corresponds to a shift <m>a=-1</m>, so we designate <m>X(s)=F(s+1)</m>. Thus
				<me>
					F(s) = X(s-1) = 6e \frac{e^{-s}}{s^2-2s+1+2s-2+10} = 6e \frac{e^{-s}}{s^2+9}.
				</me>
				Now we can apply the Shift Theorem to <m>F</m> using the definition <m>V(s)=1/(s^2+9)</m> and get 
				<me>
					f(t) = 6e H(t-1) v(t-1),
				</me>
				where <m>v(t)=\frac{1}{3}\sin(3t)</m>. When we unwind the definitions, we get 
				<me>
					x(t) = e^{-t} f(t) = 2 e^{-t+1} H(t-1)\sin(3t-3).
				</me>
			</p>
		</example>
		<p> 
			The drawback to Laplace transforms as a one-stop solver is what we observed earlier: the particular solution we find virtually always includes some terms from the homogeneous solution. Mathematically, the poles of <m>X_p(s)</m> include those of the transfer function <em>and</em> the forcing function. This combination makes the partial fraction decompositions lengthier, to say the least. Undetermined coefficients is a less sexy but simpler way to solve concrete problems in most cases, especially if one seeks a general solution without initial conditions to consider at the end.
		</p>
	</subsection>
	<subsection>
		<title>Repeated poles and resonance</title>
		<p>
			Repeated poles in the Laplace transform of a solution appear in the form of denominator terms such as <m>(s-a)^k</m> or <m>(s^2+\omega^2)^k</m> with <m>k \gt 1</m>. They can arise in two ways: from repeated eigenvalues in the homogeneous ODE (e.g., critical damping), or because the forcing function has a pole in common with one of the eigenvalues (e.g., perfect resonance). 
		</p>
		<p>
			Consider inverting the double real pole
			<me>
				X(s) = \frac{1}{(s-a)^2}
			</me>. 
			The key is to note that <m>X(s)=-F'(s)</m>, where <m>F(s) = (s-a)^{-1}</m> is the transform of <m>e^{at}</m>. From the transform definition we compute
			<md>
				<mrow>-F'(s) \amp = -\frac{d}{ds} \int_0^\infty f(t) e^{-s t}\, dt</mrow>
				<mrow>\amp = \int_0^\infty t f(t) e^{-s t}\, dt</mrow>
				<mrow>\amp = \lx[ t f(t) ] = \lx[ t e^{at} ] </mrow>
			</md>. 
			If we keep differentiating in <m>s</m> we find 
			<men xml:id="so-eq-repeatedexp">
				\lx[t^n e^{at} ] = \frac{n!}{(s-a)^{n+1}}.
			</men>
			The special case <m>a=0</m> allows us to transform any power of <m>t</m>, and therefore any polynomial. 
		</p>
		<example>
			<p>
				The critically damped oscillator <m>x'' + 6x' + 9=0</m> has a double eigenvalue at <m>-3</m>. To find the solution with <m>x(0)=0</m>, <m>x'(0)=6</m>, we use the transform to find 
				<me>
					[s^2X(s) - 6] + 6[sX(s)] + 9[X(s)] = 0 \quad \Rightarrow \quad X(s) = \frac{6}{(s+3)^2}.
				</me>
				Hence <xref ref="so-eq-repeatedexp"/> applies with <m>n=1</m>, and 
				<me>
					x(t) = 6t e^{-3t}
				</me>.			
			</p>
		</example>
		<p>
			If you look carefully at the derivation of <xref ref="so-eq-repeatedexp"/>, you will see that the identity of <m>f(t)</m> played no role in the key steps. As shown in <xref ref="so-tab-laplace"/>, we can repeat the calculation for a generic <m>F(s)</m>: 
			<me>
				\lx[t^n f(t) ] = (-1)^n F^{(n)}(s), \qquad n=1,2,3,\dots.
			</me>
			Take a moment to appreciate this. Earlier we found that a derivative in <m>t</m> essentially corresponds to multiplying by <m>s</m>. Up to a change in sign, a derivative in <m>s</m> corresponds to multiplying by <m>t</m>! 
		</p>
		<example>
			<p>
				The undamped oscillator <m>x''+9x=\sin(3t)</m> has a perfect resonance. From zero initial conditions we have
				<me>
					X(s) = \frac{3}{(s^2+9)^2}
				</me>. 
				We are going to use  
				<me>
					\frac{d}{ds} \left[ \frac{3}{s^2+9} \right] = \frac{-6s}{(s^2+9)^2}.
				</me>
				Specifically, if we designate <m>G(s)=2sX(s)</m>, then we know that  
				<me>
					\lx[ t \sin(3t) ] = -\frac{d}{ds} \left[ \frac{3}{s^2+9} \right] = G(s),
				</me>
				and therefore <m>g(t)=t\sin(3t)</m>. Next, note that 
				<me>
					2\lx[x'(t)] = 2[sX(s)-X(0)] = G(s).
				</me>
				Hence <m>x'(t) = \frac{1}{2} t \sin(3t)</m>, and integration by parts yields
				<me>
					x(t) = \frac{1}{18} [ \sin(3t) - 3t\cos(3t) ]
				</me>.
			</p>
		</example>
	</subsection>

</section>

<section xml:id="so-convolutions">
	<title>Convolution</title>
	<introduction>
		<p>
			
		</p>
	</introduction>
	
	<subsection>
		<!-- %bc = webread('https://api.coindesk.com/v1/bpi/historical/close.json');%data = jsondecode(bc); -->
		<title>Moving averages</title>
		<p>
			The following grabs and plots the closing price of Bitcoin for the last 31 days.
		</p>
			<sidebyside>
			<listing  xml:id="so_conv_bitcoin1">
				<caption>Raw bitcoin price data</caption>
	 			<program language="matlab">
	    			<input>
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-o')
					</input>
				</program>
				</listing>
				<image source="figures/so_conv_bitcoin1.svg"/>	  
			</sidebyside>
			<p>
				It's a noisy curve. We can smooth that out by taking 4-day moving averages, for example. I don't claim that this is the best way, but we can do this by:
			</p>
			<sidebyside>
				<listing  xml:id="so_conv_bitcoin2">
					<caption>Data with 4-day moving average</caption>
	 			<program language="matlab">
	    			<input>
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-ko')
for i = 4:31
    z(i) = (v(i)+v(i-1)+v(i-2)+v(i-3))/4;
end
z(1:3) = NaN;  % not a number
hold on, plot(z,'-o')
					</input>
				</program>
				</listing>
				<image source="figures/so_conv_bitcoin2.svg"/>	  
			</sidebyside>

			<p>
				We might decide, however, to weight the most recent values more heavily. Here's how this might look.
			</p>
			<sidebyside>
				<listing xml:id="so_conv_bitcoin3">
					<caption>Data with weighted moving average</caption>
	 			<program language="matlab">
	    			<input>
load bitcoindata
v = struct2cell(data.bpi);
v = cat(1,v{:});
plot(v,'-ko')
w = [4 3 2 1];
for i = 4:31
	z(i) = (w(1)*v(i) + w(2)*v(i-1)... 
	  + w(3)*v(i-2) + w(4)*v(i-3)) / sum(w);
end
z(1:3) = NaN;  % not a number
hold on, plot(z,'-o')
					</input>
				</program>
				</listing>
				<image source="figures/so_conv_bitcoin3.svg"/>	  
			</sidebyside>
			<p>
				Note that each new value <m>z_i</m> is a linear combination of the elements of <m>v</m>, weighted by the values in <m>w</m>. And we always go forward 1,2,3,4 in <m>w</m>, while on <m>v</m> we have a sliding group going backward: <m>i,i-1,i-2,i-3</m>. Put concisely, the indices add up to the constant value <m>i+1</m>. 
			</p>
	</subsection>

	<subsection>
		<title>Convolution integral</title>
		<p>
			The moving average above gave us a certain way to multiply together vectors <m>v</m> and <m>w</m> to get a new vector <m>z</m>: 
			<me>
				z_i =  \sum_{j} w_j v_{i-j} 
			</me>
			(although I reindexed the output vector from <m>i+1</m> to <m>i</m> compared to the computer code). This is analogous to a different way to multiply two <em>functions</em>, by the <term>convolution integral</term>
			<me> 
				[f*g](t) = \int_0^t f(t-\tau)g(\tau)\, d\tau
			</me>. 
			This too is like a weighted average, with the values of <m>g(\tau)</m> being multiplied against different windows of the function <m>f</m> such that the arguments sum to <m>t</m>. 
		</p>
		<p> 
			An interesting fact is that while I interpreted <m>f*g</m> as <m>g</m> acting on <m>f</m>, in fact the operation is commutative:
			<md>
				<mrow>[g*f](t) \amp = \int_0^t f(t-\tau)g(\tau)\, d\tau</mrow>
				<mrow>\amp = \int_t^0 f(u)g(t-u)\, (-du)  </mrow>
				<mrow>\amp = \int_0^t g(t-u)f(u)\, du = [f*g](t)</mrow>
			</md>. 
			There are several other properties that make convolution resemble multiplication. 
		</p>
		<fact xml:id="so-fact-convolution">
			<statement>
				<p>
					The following statements are true for functions defined for <m>t\ge 0</m>. 
					<ol>
					  <li> <m>f*g = g*f</m></li> 
					  <li> <m>(f*g)*h = f*(g*h)</m></li>
					  <li> <m>f*(g+h) = (f*g) + (f*h)</m></li>
					<li> <m>f*0=0\quad</m> (zero function)</li>
					<li><m>f*\delta = f</m></li>
					</ol>
				</p>
			</statement>
		</fact>
		<p>
			The last of these facts is the only slight curveball. It says that the delta impulse serves as the identity in convolution operations. 
		</p>
	</subsection>
	<subsection>
		<title>Transfer function and impulse response</title>
		<p>
			Following what we did for first-order problems, we make the following definitions. 
		</p>
		<definition xml:id="s0-def-impulsetransfer">
			<statement>
				<p>
					For the operator <m>\opA[x]=x'' + bx'+cx</m> with constant coefficients, we define the <idx>impulse response</idx><term>impulse response</term> as the solution of 
					<me>
						\opA[x] = \delta(t), \quad x(0)=x'(0)=0.
					</me>
					We will use <m>g(t)</m> to denote the impulse response. We also define the <idx>transfer function</idx><term>transfer function</term> of the ODE to be the transform <m>G(s)</m> of the impulse response. 
				</p>
			</statement>
		</definition>
		<p>
			Because the transform of delta is one, the immediate consequence of combining these definitions is 
			<me>
				s^2 G + bs G + cG  = 1 \quad\Rightarrow\quad G(s) = \frac{1}{s^2+bs+c}.
			</me>
			In words, <alert>the transfer function is the recciprocal of the characteristic polynomial.</alert> 
		</p>
		<example>
			<p>
				The transform of <m>x''-5x'+4x=\delta(t)</m> with zero initial conditions leads to 
				<me>
					X(s) = G(s) = \frac{1}{s^2+5s+4} = \frac{A}{s+1} + \frac{B}{s+4}. 
				</me>
				Clearing denominators and setting <m>s</m> equal to each pole in turn gives <m>A=-B=1/3</m>, so the impulse response is 
				<me>
					g(t) = \frac{1}{3}(e^{-t}-e^{-4t}).
				</me>
			</p>
		</example>
		<p>
			It is trivial to generalize the previous example to any distinct eigenvalues <m>\lambda_1,\lambda_2</m>: 
			<me>
				g(t) = \frac{1}{\lambda_1-\lambda_2}\bigl(e^{\lambda_1 t}-e^{\lambda_2 t}\bigr). 
			</me>
			Since it's a linear combination of the homogeneous exponential solutions, <m>g(t)</m> is equivalent to the solution of a homogeneous problem. You can easily check that <m>g(0)=0</m> and <m>g'(0)=1</m>. Hence:
		</p>
		<fact xml:id="so-fact-jump">
			<statement>
				<p>
					In second order problems, an impulse is equivalent to an instantaneous jump in the derivative of the solution.  
				</p>
			</statement>
		</fact>
		<p>
			This fact modifies the observation in first-order problems that the delta causes a jump in <em>value</em>.
		</p>
	</subsection>
	<subsection>
		<title>Convolution theorem</title>
		<p>
			Here is what makes convolution a really big deal. 
		</p>
		<theorem>
			<p>
				Suppose <m>\lx[f] = F(s)</m>, <m>\lx[g] = G(s)</m>, <m>x=f*g</m>, and <m>\lx[x] = X(s)</m>. Then <m>X(s)=F(s)G(s)</m>.
			</p>
		</theorem> 
		<p> 
			Why does this matter? Consider finding a particular solution of <m>x''+bx'+cx=f(t)</m> with zero initial conditions. After taking transforms we get  
			<me> 
				X_p(s) = \frac{1}{s^2+bs+c} F(s) = G(s) F(s)
			</me>, 
			where <m>G(s)</m> is the transfer function. Right away then, we know that <m>x_p(t)=f(t)*g(t)</m> is that particular solution. In theory, knowing the impulse response is enough to write down the solution of any forced problem. 
		</p>
		<example>
			<p>
				Above we found the impulse response of <m>x''-5x'+4x</m> to be <m>g(t)=\frac{1}{3}=(e^{-t}-e^{-4t})</m>. Hence a particular solution for forcing function <m>f</m> is 
				<md>
				  <mrow>x_p(t) \amp = f*g = \frac{1}{3} \int_0^t ( e^{-t+\tau}-e^{-4t+4\tau})\, f(\tau)\,d\tau </mrow>
				  <mrow>\amp = \frac{1}{3} e^{-t} \int_0^t e^{\tau}  f(\tau)\,d\tau -  \frac{1}{3} e^{-4t} \int_0^t  e^{4\tau} f(\tau)\,d\tau. </mrow>
				</md>
			</p>
		</example>
		<example>
			<p>
				The undamped oscillator <m>x''+\omega_0^2 x</m> has transfer function 
				<me>
					G(s) = \frac{1}{s^2+\omega_0^2}
				</me>, 
				so <m>g(t) = \frac{1}{\omega_0}\sin(\omega_0 t)</m>. Note that 
				<me>
					g(t-\tau) = \frac{1}{\omega_0} \sin(\omega_0 t)\cos(\omega_0\tau) - \frac{1}{\omega_0} \cos(\omega_0 t)\sin(\omega_0\tau). 
				</me>
				Therefore, using convolution with forcing function <m>f</m>,
				<me>
					x_p(t) = \frac{1}{\omega_0}  \sin(\omega_0 t) \int_0^t \cos(\omega_0\tau)f(\tau)\,d\tau - \frac{1}{\omega_0}  \cos(\omega_0 t) \int_0^t \sin(\omega_0\tau)f(\tau)\,d\tau.
				</me>
				This formula would be another way to derive both the resonant and nonresonant solutions with cosine forcing <m>f(t)=\cos(\omega t)</m>. 
			</p>
		</example>
		<p>
			By now it should not surprise you that linear problems often give you multiple options that converge on the same result. The integral formulas derived from convolution with the impulse response are identical to ones that we would derive by applying variation of parameters to the first-order system form of the ODE. Both examples above express the solution as functions times the homogeneous solutions of the system, which is the starting assumption for deriving that method.
		</p>
	</subsection>

	<exercises xml:id="so-exer-convolution">
		<exercise>
			<title>Double eigenvalue</title>
			<statement>
			  <p>
				In the impulse response for distinct eigenvalues, take the limit <m>\lambda_2 \to \lambda_1</m> to propose an impulse response <m>g(t)</m> for the case of a double eigenvalue. Then show that <m>g</m> solves the homogeneous problem, and <m>g(0)=0</m>, <m>g'(0)=1</m>. Finally, show that the transform of <m>g</m> does match the transfer function. 
			  </p>
			</statement>	
			<solution>
				<p>
					Use L'Hopital's rule to find <m>g(t)=t e^{\lambda t}</m>. Plug it into <m>g''-2\lambda g'+ \lambda^2 g </m> and simplify to get zero. Check the initial conditions. Finally, the transform table says <m>G(s)=F(s-\lambda)</m> where <m>F(s)=1/s^2</m>, which gives the correct transfer function. 
				</p>
			</solution>
		</exercise>
		<exercise>
			<title>Damped oscillator</title>
			<statement>
			  <p>
				Use convolution to derive the solution of the damped oscillator <m>x''+2\omega_0 Z x' + \omega_0^2 x = f(t)</m>. Express the formulas in real form in the underdamped case.
			  </p>
			</statement>
		</exercise>
	</exercises>
	<solutions divisional="solution"></solutions>
	
</section>


</chapter>